{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home My Daily Serverless Study Notes Select the notes from the top Navigation","title":"Home"},{"location":"#home","text":"My Daily Serverless Study Notes","title":"Home"},{"location":"Notes/01 serverless 101/01 Serverless Computing/","text":"Serverless Computing","title":"01 Serverless Computing"},{"location":"Notes/01 serverless 101/01 Serverless Computing/#serverless-computing","text":"","title":"Serverless Computing"},{"location":"Notes/01 serverless 101/02 Pro and Cons Of Serverless/","text":"Pro and Cons Of Serverless Section 1, lesson 9 Pros No server managements Easy & Efficient scaling Built in high available and fault tolerant Service integration No idle capacity Cons Vendor Lock Ins (Except Multi Provider Serverless) Public Cloud (Limited to local server implementation) Control limitation in hardware and os level","title":"02 Pro and Cons Of Serverless"},{"location":"Notes/01 serverless 101/02 Pro and Cons Of Serverless/#pro-and-cons-of-serverless","text":"Section 1, lesson 9 Pros No server managements Easy & Efficient scaling Built in high available and fault tolerant Service integration No idle capacity Cons Vendor Lock Ins (Except Multi Provider Serverless) Public Cloud (Limited to local server implementation) Control limitation in hardware and os level","title":"Pro and Cons Of Serverless"},{"location":"Notes/01 serverless 101/03 Major Players/","text":"Major Players Providers AWS Lambda Azure Function Google Cloud Function IBM Cloud AWS Preference First in market Seamless integration with other services","title":"03 Major Players"},{"location":"Notes/01 serverless 101/03 Major Players/#major-players","text":"Providers AWS Lambda Azure Function Google Cloud Function IBM Cloud AWS Preference First in market Seamless integration with other services","title":"Major Players"},{"location":"Notes/01 serverless 101/04 Core Serverless Services/","text":"Core Serverless Services 3 common services, Lambda API Gateway DynamoDB","title":"04 Core Serverless Services"},{"location":"Notes/01 serverless 101/04 Core Serverless Services/#core-serverless-services","text":"3 common services, Lambda API Gateway DynamoDB","title":"Core Serverless Services"},{"location":"Notes/01 serverless 101/05 Framework and CI-CD Overview/","text":"Framework and CI-CD Overview Two serverless framework AWS SAM (Serverless Application Model) Lightweight version of AWS Cloudformation The serverless framework (open-source 3rd party tool provided by Serverless Inc ) CI/CD Integration CodeCommit (Repository/codebase) CoedBuild (Build serverless app and create resource using cloudformation) CodePipeline (Automate App Deployment Process)","title":"05 Framework and CI CD Overview"},{"location":"Notes/01 serverless 101/05 Framework and CI-CD Overview/#framework-and-ci-cd-overview","text":"Two serverless framework AWS SAM (Serverless Application Model) Lightweight version of AWS Cloudformation The serverless framework (open-source 3rd party tool provided by Serverless Inc ) CI/CD Integration CodeCommit (Repository/codebase) CoedBuild (Build serverless app and create resource using cloudformation) CodePipeline (Automate App Deployment Process)","title":"Framework and CI-CD Overview"},{"location":"Notes/01 serverless 101/05 Use Cases/","text":"Use Cases Application backend for web, mobile or IOT devices Architecture Real-time/streaming data processing Architecture","title":"05 Use Cases"},{"location":"Notes/01 serverless 101/05 Use Cases/#use-cases","text":"Application backend for web, mobile or IOT devices Architecture Real-time/streaming data processing Architecture","title":"Use Cases"},{"location":"Notes/02 hello-world/01 Hello World Mock API/","text":"Hello World Mock API Go to Amazon API Gateway choose rest api resource is an api endpoint resource name and path can be message create a resource Create a method methods handle rest-api-endpoints with get, post, patch, etc select get protocol and create one We can have various method, choose Mock one for simplicity This works like, Method Request -> Integration Request -> Integration Response -> Method Response Update the integration response for demo select integration response expand Method Response -> Mapping Templates -> application/json in template, create a json object, { message: \"Hello World\" } Deploy API From action dropdown, select Deploy API Select new stage with a stage name test and click deploy Stage after deploy, we should go to the staging area with api endpoint go to provided_api_end_point/message and we should see output of Hello World Quick Test It is possible to test the api without deploying Go to Resources -> Get Click the Test button to go to the method execution page We do not need any query string or headers, so click Test and verify the output Hello World","title":"01 Hello World Mock API"},{"location":"Notes/02 hello-world/01 Hello World Mock API/#hello-world-mock-api","text":"Go to Amazon API Gateway choose rest api resource is an api endpoint resource name and path can be message create a resource Create a method methods handle rest-api-endpoints with get, post, patch, etc select get protocol and create one We can have various method, choose Mock one for simplicity This works like, Method Request -> Integration Request -> Integration Response -> Method Response Update the integration response for demo select integration response expand Method Response -> Mapping Templates -> application/json in template, create a json object, { message: \"Hello World\" } Deploy API From action dropdown, select Deploy API Select new stage with a stage name test and click deploy Stage after deploy, we should go to the staging area with api endpoint go to provided_api_end_point/message and we should see output of Hello World Quick Test It is possible to test the api without deploying Go to Resources -> Get Click the Test button to go to the method execution page We do not need any query string or headers, so click Test and verify the output Hello World","title":"Hello World Mock API"},{"location":"Notes/02 hello-world/02 First Lambda Method/","text":"First Lambda Method Select AWS Lambda service Select Create Function Select Author From Scratch name getRandomMessage Runtime Node 14.x For permission, the lambda will create a basic role for the execution of the function Click Create Function From source code, update index.js as, exports.handler = async event => { return 'Hello Lambda Function'; }; Since these functions are event driven, we need to create a event to test the function. To test Click test Create event with no data and named emptyEventData If we click the test again, we should see, success response with message Hello Lambda Function","title":"02 First Lambda Method"},{"location":"Notes/02 hello-world/02 First Lambda Method/#first-lambda-method","text":"Select AWS Lambda service Select Create Function Select Author From Scratch name getRandomMessage Runtime Node 14.x For permission, the lambda will create a basic role for the execution of the function Click Create Function From source code, update index.js as, exports.handler = async event => { return 'Hello Lambda Function'; }; Since these functions are event driven, we need to create a event to test the function. To test Click test Create event with no data and named emptyEventData If we click the test again, we should see, success response with message Hello Lambda Function","title":"First Lambda Method"},{"location":"Notes/02 hello-world/03 Integrate a Lambda Function with API Gateway/","text":"Integrate a Lambda Function with API Gateway Go to API Gateway we created earlier Click on the Integration Request As Integration Type select Lambda Region should be the region we are using to create Lambda function As Lambda Function put the name of of the Lambda Function Name , we created earlier Click save and confirm To test, Go to get method, and click test . We should see the response message Hello From Lambda","title":"03 Integrate a Lambda Function with API Gateway"},{"location":"Notes/02 hello-world/03 Integrate a Lambda Function with API Gateway/#integrate-a-lambda-function-with-api-gateway","text":"Go to API Gateway we created earlier Click on the Integration Request As Integration Type select Lambda Region should be the region we are using to create Lambda function As Lambda Function put the name of of the Lambda Function Name , we created earlier Click save and confirm To test, Go to get method, and click test . We should see the response message Hello From Lambda","title":"Integrate a Lambda Function with API Gateway"},{"location":"Notes/02 hello-world/04 Manipulating Response/","text":"Manipulating Response Previously, with mock endpoint, we used to get response { message: some_message } . But now, we are getting response like some_message using the lambda endpoint. To manipulate the response message, we can make use of overriding the Integration Response . Back to Method Execution Map page Click the Integration Response Expand Response -> Mapping Templates -> application/json Our response should be { \"message\": \"$input.body\" } Now back to method execution page and test","title":"04 Manipulating Response"},{"location":"Notes/02 hello-world/04 Manipulating Response/#manipulating-response","text":"Previously, with mock endpoint, we used to get response { message: some_message } . But now, we are getting response like some_message using the lambda endpoint. To manipulate the response message, we can make use of overriding the Integration Response . Back to Method Execution Map page Click the Integration Response Expand Response -> Mapping Templates -> application/json Our response should be { \"message\": \"$input.body\" } Now back to method execution page and test","title":"Manipulating Response"},{"location":"Notes/03 Environment Setup/01 Local Environment Setup/","text":"Local Environment Setup Create User Go to IAM service Go to users from left navigation Select Add User username can be serverless-admin check the Programmatic access As permission, add AdministratorAccess from existing policy Configure CLI Follow the instruction section of Install AWS CLI In Local Machine from here Run aws configure Put the access key, secret and region Another way to temporary setup can be, export AWS_ACCESS_KEY_ID=access_key_id export AWS_SECRET_ACCESS_KEY=secret_access_key export AWS_DEFAULT_REGION=default_region To verify the proper CLI setup, aws sts get-caller-identity This should output the userId, account and ARN.","title":"01 Local Environment Setup"},{"location":"Notes/03 Environment Setup/01 Local Environment Setup/#local-environment-setup","text":"Create User Go to IAM service Go to users from left navigation Select Add User username can be serverless-admin check the Programmatic access As permission, add AdministratorAccess from existing policy Configure CLI Follow the instruction section of Install AWS CLI In Local Machine from here Run aws configure Put the access key, secret and region Another way to temporary setup can be, export AWS_ACCESS_KEY_ID=access_key_id export AWS_SECRET_ACCESS_KEY=secret_access_key export AWS_DEFAULT_REGION=default_region To verify the proper CLI setup, aws sts get-caller-identity This should output the userId, account and ARN.","title":"Local Environment Setup"},{"location":"Notes/03 Serverless Foundation/01 Lambda/01 Quick Overview/","text":"","title":"01 Quick Overview"},{"location":"Notes/03 Serverless Foundation/01 Lambda/02 Console Walk Through/","text":"Console Walk Through","title":"02 Console Walk Through"},{"location":"Notes/03 Serverless Foundation/01 Lambda/02 Console Walk Through/#console-walk-through","text":"","title":"Console Walk Through"},{"location":"Notes/03 Serverless Foundation/01 Lambda/03 Permission Model/","text":"Permission Model","title":"03 Permission Model"},{"location":"Notes/03 Serverless Foundation/01 Lambda/03 Permission Model/#permission-model","text":"","title":"Permission Model"},{"location":"Notes/03 Serverless Foundation/01 Lambda/04 Lambda Handler Syntax/","text":"Lambda Handler Syntax","title":"04 Lambda Handler Syntax"},{"location":"Notes/03 Serverless Foundation/01 Lambda/04 Lambda Handler Syntax/#lambda-handler-syntax","text":"","title":"Lambda Handler Syntax"},{"location":"Notes/03 Serverless Foundation/01 Lambda/05 Event/","text":"Event Event Types Invocation Types Event Source","title":"05 Event"},{"location":"Notes/03 Serverless Foundation/01 Lambda/05 Event/#event","text":"Event Types Invocation Types Event Source","title":"Event"},{"location":"Notes/03 Serverless Foundation/01 Lambda/06 Context/","text":"Context","title":"06 Context"},{"location":"Notes/03 Serverless Foundation/01 Lambda/06 Context/#context","text":"","title":"Context"},{"location":"Notes/03 Serverless Foundation/01 Lambda/07 Upload Code With Zip File/","text":"Upload Code With Zip File Regular Upload Using CLI First upload to s3 Update lambda function","title":"07 Upload Code With Zip File"},{"location":"Notes/03 Serverless Foundation/01 Lambda/07 Upload Code With Zip File/#upload-code-with-zip-file","text":"Regular Upload Using CLI First upload to s3 Update lambda function","title":"Upload Code With Zip File"},{"location":"Notes/03 Serverless Foundation/01 Lambda/08 Resize Image With S3 Event/","text":"Resize Image With S3 Event","title":"08 Resize Image With S3 Event"},{"location":"Notes/03 Serverless Foundation/01 Lambda/08 Resize Image With S3 Event/#resize-image-with-s3-event","text":"","title":"Resize Image With S3 Event"},{"location":"Notes/03 Serverless Foundation/01 Lambda/09 Limitations/","text":"Limitations Memory Ephemeral Disk Capacity Timeout Payload Size Upload Size Package Size Concurrency","title":"09 Limitations"},{"location":"Notes/03 Serverless Foundation/01 Lambda/09 Limitations/#limitations","text":"Memory Ephemeral Disk Capacity Timeout Payload Size Upload Size Package Size Concurrency","title":"Limitations"},{"location":"Notes/03 Serverless Foundation/01 Lambda/10 Pricing/","text":"Pricing","title":"10 Pricing"},{"location":"Notes/03 Serverless Foundation/01 Lambda/10 Pricing/#pricing","text":"","title":"Pricing"},{"location":"Notes/03 Serverless Foundation/02 API Gateway/01 overview/","text":"","title":"01 overview"},{"location":"Notes/03 Serverless Foundation/02 API Gateway/02 Passing Path Params and Query String/","text":"Passing Path Params and Query String notes on lambda proxy integration","title":"02 Passing Path Params and Query String"},{"location":"Notes/03 Serverless Foundation/02 API Gateway/02 Passing Path Params and Query String/#passing-path-params-and-query-string","text":"notes on lambda proxy integration","title":"Passing Path Params and Query String"},{"location":"Notes/03 Serverless Foundation/02 API Gateway/03 CORS Configuration/","text":"CORS Configuration","title":"03 CORS Configuration"},{"location":"Notes/03 Serverless Foundation/02 API Gateway/03 CORS Configuration/#cors-configuration","text":"","title":"CORS Configuration"},{"location":"Notes/03 Serverless Foundation/02 API Gateway/04 POST Request/","text":"POST Request Create Calculator Lambda Method CreatePost API Gateway Body Mapping Templates VTL Response Mapping Test Using Postman Handle CORS","title":"04 POST Request"},{"location":"Notes/03 Serverless Foundation/02 API Gateway/04 POST Request/#post-request","text":"Create Calculator Lambda Method CreatePost API Gateway Body Mapping Templates VTL Response Mapping Test Using Postman Handle CORS","title":"POST Request"},{"location":"Notes/03 Serverless Foundation/02 API Gateway/05 API Models And Schemas/","text":"API Models And Schemas","title":"05 API Models And Schemas"},{"location":"Notes/03 Serverless Foundation/02 API Gateway/05 API Models And Schemas/#api-models-and-schemas","text":"","title":"API Models And Schemas"},{"location":"Notes/03 Serverless Foundation/02 API Gateway/06 Using API Models For Body Mapping/","text":"Using API Models For Body Mapping","title":"06 Using API Models For Body Mapping"},{"location":"Notes/03 Serverless Foundation/02 API Gateway/06 Using API Models For Body Mapping/#using-api-models-for-body-mapping","text":"","title":"Using API Models For Body Mapping"},{"location":"Notes/03 Serverless Foundation/02 API Gateway/07 Gateway Response/","text":"Gateway Response","title":"07 Gateway Response"},{"location":"Notes/03 Serverless Foundation/02 API Gateway/07 Gateway Response/#gateway-response","text":"","title":"Gateway Response"},{"location":"Notes/03 Serverless Foundation/02 API Gateway/08 Exporting API Doc/","text":"Exporting API Doc","title":"08 Exporting API Doc"},{"location":"Notes/03 Serverless Foundation/02 API Gateway/08 Exporting API Doc/#exporting-api-doc","text":"","title":"Exporting API Doc"},{"location":"Notes/03 Serverless Foundation/02 API Gateway/09 API Keys/","text":"API Keys Create API Level Usage Plan Method Level Usage Plan Passing API Keys With API Gateway Request","title":"09 API Keys"},{"location":"Notes/03 Serverless Foundation/02 API Gateway/09 API Keys/#api-keys","text":"Create API Level Usage Plan Method Level Usage Plan Passing API Keys With API Gateway Request","title":"API Keys"},{"location":"Notes/03 Serverless Foundation/03 DynamoDB/01 overview/","text":"Quick Overview","title":"01 overview"},{"location":"Notes/03 Serverless Foundation/03 DynamoDB/01 overview/#quick-overview","text":"","title":"Quick Overview"},{"location":"Notes/03 Serverless Foundation/03 DynamoDB/02 Compare with the SQL/","text":"Compare with the SQL","title":"02 Compare with the SQL"},{"location":"Notes/03 Serverless Foundation/03 DynamoDB/02 Compare with the SQL/#compare-with-the-sql","text":"","title":"Compare with the SQL"},{"location":"Notes/03 Serverless Foundation/03 DynamoDB/03 Data Types In DynamoDB/","text":"Data Types In DynamoDB","title":"03 Data Types In DynamoDB"},{"location":"Notes/03 Serverless Foundation/03 DynamoDB/03 Data Types In DynamoDB/#data-types-in-dynamodb","text":"","title":"Data Types In DynamoDB"},{"location":"Notes/03 Serverless Foundation/03 DynamoDB/04 Consistency Model/","text":"Consistency Model","title":"04 Consistency Model"},{"location":"Notes/03 Serverless Foundation/03 DynamoDB/04 Consistency Model/#consistency-model","text":"","title":"Consistency Model"},{"location":"Notes/03 Serverless Foundation/03 DynamoDB/05 Capacity Unit/","text":"Capacity Unit","title":"05 Capacity Unit"},{"location":"Notes/03 Serverless Foundation/03 DynamoDB/05 Capacity Unit/#capacity-unit","text":"","title":"Capacity Unit"},{"location":"Notes/03 Serverless Foundation/03 DynamoDB/06 Partitioning/","text":"Partitioning","title":"06 Partitioning"},{"location":"Notes/03 Serverless Foundation/03 DynamoDB/06 Partitioning/#partitioning","text":"","title":"Partitioning"},{"location":"Notes/? DynamoDB/06 misc/","text":"To avoid reserved key name like timestamp , use the expression-attribute-names not equal expression is written in dynamoDB by <> sign.","title":"06 misc"},{"location":"Notes/? DynamoDB/01 Overview/01 Overview/","text":"Overview Relational database is not efficient when it comes to deal with largely un structured data in high volume and high frequency. In this case, NoSQL or cloud database like dynamoDB come to play. Here we will dive into dynamoDB and get the basic concept of it. There is no learning curve for dynamoDB but best practices are factor for critical business success Overview Serverless cloud no-sql database (No server/backedn infrastructire) Get super high performance table on couple of clicks in AWS console Seamless scaling and predictable performance Seamless integration with other aws services like, Redshift, EMR, Cognito, IAM, Cloudtrail, CloudWatch, S3, CloudSearch, Lambda, Data Pipeline etc. Advantages Serverless Managed NoSQL database Scaling On Demand Auto Unlimited concurrent Read/Write Single digit mili second latency Suv microsecond lantency with caching (DAX) Very much cost effective Fault tolerant and secure","title":"01 Overview"},{"location":"Notes/? DynamoDB/01 Overview/01 Overview/#overview","text":"Relational database is not efficient when it comes to deal with largely un structured data in high volume and high frequency. In this case, NoSQL or cloud database like dynamoDB come to play. Here we will dive into dynamoDB and get the basic concept of it. There is no learning curve for dynamoDB but best practices are factor for critical business success Overview Serverless cloud no-sql database (No server/backedn infrastructire) Get super high performance table on couple of clicks in AWS console Seamless scaling and predictable performance Seamless integration with other aws services like, Redshift, EMR, Cognito, IAM, Cloudtrail, CloudWatch, S3, CloudSearch, Lambda, Data Pipeline etc. Advantages Serverless Managed NoSQL database Scaling On Demand Auto Unlimited concurrent Read/Write Single digit mili second latency Suv microsecond lantency with caching (DAX) Very much cost effective Fault tolerant and secure","title":"Overview"},{"location":"Notes/? DynamoDB/01 Overview/02 Architecture/","text":"Architecture At a high level, DynamoDB use a Service Oriented Architecture . It's a decoupled service, can be easily integrated with other AWS service seamlessly. Performance of DynamoDB is predefined and predictable. Internally dynamodb store data in the high performance SSD. In each Partition, dynamoDB can hold at most 10GB of data. A partition can provide max 3000 RCU and 1000 WCU. To ensure high availability, dynamoDB store data in 3 different AZ or facilities withing a region. To store data in a certain partition, dynamoDB usage consistent hashing algorithm.","title":"02 Architecture"},{"location":"Notes/? DynamoDB/01 Overview/02 Architecture/#architecture","text":"At a high level, DynamoDB use a Service Oriented Architecture . It's a decoupled service, can be easily integrated with other AWS service seamlessly. Performance of DynamoDB is predefined and predictable. Internally dynamodb store data in the high performance SSD. In each Partition, dynamoDB can hold at most 10GB of data. A partition can provide max 3000 RCU and 1000 WCU. To ensure high availability, dynamoDB store data in 3 different AZ or facilities withing a region. To store data in a certain partition, dynamoDB usage consistent hashing algorithm.","title":"Architecture"},{"location":"Notes/? DynamoDB/01 Overview/03 Partition Behaviour/","text":"Partition Behaviour Theory for calculating Number Of Partition Number of Partition based on throughput requirement, P(t) = Round( (RCUs / 3000) + (WCUs / 1000) ) Number of Partition based on storage requirement, P(s) = Round( (Storage Required) / 10 GB ) Final partition, based on throughput and storage is, P = Max (P(t), P(s)) For decimal point, aws took the next rounded number Example 1 for calculating Number Of Partition RCU: 1000 WCU: 500 Storage: 5GB Partition based on Throughput = (1000 / 3000) + (500 / 1000) = 0.67 ~ 1 Partition based on storage = 5 / 10 = 0.5 ~ 1 Final Partition number = max (1, 1) = 1 Example 2 for calculating Number Of Partition RCU: 1000 WCU: 500 Storage: 50GB Partition based on Throughput = (1000 / 3000) + (500 / 1000) = 0.67 ~ 1 Partition based on storage = 50 / 10 = 5 Final Partition number = max (1, 5) = 5 In this case, there will be 5 partition, each have (1000 / 5) = 200 RCUs (500 / 1) = 5 WCU (50 / 5) = 10 GB Storage Keeping Ratio of Provision Capacity And Storage Size If the storage size grows to 500 GB, ultimately, each partition will get only 20 RCU and 10 WCU. We can see, the more table size grows, the provision capacity WCU and RCU decrease. We should consider increasing the provision capacity as the storage size grows. To meet the desired performance, we have to upgrade the performance up to 50 times more. And the cost will also be 50 times more. A Cost Effective Solution Splitting data in multiple table can reduce a lot of cost. We can put the most frequently needed data in a table and less frequently needed data in another table. This will allow to keep the frequently accessed table size low and hence can get more WCU and RCU from the partitions. Migrating Data From SQL Migrating 5GB data to dynamoDB. When we put the data to DynamoDB, with 5GB storage, 1000RCU and 500 WCU, we can consider only one partition. For storages, P(s) (5/10) = .5 ~ 1 partition For throughput capacity, P(t) = (1000 / 3000) + (500 / 1000) = 0.67 partition Final Partition = Max (Pt, Ps) = 1 So on a running database, we will require only one partition. But when we migrate the database, from SQL to dynamoDB, to put each of rows from SQL to dynamoDB, we have a lot of write operation and this will require a lot of WCUs. To save costing, we may consider scale up the WCUs to 5000 and after couple of hours we can scale down the WCUs. This will save a lot of costing. But in this context, we have to consider, high number of WCUs will also increase the partition number. With 5000 WCUs, Pt = (1000/3000) + 5000 / 1000 = 5.3 ~ 6 Ps = 5 / 10 = .5 ~ 1 Number of partition = Max (Pt, Ps) = Max(6, 1) = 6 After migration completed, dynamoDB will de-allocated the partition from 6 to 1. But when it's comes to WCUs and RCUs, dynamoDB will still use the max number of partition 6. In this case, for the single running partition, we will get (1000 / 6) = 167 RCUs (We expected 1000 RCUs) (500 / 6) = 83 WCUs (We expected 500 WCUs) Now, if we need the desired performance, we have to increase the throughput capacity to 6 times more. This will also increase the costing. This means, we accidentally increase the costs 6 times. One way could be migrate table data without scaling up the WCUs. Another approach could be design table differently and migrate data to multiple tables to boost up the process. If splitting is not a process, need to distribute data in different partition carefully.","title":"03 Partition Behaviour"},{"location":"Notes/? DynamoDB/01 Overview/03 Partition Behaviour/#partition-behaviour","text":"Theory for calculating Number Of Partition Number of Partition based on throughput requirement, P(t) = Round( (RCUs / 3000) + (WCUs / 1000) ) Number of Partition based on storage requirement, P(s) = Round( (Storage Required) / 10 GB ) Final partition, based on throughput and storage is, P = Max (P(t), P(s)) For decimal point, aws took the next rounded number Example 1 for calculating Number Of Partition RCU: 1000 WCU: 500 Storage: 5GB Partition based on Throughput = (1000 / 3000) + (500 / 1000) = 0.67 ~ 1 Partition based on storage = 5 / 10 = 0.5 ~ 1 Final Partition number = max (1, 1) = 1 Example 2 for calculating Number Of Partition RCU: 1000 WCU: 500 Storage: 50GB Partition based on Throughput = (1000 / 3000) + (500 / 1000) = 0.67 ~ 1 Partition based on storage = 50 / 10 = 5 Final Partition number = max (1, 5) = 5 In this case, there will be 5 partition, each have (1000 / 5) = 200 RCUs (500 / 1) = 5 WCU (50 / 5) = 10 GB Storage Keeping Ratio of Provision Capacity And Storage Size If the storage size grows to 500 GB, ultimately, each partition will get only 20 RCU and 10 WCU. We can see, the more table size grows, the provision capacity WCU and RCU decrease. We should consider increasing the provision capacity as the storage size grows. To meet the desired performance, we have to upgrade the performance up to 50 times more. And the cost will also be 50 times more. A Cost Effective Solution Splitting data in multiple table can reduce a lot of cost. We can put the most frequently needed data in a table and less frequently needed data in another table. This will allow to keep the frequently accessed table size low and hence can get more WCU and RCU from the partitions. Migrating Data From SQL Migrating 5GB data to dynamoDB. When we put the data to DynamoDB, with 5GB storage, 1000RCU and 500 WCU, we can consider only one partition. For storages, P(s) (5/10) = .5 ~ 1 partition For throughput capacity, P(t) = (1000 / 3000) + (500 / 1000) = 0.67 partition Final Partition = Max (Pt, Ps) = 1 So on a running database, we will require only one partition. But when we migrate the database, from SQL to dynamoDB, to put each of rows from SQL to dynamoDB, we have a lot of write operation and this will require a lot of WCUs. To save costing, we may consider scale up the WCUs to 5000 and after couple of hours we can scale down the WCUs. This will save a lot of costing. But in this context, we have to consider, high number of WCUs will also increase the partition number. With 5000 WCUs, Pt = (1000/3000) + 5000 / 1000 = 5.3 ~ 6 Ps = 5 / 10 = .5 ~ 1 Number of partition = Max (Pt, Ps) = Max(6, 1) = 6 After migration completed, dynamoDB will de-allocated the partition from 6 to 1. But when it's comes to WCUs and RCUs, dynamoDB will still use the max number of partition 6. In this case, for the single running partition, we will get (1000 / 6) = 167 RCUs (We expected 1000 RCUs) (500 / 6) = 83 WCUs (We expected 500 WCUs) Now, if we need the desired performance, we have to increase the throughput capacity to 6 times more. This will also increase the costing. This means, we accidentally increase the costs 6 times. One way could be migrate table data without scaling up the WCUs. Another approach could be design table differently and migrate data to multiple tables to boost up the process. If splitting is not a process, need to distribute data in different partition carefully.","title":"Partition Behaviour"},{"location":"Notes/? DynamoDB/01 Overview/04 Key Design/","text":"Key Design To get effective and cost efficient performance, key design is one of the most important.","title":"04 Key Design"},{"location":"Notes/? DynamoDB/01 Overview/04 Key Design/#key-design","text":"To get effective and cost efficient performance, key design is one of the most important.","title":"Key Design"},{"location":"Notes/? DynamoDB/02 Get Started/01 Environment Setup/","text":"Environment Setup Requirements AWS Account AWS CLI Node.js To install aws CLI go to Install CLI and Configure CLI Test Setup Create a directory named, test-installation , mkdir test-installation Go to the directory and create a node project, cd test-installation npm init -y Install aws-sdk , npm i aws-sdk Create a file named, index.js touch index.js Our index.js file should be similar like, const AWS = require(\"aws-sdk\"); AWS.config.update({ region: \"ap-south-1\" }); const dynamoDB = new AWS.DynamoDB(); dynamoDB.listTables((err, data) => { if (err) { console.log(err); } else { console.log(data); } }); If we got output something like, { TableNames: [] } , then the setup is done. Instead of empty array, it could have table name if exist in that region.","title":"01 Environment Setup"},{"location":"Notes/? DynamoDB/02 Get Started/01 Environment Setup/#environment-setup","text":"Requirements AWS Account AWS CLI Node.js To install aws CLI go to Install CLI and Configure CLI Test Setup Create a directory named, test-installation , mkdir test-installation Go to the directory and create a node project, cd test-installation npm init -y Install aws-sdk , npm i aws-sdk Create a file named, index.js touch index.js Our index.js file should be similar like, const AWS = require(\"aws-sdk\"); AWS.config.update({ region: \"ap-south-1\" }); const dynamoDB = new AWS.DynamoDB(); dynamoDB.listTables((err, data) => { if (err) { console.log(err); } else { console.log(data); } }); If we got output something like, { TableNames: [] } , then the setup is done. Instead of empty array, it could have table name if exist in that region.","title":"Environment Setup"},{"location":"Notes/? DynamoDB/02 Get Started/02 Terminologies/","text":"Terminologies SQL DynamoDB Tables Tables Rows items Columns Attributes Indexes Local Secondary Indexes Views Global Secondary Indexes In SQL, it is possible a table exist with no primary key. But in DynamoDB, there should be a primary key and the primary key can consist of max two attributes. The primary key of DynamoDB consists of Partition Key (HASH Type) Sort key (Range Type) When we query on the DynamoDB, we have to use the primary key and this makes the DynamoDB super fast. Although we can scan to the DB without primary key but this is not efficient compare to query with primary key. In DynamoDB, tables are totally separate entity, there is no concept of Foreign Key relationship here. Although, it seems to be a limitation, but having no strict relationship between tables has some great benefits. like, Without table joins, this makes the table queries highly efficient Allow provision appropriate capacity for each tables, so we can get predictable performance","title":"02 Terminologies"},{"location":"Notes/? DynamoDB/02 Get Started/02 Terminologies/#terminologies","text":"SQL DynamoDB Tables Tables Rows items Columns Attributes Indexes Local Secondary Indexes Views Global Secondary Indexes In SQL, it is possible a table exist with no primary key. But in DynamoDB, there should be a primary key and the primary key can consist of max two attributes. The primary key of DynamoDB consists of Partition Key (HASH Type) Sort key (Range Type) When we query on the DynamoDB, we have to use the primary key and this makes the DynamoDB super fast. Although we can scan to the DB without primary key but this is not efficient compare to query with primary key. In DynamoDB, tables are totally separate entity, there is no concept of Foreign Key relationship here. Although, it seems to be a limitation, but having no strict relationship between tables has some great benefits. like, Without table joins, this makes the table queries highly efficient Allow provision appropriate capacity for each tables, so we can get predictable performance","title":"Terminologies"},{"location":"Notes/? DynamoDB/02 Get Started/03 Naming Convention/","text":"Naming Convention In most SQL/NoSQL, database is the top label entity. Inside the database, we create multiple tables. But in DynamoDB, tables are the top label label entity. All the tables in a aws region consider as a single database. In order to better organize our tables with related organization, a good approach is to follow some naming convention. We can use any of the followings for a table name, projectName.tableName projectName_TableName","title":"03 Naming Convention"},{"location":"Notes/? DynamoDB/02 Get Started/03 Naming Convention/#naming-convention","text":"In most SQL/NoSQL, database is the top label entity. Inside the database, we create multiple tables. But in DynamoDB, tables are the top label label entity. All the tables in a aws region consider as a single database. In order to better organize our tables with related organization, a good approach is to follow some naming convention. We can use any of the followings for a table name, projectName.tableName projectName_TableName","title":"Naming Convention"},{"location":"Notes/? DynamoDB/02 Get Started/04 Data Types/","text":"Data Types DynamoDB supports 3 types of data, Scalar Types: Can be string, number, binary, boolean, null etc. Set Types: Multiple Scaler types / Array of Scaler types Document Types: allows lists and maps Lists : Ordered collection of values, can have multiple data types maps : JSON Documents Good to Know Key or index of any tables can only be scalar types of string, number or binary. String or Binary can not be an empty value When it comes to sets, empty sets are not allowed A set can not contain duplicate values","title":"04 Data Types"},{"location":"Notes/? DynamoDB/02 Get Started/04 Data Types/#data-types","text":"DynamoDB supports 3 types of data, Scalar Types: Can be string, number, binary, boolean, null etc. Set Types: Multiple Scaler types / Array of Scaler types Document Types: allows lists and maps Lists : Ordered collection of values, can have multiple data types maps : JSON Documents Good to Know Key or index of any tables can only be scalar types of string, number or binary. String or Binary can not be an empty value When it comes to sets, empty sets are not allowed A set can not contain duplicate values","title":"Data Types"},{"location":"Notes/? DynamoDB/02 Get Started/05 Consistency Model/","text":"Consistency Model When we write data in DynamoDB, it initially write the data in a single AZ and then replicate it in at least two other AZ. In this context, DynamoDB provide 2 types of consistency, Strong Consistency : When we fetch data after being write in the DynamoDB table, we will always get the up-to-date data. Eventual Consistency : Fetching data after being written, may not always give the latest result. In DynamoDB, with eventual consistency, data used to replicate in nearly 1/2 seconds. With applications context, this 1/2 second(s) can make huge difference. Also, another thing to keep in mind, eventual consistency have almost half of the pricing compare to the strongly consistency model.","title":"05 Consistency Model"},{"location":"Notes/? DynamoDB/02 Get Started/05 Consistency Model/#consistency-model","text":"When we write data in DynamoDB, it initially write the data in a single AZ and then replicate it in at least two other AZ. In this context, DynamoDB provide 2 types of consistency, Strong Consistency : When we fetch data after being write in the DynamoDB table, we will always get the up-to-date data. Eventual Consistency : Fetching data after being written, may not always give the latest result. In DynamoDB, with eventual consistency, data used to replicate in nearly 1/2 seconds. With applications context, this 1/2 second(s) can make huge difference. Also, another thing to keep in mind, eventual consistency have almost half of the pricing compare to the strongly consistency model.","title":"Consistency Model"},{"location":"Notes/? DynamoDB/02 Get Started/06 Capacity Units/","text":"Capacity Units DynamoDB tables are isolated from each other and hence, we can tune the performance of them individually. We tune the performance using the Throughput Capacity . This throughput capacity allows dynamoDB to ensure predictable performance for the tables according to our needs. Before we go to the Throughput Capacity , we need to look out the Capacity Units . In DynamoDB, there are two types of capacity units, RCU : Read capacity units. 1 RCU stands for 1 strongly consistent read per second or 2 eventually consistency read per second up to 4KB of item data. WCU : Write capacity units. 1 WCU stands for 1 write per second with 1KB of item data. DynamoDB pricing depends on these RCUs and WCUs . We have to pay AWS by the number of RCU and WCU . DynamoDB has a feature called Burst Capacity , which can be used when the application exceeds the Throughput Capacity . A partition can have maximum 1000 WCUs and 3000 RCUs. A table can have multiple partitions. So table is not bound by the partitions capacity units. With on demand capacity model, we do not event have to define the throughput capacity. Using on demand caoacity model can be more pricy.","title":"06 Capacity Units"},{"location":"Notes/? DynamoDB/02 Get Started/06 Capacity Units/#capacity-units","text":"DynamoDB tables are isolated from each other and hence, we can tune the performance of them individually. We tune the performance using the Throughput Capacity . This throughput capacity allows dynamoDB to ensure predictable performance for the tables according to our needs. Before we go to the Throughput Capacity , we need to look out the Capacity Units . In DynamoDB, there are two types of capacity units, RCU : Read capacity units. 1 RCU stands for 1 strongly consistent read per second or 2 eventually consistency read per second up to 4KB of item data. WCU : Write capacity units. 1 WCU stands for 1 write per second with 1KB of item data. DynamoDB pricing depends on these RCUs and WCUs . We have to pay AWS by the number of RCU and WCU . DynamoDB has a feature called Burst Capacity , which can be used when the application exceeds the Throughput Capacity . A partition can have maximum 1000 WCUs and 3000 RCUs. A table can have multiple partitions. So table is not bound by the partitions capacity units. With on demand capacity model, we do not event have to define the throughput capacity. Using on demand caoacity model can be more pricy.","title":"Capacity Units"},{"location":"Notes/? DynamoDB/02 Get Started/07 Pricing/","text":"DynamoDB Pricing","title":"07 Pricing"},{"location":"Notes/? DynamoDB/02 Get Started/07 Pricing/#dynamodb-pricing","text":"","title":"DynamoDB Pricing"},{"location":"Notes/? DynamoDB/02 Get Started/08 Partitions/","text":"Partitions DynamoDB tables store the data inside partitions. These partitions are nothing but the memory blocks. Partitions are managed by the AWS but we can influenced AWS on partitioning the data to tune performance of the table. Partitions Spec Can store at most 10 GB of data Can perform maximum 1000 WCUs or 3000 RCUs In any case, if we exceed these spec, DynamoDB itself will allocate a new partition with zero downtime.","title":"08 Partitions"},{"location":"Notes/? DynamoDB/02 Get Started/08 Partitions/#partitions","text":"DynamoDB tables store the data inside partitions. These partitions are nothing but the memory blocks. Partitions are managed by the AWS but we can influenced AWS on partitioning the data to tune performance of the table. Partitions Spec Can store at most 10 GB of data Can perform maximum 1000 WCUs or 3000 RCUs In any case, if we exceed these spec, DynamoDB itself will allocate a new partition with zero downtime.","title":"Partitions"},{"location":"Notes/? DynamoDB/02 Get Started/09 Indexes/","text":"Indexes A DynamoDB table can have two types of primary key, Simple primary key: Consist of a HASH key or partition key Composite primary key: Consist of two keys, HASH or partition key and RANGE or sort key Partition keys used to determine, in which partition the item should be placed.","title":"09 Indexes"},{"location":"Notes/? DynamoDB/02 Get Started/09 Indexes/#indexes","text":"A DynamoDB table can have two types of primary key, Simple primary key: Consist of a HASH key or partition key Composite primary key: Consist of two keys, HASH or partition key and RANGE or sort key Partition keys used to determine, in which partition the item should be placed.","title":"Indexes"},{"location":"Notes/? DynamoDB/02 Get Started/10 LSI & GSI/","text":"LSI & GSI DynamoDB requires us to specify the primary key for all operations and hence provide faster performance. We can scan on tables without primary key and it is not efficient and should avoid as much as possible. LSI : Stands for local secondary index. We must create the LSI whenever we create table. After a table being created, we can not update the LSI. We can create up to 5 local secondary indexes. LSI supports both the eventual consistency and strongly consistency. GSI : Stands for global secondary index. A global secondary index can be created with entirely new partition key and sort key. Global secondary index took a new partition and hence it needs a completely new throughput capacity for the RCU and WCU. Also this GSI can be created anytime, not bound to the table creation time like the LSI.","title":"10 LSI & GSI"},{"location":"Notes/? DynamoDB/02 Get Started/10 LSI & GSI/#lsi-gsi","text":"DynamoDB requires us to specify the primary key for all operations and hence provide faster performance. We can scan on tables without primary key and it is not efficient and should avoid as much as possible. LSI : Stands for local secondary index. We must create the LSI whenever we create table. After a table being created, we can not update the LSI. We can create up to 5 local secondary indexes. LSI supports both the eventual consistency and strongly consistency. GSI : Stands for global secondary index. A global secondary index can be created with entirely new partition key and sort key. Global secondary index took a new partition and hence it needs a completely new throughput capacity for the RCU and WCU. Also this GSI can be created anytime, not bound to the table creation time like the LSI.","title":"LSI &amp; GSI"},{"location":"Notes/? DynamoDB/02 Get Started/11 Interacting With DynamoDB/","text":"Interacting With DynamoDB DynamoDB allows 3 ways to interact with it, AWS Console AWS CLI AWS sdk","title":"11 Interacting With DynamoDB"},{"location":"Notes/? DynamoDB/02 Get Started/11 Interacting With DynamoDB/#interacting-with-dynamodb","text":"DynamoDB allows 3 ways to interact with it, AWS Console AWS CLI AWS sdk","title":"Interacting With DynamoDB"},{"location":"Notes/? DynamoDB/03 Interact DynamoDB with AWS Console/01 Table Create Operation/","text":"Table Create Operation While creating a table, it is recommended to use a string as primary key rather than number, since string has much more variety compare to number. For sort key, it is recommended to use a number. These are not rules, just some recommendation. When we create a table, we must include the local secondary index. We can not delete/alter/change these indexes after a table being created. All local secondary index should have the same partition key. Using the Projected Attributes we can determine which item attribute will be stored in the indexing other than the partition key and the sort key. With includes type Project Attributes , we can define, which attributes should be included in the indexing With All type of Project Attributes , all the item attributes will be included in the indexing With Keys type of the Project Attributes , only the partition key and sort key will be included in the indexing. By selecting proper types of Projected Attributes we can reduce the size of the table.","title":"01 Table Create Operation"},{"location":"Notes/? DynamoDB/03 Interact DynamoDB with AWS Console/01 Table Create Operation/#table-create-operation","text":"While creating a table, it is recommended to use a string as primary key rather than number, since string has much more variety compare to number. For sort key, it is recommended to use a number. These are not rules, just some recommendation. When we create a table, we must include the local secondary index. We can not delete/alter/change these indexes after a table being created. All local secondary index should have the same partition key. Using the Projected Attributes we can determine which item attribute will be stored in the indexing other than the partition key and the sort key. With includes type Project Attributes , we can define, which attributes should be included in the indexing With All type of Project Attributes , all the item attributes will be included in the indexing With Keys type of the Project Attributes , only the partition key and sort key will be included in the indexing. By selecting proper types of Projected Attributes we can reduce the size of the table.","title":"Table Create Operation"},{"location":"Notes/? DynamoDB/03 Interact DynamoDB with AWS Console/02 Item CRUD/","text":"Item CRUD From ui -> items we can do scan query When we query data in table, we can do some filtering along with it. In DynamoDB, filtering take place after the query is performed and data is retrieved. Adding/removing filtering does not affect the RCU or WCU. For number type, we can query/filter by equal, greater, smaller, not greater, not smaller etc.","title":"02 Item CRUD"},{"location":"Notes/? DynamoDB/03 Interact DynamoDB with AWS Console/02 Item CRUD/#item-crud","text":"From ui -> items we can do scan query When we query data in table, we can do some filtering along with it. In DynamoDB, filtering take place after the query is performed and data is retrieved. Adding/removing filtering does not affect the RCU or WCU. For number type, we can query/filter by equal, greater, smaller, not greater, not smaller etc.","title":"Item CRUD"},{"location":"Notes/? DynamoDB/03 Interact DynamoDB with AWS Console/03 Console/","text":"Console metrics tab are great for checking the RCU and WCU.","title":"03 Console"},{"location":"Notes/? DynamoDB/03 Interact DynamoDB with AWS Console/03 Console/#console","text":"metrics tab are great for checking the RCU and WCU.","title":"Console"},{"location":"Notes/? DynamoDB/04 Interact DynamoDB with AWS CLI/01 Manipulating Table/","text":"Manipulating Table Table List To get list of tables using, aws dynamodb list-tables This should give the list of tables already being created. Table Details To see a specific table metadata, we can use, aws dynamodb describe-table --table-name table_name This should return all the metadata of the table, like attributes, types, indexes etc. Create Table aws dynamodb create-table \\ --table-name td_notes_test \\ --attribute-definitions \\ AttributeName=user_id,AttributeType=S \\ AttributeName=timestamp,AttributeType=N \\ --key-schema \\ AttributeName=user_id,KeyType=HASH \\ AttributeName=timestamp,KeyType=RANGE \\ --provisioned-throughput \\ ReadCapacityUnits=1,WriteCapacityUnits=1 In response we should get TableStatus: \"CREATING\" in json format. We can verify table, aws dynamodb describe-table --table-name td_notes_test Also the table should be listed in the following commands output, aws dynamodb list-tables Delete a Table We can delete a table by the name, aws dynamodb delete-table --table-name td_notes_test In response we should get TableStatus: \"DELETING\" in json format.","title":"01 Manipulating Table"},{"location":"Notes/? DynamoDB/04 Interact DynamoDB with AWS CLI/01 Manipulating Table/#manipulating-table","text":"Table List To get list of tables using, aws dynamodb list-tables This should give the list of tables already being created. Table Details To see a specific table metadata, we can use, aws dynamodb describe-table --table-name table_name This should return all the metadata of the table, like attributes, types, indexes etc. Create Table aws dynamodb create-table \\ --table-name td_notes_test \\ --attribute-definitions \\ AttributeName=user_id,AttributeType=S \\ AttributeName=timestamp,AttributeType=N \\ --key-schema \\ AttributeName=user_id,KeyType=HASH \\ AttributeName=timestamp,KeyType=RANGE \\ --provisioned-throughput \\ ReadCapacityUnits=1,WriteCapacityUnits=1 In response we should get TableStatus: \"CREATING\" in json format. We can verify table, aws dynamodb describe-table --table-name td_notes_test Also the table should be listed in the following commands output, aws dynamodb list-tables Delete a Table We can delete a table by the name, aws dynamodb delete-table --table-name td_notes_test In response we should get TableStatus: \"DELETING\" in json format.","title":"Manipulating Table"},{"location":"Notes/? DynamoDB/04 Interact DynamoDB with AWS CLI/02 Item Create, Update, Delete Operations/","text":"Item Create, Update, Delete Operations If there is no table named td_notes_test exist, create one by, aws dynamodb create-table \\ --table-name td_notes_test \\ --attribute-definitions \\ AttributeName=user_id,AttributeType=S \\ AttributeName=timestamp,AttributeType=N \\ --key-schema \\ AttributeName=user_id,KeyType=HASH \\ AttributeName=timestamp,KeyType=RANGE \\ --provisioned-throughput \\ ReadCapacityUnits=1,WriteCapacityUnits=1 To add or replace an item we use the put operation. To update a item, we use update operation. Create Item With put operation, we can create a item or completely replace an existing item by matching with the primary key. Create a JSON file, we want to add to the dynamoDB table, named item.json . Our item.json can be very similar to, { \"user_id\": { \"S\": \"uuid_1\" }, \"timestamp\": { \"N\": \"1621865859\" }, \"title\": { \"S\": \"title 1\" }, \"content\": { \"S\": \"content 1\" } } Now put the item to dynamodb td_notes_test table by, aws dynamodb put-item --table-name td_notes_test --item file://item.json If we go to dynamoDB console, we should see the new item. Similarly, we can change the item.json file and create new items. If we have a JSON item with same primary key and we do a put operation, it will completely replace all the attributes of the previous one and instead use the new one. Update Item To update a item, we will need three json files, In the key.json file we have to define the primary key of the item. Our key.json be like, { \"user_id\": { \"S\": \"uuid_1\" }, \"timestamp\": { \"N\": \"1621865859\" } } In the attribute-names.json file we have to define the attribute we wants to update with variable start with # . Our attribute-names.json file be like, { \"#t\": \"title\" } In the attribute-values.json file, we have to put the updated content for the attribute. Our attribute-values.json file should be like, { \":t\": { \"S\": \"Updated Variable\" } } Here in attribute-values.json file, the t comes from the attribute-names.json file. Now we update the item by, aws dynamodb update-item --table-name td_notes_test \\ --key file://key.json \\ --update-expression \"SET #t = :t\" \\ --expression-attribute-names file://attribute-names.json \\ --expression-attribute-values file://attribute-values.json Now if we go to console an check the item with primary key { user_id: \"uuid_1\", timestamp: \"1621865859\" } , we should see the title as Updated title Delete Item To delete an item we have to put the primary key in a JSON file. We already have similar JSON file named key.json . So we can simply delete by, aws dynamodb delete-item --table-name td_notes_test --key file://key.json In the console, we should see the item with key.json primary key is deleted. Batch Write Item With batch writing we can perform multiple put or delete operation. Let's create a file for the operation named, items.json . Our items.json file should be similar to, { \"td_notes_test\": [ { \"PutRequest\": { \"Item\": { \"user_id\": { \"S\": \"uuid_1\" }, \"timestamp\": { \"N\": \"1621865861\" }, \"title\": { \"S\": \"title 1\" }, \"content\": { \"S\": \"content 1\" } } } }, { \"PutRequest\": { \"Item\": { \"user_id\": { \"S\": \"uuid_2\" }, \"timestamp\": { \"N\": \"1621865862\" }, \"title\": { \"S\": \"title 2\" }, \"content\": { \"S\": \"content 2\" } } } }, { \"PutRequest\": { \"Item\": { \"user_id\": { \"S\": \"uuid_3\" }, \"timestamp\": { \"N\": \"1621865863\" }, \"title\": { \"S\": \"title 3\" }, \"content\": { \"S\": \"content 3\" } } } }, { \"DeleteRequest\": { \"Key\": { \"user_id\": { \"S\": \"uuid_d\" }, \"timestamp\": { \"N\": \"123456\" } } } } ] } No do the batch write operation, aws dynamodb batch-write-item --request-items file://items.json With batch write we can update at most 16MB od data.","title":"02 Item Create, Update, Delete Operations"},{"location":"Notes/? DynamoDB/04 Interact DynamoDB with AWS CLI/02 Item Create, Update, Delete Operations/#item-create-update-delete-operations","text":"If there is no table named td_notes_test exist, create one by, aws dynamodb create-table \\ --table-name td_notes_test \\ --attribute-definitions \\ AttributeName=user_id,AttributeType=S \\ AttributeName=timestamp,AttributeType=N \\ --key-schema \\ AttributeName=user_id,KeyType=HASH \\ AttributeName=timestamp,KeyType=RANGE \\ --provisioned-throughput \\ ReadCapacityUnits=1,WriteCapacityUnits=1 To add or replace an item we use the put operation. To update a item, we use update operation. Create Item With put operation, we can create a item or completely replace an existing item by matching with the primary key. Create a JSON file, we want to add to the dynamoDB table, named item.json . Our item.json can be very similar to, { \"user_id\": { \"S\": \"uuid_1\" }, \"timestamp\": { \"N\": \"1621865859\" }, \"title\": { \"S\": \"title 1\" }, \"content\": { \"S\": \"content 1\" } } Now put the item to dynamodb td_notes_test table by, aws dynamodb put-item --table-name td_notes_test --item file://item.json If we go to dynamoDB console, we should see the new item. Similarly, we can change the item.json file and create new items. If we have a JSON item with same primary key and we do a put operation, it will completely replace all the attributes of the previous one and instead use the new one. Update Item To update a item, we will need three json files, In the key.json file we have to define the primary key of the item. Our key.json be like, { \"user_id\": { \"S\": \"uuid_1\" }, \"timestamp\": { \"N\": \"1621865859\" } } In the attribute-names.json file we have to define the attribute we wants to update with variable start with # . Our attribute-names.json file be like, { \"#t\": \"title\" } In the attribute-values.json file, we have to put the updated content for the attribute. Our attribute-values.json file should be like, { \":t\": { \"S\": \"Updated Variable\" } } Here in attribute-values.json file, the t comes from the attribute-names.json file. Now we update the item by, aws dynamodb update-item --table-name td_notes_test \\ --key file://key.json \\ --update-expression \"SET #t = :t\" \\ --expression-attribute-names file://attribute-names.json \\ --expression-attribute-values file://attribute-values.json Now if we go to console an check the item with primary key { user_id: \"uuid_1\", timestamp: \"1621865859\" } , we should see the title as Updated title Delete Item To delete an item we have to put the primary key in a JSON file. We already have similar JSON file named key.json . So we can simply delete by, aws dynamodb delete-item --table-name td_notes_test --key file://key.json In the console, we should see the item with key.json primary key is deleted. Batch Write Item With batch writing we can perform multiple put or delete operation. Let's create a file for the operation named, items.json . Our items.json file should be similar to, { \"td_notes_test\": [ { \"PutRequest\": { \"Item\": { \"user_id\": { \"S\": \"uuid_1\" }, \"timestamp\": { \"N\": \"1621865861\" }, \"title\": { \"S\": \"title 1\" }, \"content\": { \"S\": \"content 1\" } } } }, { \"PutRequest\": { \"Item\": { \"user_id\": { \"S\": \"uuid_2\" }, \"timestamp\": { \"N\": \"1621865862\" }, \"title\": { \"S\": \"title 2\" }, \"content\": { \"S\": \"content 2\" } } } }, { \"PutRequest\": { \"Item\": { \"user_id\": { \"S\": \"uuid_3\" }, \"timestamp\": { \"N\": \"1621865863\" }, \"title\": { \"S\": \"title 3\" }, \"content\": { \"S\": \"content 3\" } } } }, { \"DeleteRequest\": { \"Key\": { \"user_id\": { \"S\": \"uuid_d\" }, \"timestamp\": { \"N\": \"123456\" } } } } ] } No do the batch write operation, aws dynamodb batch-write-item --request-items file://items.json With batch write we can update at most 16MB od data.","title":"Item Create, Update, Delete Operations"},{"location":"Notes/? DynamoDB/04 Interact DynamoDB with AWS CLI/03 Item Read Operation/","text":"Item Read Operation Get item by primary key Query operation by hash key and with sort key Scan operation across table partition Filter data after query Create sample Items For Read Operation First batch write these functions, { \"td_notes_test\": [ { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"A\"}, \"timestamp\": {\"N\": \"1\"}, \"content\": {\"S\": \"Content A1\"}, \"title\": {\"S\": \"Title A1\"}, \"username\": {\"S\": \"User A\"}, \"cat\": {\"S\": \"general\"}, \"note_id\": {\"S\": \"n1\"} } } }, { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"B\"}, \"timestamp\": {\"N\": \"2\"}, \"content\": {\"S\": \"Content B2\"}, \"title\": {\"S\": \"Title B2\"}, \"username\": {\"S\": \"User B\"}, \"cat\": {\"S\": \"general\"}, \"note_id\": {\"S\": \"n2\"} } } }, { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"C\"}, \"timestamp\": {\"N\": \"3\"}, \"content\": {\"S\": \"Content C3\"}, \"title\": {\"S\": \"Title C3\"}, \"username\": {\"S\": \"User C\"}, \"cat\": {\"S\": \"general\"}, \"note_id\": {\"S\": \"n3\"} } } }, { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"A\"}, \"timestamp\": {\"N\": \"4\"}, \"content\": {\"S\": \"Content A4\"}, \"title\": {\"S\": \"Title A4\"}, \"username\": {\"S\": \"User A\"}, \"cat\": {\"S\": \"test\"}, \"note_id\": {\"S\": \"n4\"} } } }, { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"B\"}, \"timestamp\": {\"N\": \"5\"}, \"content\": {\"S\": \"Content B5\"}, \"title\": {\"S\": \"Title B5\"}, \"username\": {\"S\": \"User B\"}, \"cat\": {\"S\": \"test\"}, \"note_id\": {\"S\": \"n5\"} } } }, { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"C\"}, \"timestamp\": {\"N\": \"6\"}, \"content\": {\"S\": \"Content C6\"}, \"title\": {\"S\": \"Title C6\"}, \"username\": {\"S\": \"User C\"}, \"cat\": {\"S\": \"test\"}, \"note_id\": {\"S\": \"n6\"} } } }, { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"A\"}, \"timestamp\": {\"N\": \"7\"}, \"content\": {\"S\": \"Content A7\"}, \"title\": {\"S\": \"Title A7\"}, \"username\": {\"S\": \"User A\"}, \"cat\": {\"S\": \"todo\"}, \"note_id\": {\"S\": \"n7\"} } } }, { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"B\"}, \"timestamp\": {\"N\": \"8\"}, \"content\": {\"S\": \"Content B8\"}, \"title\": {\"S\": \"Title B8\"}, \"username\": {\"S\": \"User B\"}, \"cat\": {\"S\": \"todo\"}, \"note_id\": {\"S\": \"n8\"} } } }, { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"C\"}, \"timestamp\": {\"N\": \"9\"}, \"content\": {\"S\": \"Content C9\"}, \"title\": {\"S\": \"Title C9\"}, \"username\": {\"S\": \"User C\"}, \"cat\": {\"S\": \"todo\"}, \"note_id\": {\"S\": \"n9\"} } } }, { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"A\"}, \"timestamp\": {\"N\": \"10\"}, \"content\": {\"S\": \"Content A10\"}, \"title\": {\"S\": \"Title A10\"}, \"username\": {\"S\": \"User A\"}, \"cat\": {\"S\": \"aws\"}, \"note_id\": {\"S\": \"n10\"} } } } ] } Read By Primary Key To read a item by primary key, we can first create a file named read-key.json , and the read-key.json file will be like, { \"user_id\": { \"S\": \"A\" }, \"timestamp\": { \"N\": \"1\" } } Now get the item using, aws dynamodb get-item --table-name td_notes_test --key file://read-key.json We should get the item. Query Data Lets we want to query all the items with partition key A . Now we need a file for expression attribute values. Create a file named expression-attribute-values.json , similar like, { \":uid\": { \"S\": \"A\" } } Now execute the query by, aws dynamodb query --table-name td_notes_test --key-condition-expression \"user_id = :uid\" --expression-attribute-value file://expression-attribute-values.json We should get all the 4 items, all with partition key A . Now if we want all the items that have partition key A and sort key timestamp are greater than 5 , create a new expression-attribute-values-for-comparison.json file, { \":uid\": { \"S\": \"A\" }, \":t\": { \"N\": 5 } } Since we have the sort key named timestamp , it is a reserved keyword in the dynamodb. In this case, we have to use another file to use this key. The file name can be expression-attribute-names-for-comparison.json similar to, { \"#t\": \"timestamp\" } Now run the query, aws dynamodb query --table-name td_notes_test --key-condition-expression \"user_id = :uid AND #t > :t\" --expression-attribute-value file://expression-attribute-values-for-comparison.json --expression-attribute-names file://expression-attribute-names-for-comparison.json Filtering Data Lets do a filtering on category, to get all the items with todo category. Lets create expression-attribute-values-for-filtering.json , { \":uid\": { \"S\": \"A\" }, \":t\": { \"N\": \"5\" }, \":cat\": { \"S\": \"todo\" } } We will use the previous query and use the filtering on the top of it, aws dynamodb query --table-name td_notes_test --key-condition-expression \"user_id = :uid AND #t > :t\" --expression-attribute-value file://expression-attribute-values-for-filtering.json --expression-attribute-names file://expression-attribute-names-for-comparison.json --filter-expression \"cat = :cat\" With additional flag, --return-consumed-capacity we can see the consumed capacity of the query. With --consistent-read we can do strongly consistent. Multiple Get Items Batch Query Scan Operation Scan operation simply go through the whole table and find data. In a single scan, we can get at most 1MB data. To get more data, we have to scan again with last scan result index. A simple scan can be, aws dynamodb scan --table-name td_notes_test We can also add a filtering after the scan. To do so, first create a file, scan-filtering-attribute-values.json , { \":uname\": { \"S\": \"User B\" } } Now run the scan + filtering, aws dynamodb scan --table-name td_notes_test --filter-expression \"username = :uname\" --expression-attribute-values file://scan-filtering-attribute-values.json","title":"03 Item Read Operation"},{"location":"Notes/? DynamoDB/04 Interact DynamoDB with AWS CLI/03 Item Read Operation/#item-read-operation","text":"Get item by primary key Query operation by hash key and with sort key Scan operation across table partition Filter data after query Create sample Items For Read Operation First batch write these functions, { \"td_notes_test\": [ { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"A\"}, \"timestamp\": {\"N\": \"1\"}, \"content\": {\"S\": \"Content A1\"}, \"title\": {\"S\": \"Title A1\"}, \"username\": {\"S\": \"User A\"}, \"cat\": {\"S\": \"general\"}, \"note_id\": {\"S\": \"n1\"} } } }, { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"B\"}, \"timestamp\": {\"N\": \"2\"}, \"content\": {\"S\": \"Content B2\"}, \"title\": {\"S\": \"Title B2\"}, \"username\": {\"S\": \"User B\"}, \"cat\": {\"S\": \"general\"}, \"note_id\": {\"S\": \"n2\"} } } }, { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"C\"}, \"timestamp\": {\"N\": \"3\"}, \"content\": {\"S\": \"Content C3\"}, \"title\": {\"S\": \"Title C3\"}, \"username\": {\"S\": \"User C\"}, \"cat\": {\"S\": \"general\"}, \"note_id\": {\"S\": \"n3\"} } } }, { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"A\"}, \"timestamp\": {\"N\": \"4\"}, \"content\": {\"S\": \"Content A4\"}, \"title\": {\"S\": \"Title A4\"}, \"username\": {\"S\": \"User A\"}, \"cat\": {\"S\": \"test\"}, \"note_id\": {\"S\": \"n4\"} } } }, { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"B\"}, \"timestamp\": {\"N\": \"5\"}, \"content\": {\"S\": \"Content B5\"}, \"title\": {\"S\": \"Title B5\"}, \"username\": {\"S\": \"User B\"}, \"cat\": {\"S\": \"test\"}, \"note_id\": {\"S\": \"n5\"} } } }, { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"C\"}, \"timestamp\": {\"N\": \"6\"}, \"content\": {\"S\": \"Content C6\"}, \"title\": {\"S\": \"Title C6\"}, \"username\": {\"S\": \"User C\"}, \"cat\": {\"S\": \"test\"}, \"note_id\": {\"S\": \"n6\"} } } }, { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"A\"}, \"timestamp\": {\"N\": \"7\"}, \"content\": {\"S\": \"Content A7\"}, \"title\": {\"S\": \"Title A7\"}, \"username\": {\"S\": \"User A\"}, \"cat\": {\"S\": \"todo\"}, \"note_id\": {\"S\": \"n7\"} } } }, { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"B\"}, \"timestamp\": {\"N\": \"8\"}, \"content\": {\"S\": \"Content B8\"}, \"title\": {\"S\": \"Title B8\"}, \"username\": {\"S\": \"User B\"}, \"cat\": {\"S\": \"todo\"}, \"note_id\": {\"S\": \"n8\"} } } }, { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"C\"}, \"timestamp\": {\"N\": \"9\"}, \"content\": {\"S\": \"Content C9\"}, \"title\": {\"S\": \"Title C9\"}, \"username\": {\"S\": \"User C\"}, \"cat\": {\"S\": \"todo\"}, \"note_id\": {\"S\": \"n9\"} } } }, { \"PutRequest\": { \"Item\": { \"user_id\": {\"S\": \"A\"}, \"timestamp\": {\"N\": \"10\"}, \"content\": {\"S\": \"Content A10\"}, \"title\": {\"S\": \"Title A10\"}, \"username\": {\"S\": \"User A\"}, \"cat\": {\"S\": \"aws\"}, \"note_id\": {\"S\": \"n10\"} } } } ] } Read By Primary Key To read a item by primary key, we can first create a file named read-key.json , and the read-key.json file will be like, { \"user_id\": { \"S\": \"A\" }, \"timestamp\": { \"N\": \"1\" } } Now get the item using, aws dynamodb get-item --table-name td_notes_test --key file://read-key.json We should get the item. Query Data Lets we want to query all the items with partition key A . Now we need a file for expression attribute values. Create a file named expression-attribute-values.json , similar like, { \":uid\": { \"S\": \"A\" } } Now execute the query by, aws dynamodb query --table-name td_notes_test --key-condition-expression \"user_id = :uid\" --expression-attribute-value file://expression-attribute-values.json We should get all the 4 items, all with partition key A . Now if we want all the items that have partition key A and sort key timestamp are greater than 5 , create a new expression-attribute-values-for-comparison.json file, { \":uid\": { \"S\": \"A\" }, \":t\": { \"N\": 5 } } Since we have the sort key named timestamp , it is a reserved keyword in the dynamodb. In this case, we have to use another file to use this key. The file name can be expression-attribute-names-for-comparison.json similar to, { \"#t\": \"timestamp\" } Now run the query, aws dynamodb query --table-name td_notes_test --key-condition-expression \"user_id = :uid AND #t > :t\" --expression-attribute-value file://expression-attribute-values-for-comparison.json --expression-attribute-names file://expression-attribute-names-for-comparison.json Filtering Data Lets do a filtering on category, to get all the items with todo category. Lets create expression-attribute-values-for-filtering.json , { \":uid\": { \"S\": \"A\" }, \":t\": { \"N\": \"5\" }, \":cat\": { \"S\": \"todo\" } } We will use the previous query and use the filtering on the top of it, aws dynamodb query --table-name td_notes_test --key-condition-expression \"user_id = :uid AND #t > :t\" --expression-attribute-value file://expression-attribute-values-for-filtering.json --expression-attribute-names file://expression-attribute-names-for-comparison.json --filter-expression \"cat = :cat\" With additional flag, --return-consumed-capacity we can see the consumed capacity of the query. With --consistent-read we can do strongly consistent. Multiple Get Items Batch Query Scan Operation Scan operation simply go through the whole table and find data. In a single scan, we can get at most 1MB data. To get more data, we have to scan again with last scan result index. A simple scan can be, aws dynamodb scan --table-name td_notes_test We can also add a filtering after the scan. To do so, first create a file, scan-filtering-attribute-values.json , { \":uname\": { \"S\": \"User B\" } } Now run the scan + filtering, aws dynamodb scan --table-name td_notes_test --filter-expression \"username = :uname\" --expression-attribute-values file://scan-filtering-attribute-values.json","title":"Item Read Operation"},{"location":"Notes/? DynamoDB/05 DynamoDB with AWS SDK/01 Get started/","text":"Get started Create a npm project npm init -y Install aws sdk npm install aws-sdk","title":"01 Get started"},{"location":"Notes/? DynamoDB/05 DynamoDB with AWS SDK/01 Get started/#get-started","text":"Create a npm project npm init -y Install aws sdk npm install aws-sdk","title":"Get started"},{"location":"Notes/? DynamoDB/05 DynamoDB with AWS SDK/02 Table Operation/","text":"Table Operation Create Table create a file table-ops.js , const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const ddb = new AWS.DynamoDB(); ddb.listTables({Limit: 10}, function(err, data) { if (err) { console.log(\"Error\", err.code); } else { console.log(\"Table names are \", data.TableNames); } }); If we run this script, node table-ops.js , we should see the list of the tables. Doc for listing tables. Get Table Details To get detail of a table we should pass the table name as params. const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const ddb = new AWS.DynamoDB(); ddb.describeTable({ TableName: 'td_notes_test' }, function(err, data) { if (err) { console.log(\"Error\", err); } else { console.log(\"Success\", JSON.stringify(data, null, 2)); } }); If we run the script, we should see the table details. doc for getting details of the table. Create Table To create a table, we have to pass all the tables definition as params. const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const params = { AttributeDefinitions: [ { AttributeName: 'user_id', AttributeType: 'S' }, { AttributeName: 'timestamp', AttributeType: 'N' } ], KeySchema: [ { AttributeName: 'user_id', KeyType: 'HASH' }, { AttributeName: 'timestamp', KeyType: 'RANGE' } ], TableName: 'td_notes_sdk', ProvisionedThroughput: { ReadCapacityUnits: 1, WriteCapacityUnits: 1 }, }; // Call DynamoDB to create the table ddb.createTable(params, function(err, data) { if (err) { console.log(\"Error\", err); } else { console.log(\"Table Created\", data); } }); If we run the script, we should see a table is being being created. doc for creating a table. Update Table To update table, we have to pass the updated information as params, const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const ddb = new AWS.DynamoDB(); ddb.updateTable({ TableName: 'td_notes_sdk', ProvisionedThroughput: { ReadCapacityUnits: 2, WriteCapacityUnits: 1 }, }, function(err, data) { if (err) { console.log(\"Error\", err); } else { console.log(\"Success\", JSON.stringify(data, null, 2)); } }); As output, we should see the updating status. doc for update dynamoDB table. Delete Table To delete a table, we can simply call delete method with table name as parameter, const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const ddb = new AWS.DynamoDB(); ddb.deleteTable({ TableName: 'td_notes_sdk', }, function(err, data) { if (err) { console.log(\"Error\", err); } else { console.log(\"Success\", JSON.stringify(data, null, 2)); } }); As output we should see, the table is being deleted.","title":"02 Table Operation"},{"location":"Notes/? DynamoDB/05 DynamoDB with AWS SDK/02 Table Operation/#table-operation","text":"Create Table create a file table-ops.js , const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const ddb = new AWS.DynamoDB(); ddb.listTables({Limit: 10}, function(err, data) { if (err) { console.log(\"Error\", err.code); } else { console.log(\"Table names are \", data.TableNames); } }); If we run this script, node table-ops.js , we should see the list of the tables. Doc for listing tables. Get Table Details To get detail of a table we should pass the table name as params. const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const ddb = new AWS.DynamoDB(); ddb.describeTable({ TableName: 'td_notes_test' }, function(err, data) { if (err) { console.log(\"Error\", err); } else { console.log(\"Success\", JSON.stringify(data, null, 2)); } }); If we run the script, we should see the table details. doc for getting details of the table. Create Table To create a table, we have to pass all the tables definition as params. const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const params = { AttributeDefinitions: [ { AttributeName: 'user_id', AttributeType: 'S' }, { AttributeName: 'timestamp', AttributeType: 'N' } ], KeySchema: [ { AttributeName: 'user_id', KeyType: 'HASH' }, { AttributeName: 'timestamp', KeyType: 'RANGE' } ], TableName: 'td_notes_sdk', ProvisionedThroughput: { ReadCapacityUnits: 1, WriteCapacityUnits: 1 }, }; // Call DynamoDB to create the table ddb.createTable(params, function(err, data) { if (err) { console.log(\"Error\", err); } else { console.log(\"Table Created\", data); } }); If we run the script, we should see a table is being being created. doc for creating a table. Update Table To update table, we have to pass the updated information as params, const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const ddb = new AWS.DynamoDB(); ddb.updateTable({ TableName: 'td_notes_sdk', ProvisionedThroughput: { ReadCapacityUnits: 2, WriteCapacityUnits: 1 }, }, function(err, data) { if (err) { console.log(\"Error\", err); } else { console.log(\"Success\", JSON.stringify(data, null, 2)); } }); As output, we should see the updating status. doc for update dynamoDB table. Delete Table To delete a table, we can simply call delete method with table name as parameter, const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const ddb = new AWS.DynamoDB(); ddb.deleteTable({ TableName: 'td_notes_sdk', }, function(err, data) { if (err) { console.log(\"Error\", err); } else { console.log(\"Success\", JSON.stringify(data, null, 2)); } }); As output we should see, the table is being deleted.","title":"Table Operation"},{"location":"Notes/? DynamoDB/05 DynamoDB with AWS SDK/03 Item Write and Delete Operation/","text":"Item Write and Delete Operation AWS DocumentClient is a wrapper class of the DynamoDB class. This wrapper helps to write code without being define the types of attributes. Advantages of DocumentClient class, simplify using the dynamoDB by avoiding unnecessary details interpret dynamoDB native types to javascript types Doc for documentClient Create a node app with named write-ops.js . Create an Item To write an item using javascript sdk, we can use, const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.put({ TableName: 'td_notes_sdk', Item: { user_id: 'bb', timestamp: 1, title: 'my title', content: 'my content' } }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); }); Update Item With Put Method To update an existing item, we have to give the same primary key and updated property. We can update the previous item we created, by keeping the primary key and different contents, const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.put({ TableName: 'td_notes_sdk', Item: { user_id: 'bb', timestamp: 1, title: 'changed title', content: 'changed content' } }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); }); Update Item With Update Method If we use put method, we only provide the updated property values. const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.update({ TableName: 'td_notes_sdk', Key: { user_id: 'bb', timestamp: 2 }, UpdateExpression: 'set #t = :t', ExpressionAttributeNames: { '#t': 'title' }, ExpressionAttributeValues: { ':t': 'Updated Title' } }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); }); Delete an Item To delete an item we simply have to pass the key of the item, const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.delete({ TableName: 'td_notes_sdk', Key: { user_id: 'bb', timestamp: 2 }, }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); }); Batch Write Items We can do both write and delete using the batch write operation, const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.batchWrite({ RequestItems: { 'td_notes_sdk': [ { DeleteRequest: { Key: { user_id: 'bb', timestamp: 1 } } }, { PutRequest: { Item: { user_id: '11', timestamp: 1, title: 'Title 11', content: 'Content 11' } } }, { PutRequest: { Item: { user_id: '22', timestamp: 2, title: 'Title 22', content: 'Content 22' } } } ] } }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); });","title":"03 Item Write and Delete Operation"},{"location":"Notes/? DynamoDB/05 DynamoDB with AWS SDK/03 Item Write and Delete Operation/#item-write-and-delete-operation","text":"AWS DocumentClient is a wrapper class of the DynamoDB class. This wrapper helps to write code without being define the types of attributes. Advantages of DocumentClient class, simplify using the dynamoDB by avoiding unnecessary details interpret dynamoDB native types to javascript types Doc for documentClient Create a node app with named write-ops.js . Create an Item To write an item using javascript sdk, we can use, const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.put({ TableName: 'td_notes_sdk', Item: { user_id: 'bb', timestamp: 1, title: 'my title', content: 'my content' } }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); }); Update Item With Put Method To update an existing item, we have to give the same primary key and updated property. We can update the previous item we created, by keeping the primary key and different contents, const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.put({ TableName: 'td_notes_sdk', Item: { user_id: 'bb', timestamp: 1, title: 'changed title', content: 'changed content' } }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); }); Update Item With Update Method If we use put method, we only provide the updated property values. const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.update({ TableName: 'td_notes_sdk', Key: { user_id: 'bb', timestamp: 2 }, UpdateExpression: 'set #t = :t', ExpressionAttributeNames: { '#t': 'title' }, ExpressionAttributeValues: { ':t': 'Updated Title' } }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); }); Delete an Item To delete an item we simply have to pass the key of the item, const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.delete({ TableName: 'td_notes_sdk', Key: { user_id: 'bb', timestamp: 2 }, }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); }); Batch Write Items We can do both write and delete using the batch write operation, const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.batchWrite({ RequestItems: { 'td_notes_sdk': [ { DeleteRequest: { Key: { user_id: 'bb', timestamp: 1 } } }, { PutRequest: { Item: { user_id: '11', timestamp: 1, title: 'Title 11', content: 'Content 11' } } }, { PutRequest: { Item: { user_id: '22', timestamp: 2, title: 'Title 22', content: 'Content 22' } } } ] } }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); });","title":"Item Write and Delete Operation"},{"location":"Notes/? DynamoDB/05 DynamoDB with AWS SDK/04 Conditional Write Operation/","text":"Conditional Write Operation Before we write, we will check, if the items timestamp exists in the table. If not exists, only then we will proceed the write operation. const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.put({ TableName: 'td_notes_sdk', Item: { user_id: 'ABC', timestamp: 1, title: 'Initial Title', content: 'Initial Content' }, ConditionExpression: '#t <> :t', ExpressionAttributeNames: { '#t': 'timestamp' }, ExpressionAttributeValues: { ':t': 1 } }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); }); If we again try to put the item in the table, we will get an error, because, the timestamp 1 now exist. Even if we do not write a item in the table with conditional write, dynamoDB still count this in WCU.","title":"04 Conditional Write Operation"},{"location":"Notes/? DynamoDB/05 DynamoDB with AWS SDK/04 Conditional Write Operation/#conditional-write-operation","text":"Before we write, we will check, if the items timestamp exists in the table. If not exists, only then we will proceed the write operation. const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.put({ TableName: 'td_notes_sdk', Item: { user_id: 'ABC', timestamp: 1, title: 'Initial Title', content: 'Initial Content' }, ConditionExpression: '#t <> :t', ExpressionAttributeNames: { '#t': 'timestamp' }, ExpressionAttributeValues: { ':t': 1 } }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); }); If we again try to put the item in the table, we will get an error, because, the timestamp 1 now exist. Even if we do not write a item in the table with conditional write, dynamoDB still count this in WCU.","title":"Conditional Write Operation"},{"location":"Notes/? DynamoDB/05 DynamoDB with AWS SDK/05 Atomic Counters/","text":"Atomic Counters Atomic counter Increment or decrement values independently All the request will be ordered as they were received New operation takes place only when the previous one is completed Atomic counter can be used to store number of visitors of a site. It is not suitable for high degree of accuracy application, like banking application. const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.update({ TableName: 'td_notes_sdk', Key: { user_id: 'ABC', timestamp: 1 }, UpdateExpression: 'set #v = #v + :incr', ExpressionAttributeNames: { '#v': 'views' }, ExpressionAttributeValues: { ':incr': 1 } }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); });","title":"05 Atomic Counters"},{"location":"Notes/? DynamoDB/05 DynamoDB with AWS SDK/05 Atomic Counters/#atomic-counters","text":"Atomic counter Increment or decrement values independently All the request will be ordered as they were received New operation takes place only when the previous one is completed Atomic counter can be used to store number of visitors of a site. It is not suitable for high degree of accuracy application, like banking application. const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.update({ TableName: 'td_notes_sdk', Key: { user_id: 'ABC', timestamp: 1 }, UpdateExpression: 'set #v = #v + :incr', ExpressionAttributeNames: { '#v': 'views' }, ExpressionAttributeValues: { ':incr': 1 } }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); });","title":"Atomic Counters"},{"location":"Notes/? DynamoDB/05 DynamoDB with AWS SDK/06 Read Operations/","text":"Read Operations Get By Primary Key const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.get({ TableName: 'td_notes_sdk', Key: { user_id: 'A', timestamp: 1, } }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); }); Query Operation Query where user_id is A . const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.query({ TableName: 'td_notes_sdk', KeyConditionExpression: \"user_id = :uid\", ExpressionAttributeValues: { ':uid': 'A' } }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); }); Scan Operation const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.scan({ TableName: 'td_notes_sdk', }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); }); Filter Operation const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.scan({ TableName: 'td_notes_sdk', FilterExpression: \"cat = :cat\", ExpressionAttributeValues: { \":cat\": 'general' } }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); }); Batch Get Getting items from two tables using batch get const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); var params = { RequestItems: { 'td_notes_test': { Keys: [ { user_id: 'A', timestamp: 1 }, { user_id: 'B', timestamp: 2 } ] }, 'td_notes_sdk': { Keys: [ { user_id: 'A', timestamp: 10 } ] } } }; documentClient.batchGet(params, function(err, data) { if (err) console.log(err); else console.log(JSON.stringify(data, null, 2)); });","title":"06 Read Operations"},{"location":"Notes/? DynamoDB/05 DynamoDB with AWS SDK/06 Read Operations/#read-operations","text":"Get By Primary Key const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.get({ TableName: 'td_notes_sdk', Key: { user_id: 'A', timestamp: 1, } }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); }); Query Operation Query where user_id is A . const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.query({ TableName: 'td_notes_sdk', KeyConditionExpression: \"user_id = :uid\", ExpressionAttributeValues: { ':uid': 'A' } }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); }); Scan Operation const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.scan({ TableName: 'td_notes_sdk', }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); }); Filter Operation const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); documentClient.scan({ TableName: 'td_notes_sdk', FilterExpression: \"cat = :cat\", ExpressionAttributeValues: { \":cat\": 'general' } }, (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); }); Batch Get Getting items from two tables using batch get const AWS = require('aws-sdk'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); var params = { RequestItems: { 'td_notes_test': { Keys: [ { user_id: 'A', timestamp: 1 }, { user_id: 'B', timestamp: 2 } ] }, 'td_notes_sdk': { Keys: [ { user_id: 'A', timestamp: 10 } ] } } }; documentClient.batchGet(params, function(err, data) { if (err) console.log(err); else console.log(JSON.stringify(data, null, 2)); });","title":"Read Operations"},{"location":"Notes/? DynamoDB/05 DynamoDB with AWS SDK/07 Pagination/","text":"Pagination DynamoDB does not return more than 1MB data at once. We can make use of pagination in dynamoDB using the LastEvaluatedKey and ExclusiveStartKey . When dynamoDB return partial data due to limit, it provides the LashEvaluatedKey also. We can get next chunk of data using the LastEvaluatedKey as a ExclusiveStartKey for the next request. const AWS = require('aws-sdk'); const async = require('async'); const _ = require('lodash'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); let startKey = []; let results = []; let pages = 0; async.doWhilst( callback => { const params = { TableName: 'td_notes_sdk', Limit: 3 } if (!_.isEmpty(startKey)) { params.ExclusiveStartKey = startKey; } documentClient.scan(params, (err, data) => { if (err) { console.log(err); return callback(err, {}) } if (typeof data.LastEvaluatedKey !== 'undefined') { startKey = data.LastEvaluatedKey; } else { startKey = []; } if (!_.isEmpty(data.Items)) { results = _.union(results, data.Items); } console.log(JSON.stringify(data, null, 2)) pages++; callback(null, results); }); }, // truth test async () => { if (_.isEmpty(startKey)) { return false; } else { return true; } }, // callback method (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); console.log(\"Item Count: \", data.length); console.log('Pages: ', pages); } );","title":"07 Pagination"},{"location":"Notes/? DynamoDB/05 DynamoDB with AWS SDK/07 Pagination/#pagination","text":"DynamoDB does not return more than 1MB data at once. We can make use of pagination in dynamoDB using the LastEvaluatedKey and ExclusiveStartKey . When dynamoDB return partial data due to limit, it provides the LashEvaluatedKey also. We can get next chunk of data using the LastEvaluatedKey as a ExclusiveStartKey for the next request. const AWS = require('aws-sdk'); const async = require('async'); const _ = require('lodash'); AWS.config.update({ region: 'ap-south-1' }); const documentClient = new AWS.DynamoDB.DocumentClient(); let startKey = []; let results = []; let pages = 0; async.doWhilst( callback => { const params = { TableName: 'td_notes_sdk', Limit: 3 } if (!_.isEmpty(startKey)) { params.ExclusiveStartKey = startKey; } documentClient.scan(params, (err, data) => { if (err) { console.log(err); return callback(err, {}) } if (typeof data.LastEvaluatedKey !== 'undefined') { startKey = data.LastEvaluatedKey; } else { startKey = []; } if (!_.isEmpty(data.Items)) { results = _.union(results, data.Items); } console.log(JSON.stringify(data, null, 2)) pages++; callback(null, results); }); }, // truth test async () => { if (_.isEmpty(startKey)) { return false; } else { return true; } }, // callback method (err, data) => { if (err) { console.log(err); return; } console.log(JSON.stringify(data, null, 2)); console.log(\"Item Count: \", data.length); console.log('Pages: ', pages); } );","title":"Pagination"},{"location":"Notes/? sam/01 sam stack/01 Hello World/","text":"Hello World","title":"01 Hello World"},{"location":"Notes/? sam/01 sam stack/01 Hello World/#hello-world","text":"","title":"Hello World"},{"location":"Notes/? sam/02 sam CLI/01 setting up sam CLI/","text":"setting up sam CLI instruction cmd: sha256sum aws-sam-cli-linux-x86_64.zip 01f9d2059c02ace13f3cc1429f6a9a89a6b50e3d339585aab36f395dba0e8ae8 aws-sam-cli-linux-x86_64.zip cmd: unzip aws-sam-cli-linux-x86_64.zip -d sam-installation From home directory --update , since already aws cli is installed cmd: sudo ./sam-installation/install --update cmd: /usr/local/bin/sam --version SAM CLI, version 1.23.0 cmd: sam --version SAM CLI, version 1.23.0","title":"01 setting up sam CLI"},{"location":"Notes/? sam/02 sam CLI/01 setting up sam CLI/#setting-up-sam-cli","text":"instruction cmd: sha256sum aws-sam-cli-linux-x86_64.zip 01f9d2059c02ace13f3cc1429f6a9a89a6b50e3d339585aab36f395dba0e8ae8 aws-sam-cli-linux-x86_64.zip cmd: unzip aws-sam-cli-linux-x86_64.zip -d sam-installation From home directory --update , since already aws cli is installed cmd: sudo ./sam-installation/install --update cmd: /usr/local/bin/sam --version SAM CLI, version 1.23.0 cmd: sam --version SAM CLI, version 1.23.0","title":"setting up sam CLI"},{"location":"Notes/? sam/02 sam CLI/02 Hello World/","text":"Hello World -- cmd: sam init cmd: echo '{}' | sam local invoke HelloWorldFunction {\"statusCode\":200,\"body\":\"{\\\"message\\\":\\\"hello world\\\"}\"} cmd: sam local start-lambda create a node app with aws sdk resource/sam/sam-app/test-app invoking the test.js inside resource/sam/sam-app/test-app will invoke the lambda method and response { StatusCode: 200, Payload: '{\"statusCode\":200,\"body\":\"{\\\\\"message\\\\\":\\\\\"hello world\\\\\"}\"}' }","title":"02 Hello World"},{"location":"Notes/? sam/02 sam CLI/02 Hello World/#hello-world","text":"-- cmd: sam init cmd: echo '{}' | sam local invoke HelloWorldFunction {\"statusCode\":200,\"body\":\"{\\\"message\\\":\\\"hello world\\\"}\"} cmd: sam local start-lambda create a node app with aws sdk resource/sam/sam-app/test-app invoking the test.js inside resource/sam/sam-app/test-app will invoke the lambda method and response { StatusCode: 200, Payload: '{\"statusCode\":200,\"body\":\"{\\\\\"message\\\\\":\\\\\"hello world\\\\\"}\"}' }","title":"Hello World"},{"location":"Notes/? sam/02 sam CLI/03 Running API Gateway/","text":"Running API Gateway cmd: sam local start-api","title":"03 Running API Gateway"},{"location":"Notes/? sam/02 sam CLI/03 Running API Gateway/#running-api-gateway","text":"cmd: sam local start-api","title":"Running API Gateway"},{"location":"Notes/? serverless framework/01 overview/","text":"","title":"01 overview"},{"location":"Notes/? serverless framework/02 installation/","text":"installation npm i serverless sls -v","title":"02 installation"},{"location":"Notes/? serverless framework/02 installation/#installation","text":"npm i serverless sls -v","title":"installation"},{"location":"Notes/? serverless framework/03 Get Started/","text":"plugins: serverless-offline custom: allowedHeaders: - Accept - Content-Type - Content-Length - Authorization - X-Amz-Date - X-Api-Key - X-Amz-Security-Token - X-Amz-User-Agent provider: name: aws runtime: nodejs12.x lambdaHashingVersion: 20201221 region: ap-south-1 stage: dev memorySize: 128 timeout: 5 endpointType: regional environment: TourAssistantTable: ${self:service}-${opt:stage, self:provider.stage}","title":"03 Get Started"},{"location":"Notes/? serverless framework/04 Set Up Git Repository/","text":"Set Up Git Repository make sure create branch master","title":"04 Set Up Git Repository"},{"location":"Notes/? serverless framework/04 Set Up Git Repository/#set-up-git-repository","text":"make sure create branch master","title":"Set Up Git Repository"},{"location":"Notes/? serverless framework/05 Add CI-CD To App/","text":"Add CI/CD To App Create a buildspec.yml , version: 0.2 phases: install: commands: - echo installing serverless... - npm install -g serverless pre_build: commands: - echo installing npm dependencies - npm install build: commands: - echo deployment started on `date` - echo deploying with serverless framework - sls deploy -v -s $ENV_NAME post_build: commands: - echo deployment completed on `date` Set Up Git Repository Set Up Code Pipeline Create a role for code build Go to IAM Go to roles Create role Select code build Add permission Add policy of Administrator Access Put a name Review and create the role Go to Code Pipeline . create code pipeline name tour-assistant-api-code-pipeline Let the role name as it is Source Provider as AWS Code Commit as Github Connect github Allow access Repo name tour-assistant-api Branch name master Build Provider AWS Codebuild Select region ap-south-1 Create Project Set Up Code Build Project name tour-assistant-api-code-build Operating System Ubuntu Runtime Standard Image aws/codebuild/standard:4.0 Role codebuild serverless admin How to create this role From additional config, add ENV_NAME as prod Make sure the stage under provider is ${opt:stage, 'dev'} add AWS_ACCESS_KEY_ID add AWS_DEFAULT_REGION add AWS_SECRET_ACCESS_KEY Continue to Code Pipeline Skip Deploy Stage","title":"05 Add CI CD To App"},{"location":"Notes/? serverless framework/05 Add CI-CD To App/#add-cicd-to-app","text":"Create a buildspec.yml , version: 0.2 phases: install: commands: - echo installing serverless... - npm install -g serverless pre_build: commands: - echo installing npm dependencies - npm install build: commands: - echo deployment started on `date` - echo deploying with serverless framework - sls deploy -v -s $ENV_NAME post_build: commands: - echo deployment completed on `date` Set Up Git Repository Set Up Code Pipeline Create a role for code build Go to IAM Go to roles Create role Select code build Add permission Add policy of Administrator Access Put a name Review and create the role Go to Code Pipeline . create code pipeline name tour-assistant-api-code-pipeline Let the role name as it is Source Provider as AWS Code Commit as Github Connect github Allow access Repo name tour-assistant-api Branch name master Build Provider AWS Codebuild Select region ap-south-1 Create Project Set Up Code Build Project name tour-assistant-api-code-build Operating System Ubuntu Runtime Standard Image aws/codebuild/standard:4.0 Role codebuild serverless admin How to create this role From additional config, add ENV_NAME as prod Make sure the stage under provider is ${opt:stage, 'dev'} add AWS_ACCESS_KEY_ID add AWS_DEFAULT_REGION add AWS_SECRET_ACCESS_KEY Continue to Code Pipeline","title":"Add CI/CD To App"},{"location":"Notes/? serverless framework/05 Add CI-CD To App/#skip-deploy-stage","text":"","title":"Skip Deploy Stage"},{"location":"Notes/? serverless framework/06 Database Schema/","text":"Database Schema service: tour-assistant-api frameworkVersion: '2' plugins: - serverless-offline custom: allowedHeaders: - Accept - Content-Type - Content-Length - Authorization - X-Amz-Date - X-Api-Key - X-Amz-Security-Token - X-Amz-User-Agent provider: name: aws runtime: nodejs12.x lambdaHashingVersion: 20201221 region: ap-south-1 stage: prod memorySize: 128 timeout: 5 endpointType: regional environment: TOUR_ASSISTANT_TABLE: ${self:service}-${opt:stage, self:provider.stage} iamRoleStatements: - Effect: 'Allow' Action: - dynamodb:Query - dynamodb:PutItem - dynamodb:DeleteItem Resource: 'arn:aws:dynamodb:${opt:region, self:provider.region}:*:table/${self:provider.environment.TOUR_ASSISTANT_TABLE}' functions: add-events: handler: api/events/add-event.handler description: POST /event events: - http: path: event method: post cors: origin: '*' headers: ${self:custom.allowedHeaders} get-events: handler: api/events/get-events.handler description: GET /events events: - http: path: events method: get cors: origin: '*' headers: ${self:custom.allowedHeaders} resources: Resources: TourAssistantTable: Type: AWS::DynamoDB::Table DeletionPolicy: Retain Properties: TableName: ${self:provider.environment.TOUR_ASSISTANT_TABLE} AttributeDefinitions: - AttributeName: title AttributeType: S - AttributeName: eventDate AttributeType: N - AttributeName: eventId AttributeType: S - AttributeName: district AttributeType: S KeySchema: - AttributeName: title KeyType: HASH - AttributeName: eventDate KeyType: RANGE ProvisionedThroughput: ReadCapacityUnits: 1 WriteCapacityUnits: 1 GlobalSecondaryIndexes: - IndexName: eventId-index KeySchema: - AttributeName: eventId KeyType: HASH Projection: ProjectionType: ALL ProvisionedThroughput: ReadCapacityUnits: 1 WriteCapacityUnits: 1 - IndexName: district-index KeySchema: - AttributeName: district KeyType: HASH Projection: ProjectionType: ALL ProvisionedThroughput: ReadCapacityUnits: 1 WriteCapacityUnits: 1","title":"06 Database Schema"},{"location":"Notes/? serverless framework/06 Database Schema/#database-schema","text":"service: tour-assistant-api frameworkVersion: '2' plugins: - serverless-offline custom: allowedHeaders: - Accept - Content-Type - Content-Length - Authorization - X-Amz-Date - X-Api-Key - X-Amz-Security-Token - X-Amz-User-Agent provider: name: aws runtime: nodejs12.x lambdaHashingVersion: 20201221 region: ap-south-1 stage: prod memorySize: 128 timeout: 5 endpointType: regional environment: TOUR_ASSISTANT_TABLE: ${self:service}-${opt:stage, self:provider.stage} iamRoleStatements: - Effect: 'Allow' Action: - dynamodb:Query - dynamodb:PutItem - dynamodb:DeleteItem Resource: 'arn:aws:dynamodb:${opt:region, self:provider.region}:*:table/${self:provider.environment.TOUR_ASSISTANT_TABLE}' functions: add-events: handler: api/events/add-event.handler description: POST /event events: - http: path: event method: post cors: origin: '*' headers: ${self:custom.allowedHeaders} get-events: handler: api/events/get-events.handler description: GET /events events: - http: path: events method: get cors: origin: '*' headers: ${self:custom.allowedHeaders} resources: Resources: TourAssistantTable: Type: AWS::DynamoDB::Table DeletionPolicy: Retain Properties: TableName: ${self:provider.environment.TOUR_ASSISTANT_TABLE} AttributeDefinitions: - AttributeName: title AttributeType: S - AttributeName: eventDate AttributeType: N - AttributeName: eventId AttributeType: S - AttributeName: district AttributeType: S KeySchema: - AttributeName: title KeyType: HASH - AttributeName: eventDate KeyType: RANGE ProvisionedThroughput: ReadCapacityUnits: 1 WriteCapacityUnits: 1 GlobalSecondaryIndexes: - IndexName: eventId-index KeySchema: - AttributeName: eventId KeyType: HASH Projection: ProjectionType: ALL ProvisionedThroughput: ReadCapacityUnits: 1 WriteCapacityUnits: 1 - IndexName: district-index KeySchema: - AttributeName: district KeyType: HASH Projection: ProjectionType: ALL ProvisionedThroughput: ReadCapacityUnits: 1 WriteCapacityUnits: 1","title":"Database Schema"},{"location":"Notes/? serverless framework/07 Setting Up Custom Domain/","text":"Setting Up Custom Domain Can be done through the AWS Management console. Here we do it through serverless framework config. Plugin Serverless Domain Manager Only in dev, not in prod yarn add -D serverless-domain-manager Add the domain to the config plugin section. Add a section customDomain under the custom . Already have a domain and hosted zone in Route53 . Will use a sub domain, custom: customDomain: domainName: tour-api.my-tour-assistant.com We also need to add the basePath . A base path is after the domain and before the resource path. For example, in https://something.amazonaws.com/dev/todo , the /dev is the base path. In our case, we will replace the base path with v1 or v2 etc. Updated config should be, custom: customDomain: domainName: tour-api.my-tour-assistant.com basePath: \"v1\" stage: custom: customDomain: domainName: tour-api.my-tour-assistant.com basePath: \"v1\" stage: ${self:provider.stage} Certificate: custom: customDomain: domainName: tour-api.my-tour-assistant.com basePath: \"v1\" stage: ${self:provider.stage} certificateName: my-tour-assistant.com need to create a route53 record, custom: customDomain: domainName: tour-api.my-tour-assistant.com basePath: \"v1\" stage: ${self:provider.stage} certificateName: my-tour-assistant.com createRoute53Record: true Create domain by, sls create_domain In api gateway, under the Custom Domain Names I should see the new domain name being added as well as ssl certificate. Now deploy the serverless stack, sls deploy","title":"07 Setting Up Custom Domain"},{"location":"Notes/? serverless framework/07 Setting Up Custom Domain/#setting-up-custom-domain","text":"Can be done through the AWS Management console. Here we do it through serverless framework config. Plugin Serverless Domain Manager Only in dev, not in prod yarn add -D serverless-domain-manager Add the domain to the config plugin section. Add a section customDomain under the custom . Already have a domain and hosted zone in Route53 . Will use a sub domain, custom: customDomain: domainName: tour-api.my-tour-assistant.com We also need to add the basePath . A base path is after the domain and before the resource path. For example, in https://something.amazonaws.com/dev/todo , the /dev is the base path. In our case, we will replace the base path with v1 or v2 etc. Updated config should be, custom: customDomain: domainName: tour-api.my-tour-assistant.com basePath: \"v1\" stage: custom: customDomain: domainName: tour-api.my-tour-assistant.com basePath: \"v1\" stage: ${self:provider.stage} Certificate: custom: customDomain: domainName: tour-api.my-tour-assistant.com basePath: \"v1\" stage: ${self:provider.stage} certificateName: my-tour-assistant.com need to create a route53 record, custom: customDomain: domainName: tour-api.my-tour-assistant.com basePath: \"v1\" stage: ${self:provider.stage} certificateName: my-tour-assistant.com createRoute53Record: true Create domain by, sls create_domain In api gateway, under the Custom Domain Names I should see the new domain name being added as well as ssl certificate. Now deploy the serverless stack, sls deploy","title":"Setting Up Custom Domain"},{"location":"Notes/resource/sam/sam-app/","text":"sam-app This project contains source code and supporting files for a serverless application that you can deploy with the SAM CLI. It includes the following files and folders. hello-world - Code for the application's Lambda function. events - Invocation events that you can use to invoke the function. hello-world/tests - Unit tests for the application code. template.yaml - A template that defines the application's AWS resources. The application uses several AWS resources, including Lambda functions and an API Gateway API. These resources are defined in the template.yaml file in this project. You can update the template to add AWS resources through the same deployment process that updates your application code. If you prefer to use an integrated development environment (IDE) to build and test your application, you can use the AWS Toolkit. The AWS Toolkit is an open source plug-in for popular IDEs that uses the SAM CLI to build and deploy serverless applications on AWS. The AWS Toolkit also adds a simplified step-through debugging experience for Lambda function code. See the following links to get started. CLion GoLand IntelliJ WebStorm Rider PhpStorm PyCharm RubyMine DataGrip VS Code Visual Studio Deploy the sample application The Serverless Application Model Command Line Interface (SAM CLI) is an extension of the AWS CLI that adds functionality for building and testing Lambda applications. It uses Docker to run your functions in an Amazon Linux environment that matches Lambda. It can also emulate your application's build environment and API. To use the SAM CLI, you need the following tools. SAM CLI - Install the SAM CLI Node.js - Install Node.js 10 , including the NPM package management tool. Docker - Install Docker community edition To build and deploy your application for the first time, run the following in your shell: sam build sam deploy --guided The first command will build the source of your application. The second command will package and deploy your application to AWS, with a series of prompts: Stack Name : The name of the stack to deploy to CloudFormation. This should be unique to your account and region, and a good starting point would be something matching your project name. AWS Region : The AWS region you want to deploy your app to. Confirm changes before deploy : If set to yes, any change sets will be shown to you before execution for manual review. If set to no, the AWS SAM CLI will automatically deploy application changes. Allow SAM CLI IAM role creation : Many AWS SAM templates, including this example, create AWS IAM roles required for the AWS Lambda function(s) included to access AWS services. By default, these are scoped down to minimum required permissions. To deploy an AWS CloudFormation stack which creates or modifies IAM roles, the CAPABILITY_IAM value for capabilities must be provided. If permission isn't provided through this prompt, to deploy this example you must explicitly pass --capabilities CAPABILITY_IAM to the sam deploy command. Save arguments to samconfig.toml : If set to yes, your choices will be saved to a configuration file inside the project, so that in the future you can just re-run sam deploy without parameters to deploy changes to your application. You can find your API Gateway Endpoint URL in the output values displayed after deployment. Use the SAM CLI to build and test locally Build your application with the sam build command. sam-app$ sam build The SAM CLI installs dependencies defined in hello-world/package.json , creates a deployment package, and saves it in the .aws-sam/build folder. Test a single function by invoking it directly with a test event. An event is a JSON document that represents the input that the function receives from the event source. Test events are included in the events folder in this project. Run functions locally and invoke them with the sam local invoke command. sam-app$ sam local invoke HelloWorldFunction --event events/event.json The SAM CLI can also emulate your application's API. Use the sam local start-api to run the API locally on port 3000. sam-app$ sam local start-api sam-app$ curl http://localhost:3000/ The SAM CLI reads the application template to determine the API's routes and the functions that they invoke. The Events property on each function's definition includes the route and method for each path. Events: HelloWorld: Type: Api Properties: Path: /hello Method: get Add a resource to your application The application template uses AWS Serverless Application Model (AWS SAM) to define application resources. AWS SAM is an extension of AWS CloudFormation with a simpler syntax for configuring common serverless application resources such as functions, triggers, and APIs. For resources not included in the SAM specification , you can use standard AWS CloudFormation resource types. Fetch, tail, and filter Lambda function logs To simplify troubleshooting, SAM CLI has a command called sam logs . sam logs lets you fetch logs generated by your deployed Lambda function from the command line. In addition to printing the logs on the terminal, this command has several nifty features to help you quickly find the bug. NOTE : This command works for all AWS Lambda functions; not just the ones you deploy using SAM. sam-app$ sam logs -n HelloWorldFunction --stack-name sam-app --tail You can find more information and examples about filtering Lambda function logs in the SAM CLI Documentation . Unit tests Tests are defined in the hello-world/tests folder in this project. Use NPM to install the Mocha test framework and run unit tests. sam-app$ cd hello-world hello-world$ npm install hello-world$ npm run test Cleanup To delete the sample application that you created, use the AWS CLI. Assuming you used your project name for the stack name, you can run the following: aws cloudformation delete-stack --stack-name sam-app Resources See the AWS SAM developer guide for an introduction to SAM specification, the SAM CLI, and serverless application concepts. Next, you can use AWS Serverless Application Repository to deploy ready to use Apps that go beyond hello world samples and learn how authors developed their applications: AWS Serverless Application Repository main page","title":"sam-app"},{"location":"Notes/resource/sam/sam-app/#sam-app","text":"This project contains source code and supporting files for a serverless application that you can deploy with the SAM CLI. It includes the following files and folders. hello-world - Code for the application's Lambda function. events - Invocation events that you can use to invoke the function. hello-world/tests - Unit tests for the application code. template.yaml - A template that defines the application's AWS resources. The application uses several AWS resources, including Lambda functions and an API Gateway API. These resources are defined in the template.yaml file in this project. You can update the template to add AWS resources through the same deployment process that updates your application code. If you prefer to use an integrated development environment (IDE) to build and test your application, you can use the AWS Toolkit. The AWS Toolkit is an open source plug-in for popular IDEs that uses the SAM CLI to build and deploy serverless applications on AWS. The AWS Toolkit also adds a simplified step-through debugging experience for Lambda function code. See the following links to get started. CLion GoLand IntelliJ WebStorm Rider PhpStorm PyCharm RubyMine DataGrip VS Code Visual Studio","title":"sam-app"},{"location":"Notes/resource/sam/sam-app/#deploy-the-sample-application","text":"The Serverless Application Model Command Line Interface (SAM CLI) is an extension of the AWS CLI that adds functionality for building and testing Lambda applications. It uses Docker to run your functions in an Amazon Linux environment that matches Lambda. It can also emulate your application's build environment and API. To use the SAM CLI, you need the following tools. SAM CLI - Install the SAM CLI Node.js - Install Node.js 10 , including the NPM package management tool. Docker - Install Docker community edition To build and deploy your application for the first time, run the following in your shell: sam build sam deploy --guided The first command will build the source of your application. The second command will package and deploy your application to AWS, with a series of prompts: Stack Name : The name of the stack to deploy to CloudFormation. This should be unique to your account and region, and a good starting point would be something matching your project name. AWS Region : The AWS region you want to deploy your app to. Confirm changes before deploy : If set to yes, any change sets will be shown to you before execution for manual review. If set to no, the AWS SAM CLI will automatically deploy application changes. Allow SAM CLI IAM role creation : Many AWS SAM templates, including this example, create AWS IAM roles required for the AWS Lambda function(s) included to access AWS services. By default, these are scoped down to minimum required permissions. To deploy an AWS CloudFormation stack which creates or modifies IAM roles, the CAPABILITY_IAM value for capabilities must be provided. If permission isn't provided through this prompt, to deploy this example you must explicitly pass --capabilities CAPABILITY_IAM to the sam deploy command. Save arguments to samconfig.toml : If set to yes, your choices will be saved to a configuration file inside the project, so that in the future you can just re-run sam deploy without parameters to deploy changes to your application. You can find your API Gateway Endpoint URL in the output values displayed after deployment.","title":"Deploy the sample application"},{"location":"Notes/resource/sam/sam-app/#use-the-sam-cli-to-build-and-test-locally","text":"Build your application with the sam build command. sam-app$ sam build The SAM CLI installs dependencies defined in hello-world/package.json , creates a deployment package, and saves it in the .aws-sam/build folder. Test a single function by invoking it directly with a test event. An event is a JSON document that represents the input that the function receives from the event source. Test events are included in the events folder in this project. Run functions locally and invoke them with the sam local invoke command. sam-app$ sam local invoke HelloWorldFunction --event events/event.json The SAM CLI can also emulate your application's API. Use the sam local start-api to run the API locally on port 3000. sam-app$ sam local start-api sam-app$ curl http://localhost:3000/ The SAM CLI reads the application template to determine the API's routes and the functions that they invoke. The Events property on each function's definition includes the route and method for each path. Events: HelloWorld: Type: Api Properties: Path: /hello Method: get","title":"Use the SAM CLI to build and test locally"},{"location":"Notes/resource/sam/sam-app/#add-a-resource-to-your-application","text":"The application template uses AWS Serverless Application Model (AWS SAM) to define application resources. AWS SAM is an extension of AWS CloudFormation with a simpler syntax for configuring common serverless application resources such as functions, triggers, and APIs. For resources not included in the SAM specification , you can use standard AWS CloudFormation resource types.","title":"Add a resource to your application"},{"location":"Notes/resource/sam/sam-app/#fetch-tail-and-filter-lambda-function-logs","text":"To simplify troubleshooting, SAM CLI has a command called sam logs . sam logs lets you fetch logs generated by your deployed Lambda function from the command line. In addition to printing the logs on the terminal, this command has several nifty features to help you quickly find the bug. NOTE : This command works for all AWS Lambda functions; not just the ones you deploy using SAM. sam-app$ sam logs -n HelloWorldFunction --stack-name sam-app --tail You can find more information and examples about filtering Lambda function logs in the SAM CLI Documentation .","title":"Fetch, tail, and filter Lambda function logs"},{"location":"Notes/resource/sam/sam-app/#unit-tests","text":"Tests are defined in the hello-world/tests folder in this project. Use NPM to install the Mocha test framework and run unit tests. sam-app$ cd hello-world hello-world$ npm install hello-world$ npm run test","title":"Unit tests"},{"location":"Notes/resource/sam/sam-app/#cleanup","text":"To delete the sample application that you created, use the AWS CLI. Assuming you used your project name for the stack name, you can run the following: aws cloudformation delete-stack --stack-name sam-app","title":"Cleanup"},{"location":"Notes/resource/sam/sam-app/#resources","text":"See the AWS SAM developer guide for an introduction to SAM specification, the SAM CLI, and serverless application concepts. Next, you can use AWS Serverless Application Repository to deploy ready to use Apps that go beyond hello world samples and learn how authors developed their applications: AWS Serverless Application Repository main page","title":"Resources"},{"location":"Notes/resource/serverless/sls-notes-app-ui-without-auth/readme/","text":"Before running this project, kindly run \"npm install\" to install project dependencies","title":"Readme"}]}