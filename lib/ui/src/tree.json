[{"type":"folder","name":"01 Docker Essential","children":[{"type":"file","name":"01 Docker 101","children":null,"path":"../..//01 Docker Essential/01 Docker 101.md","content":"## Docker 101\n\n### What is Docker?\n\n---\n\n> According to wikipedia `Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called containers.` Just like we can manage our application, docker enables us to manage the application infrastructure as well.\n\n### Why Docker?\n\n---\n\nWhile we try to run an existing code base, we often have to troubleshoot environment issues. This could be dependency issue, module installation problem or environment mis-match.\n\nDocker, in its core is trying to fix these problems. Docker is trying to make it super easy and really straight-forward for anyone to run any code-base or software in any pc, desktop or even server.\n\nIn a nutshell\n\n> Docker make it really easy to install and run software without worrying about setup and dependencies.\n\n### Docker Ecosystem\n\n---\n\nDockers ecosystem contains\n\n- Docker Client\n- Docker Server\n- Docker Machine\n- Docker Image\n- Docker Hub\n- Docker Compose\n\n**Docker Image :** Single file with all the dependencies and config required to run a program.\n\n**Docker Container :** Instance of the `Docker Image`.\n\n**Docker Hub :** Repository of free public `Docker Images`, can be downloaded to local machine to use.\n\n**Docker Client :**\n\n- Took the command from the user\n- Do the pre-processing\n- Pass it to the `Docker Server`\n- `Docker Server` do the heavy processing\n\n**An Docker Example :**\n\nAssuming you have already installed docker in your system, let's run,\n\n```bash\ndocker run hello-world\n```\n\n- It imply to run an `container` from the image `hello-world`\n- This `hello-world` is a tiny little program, whose sole purpose is to print `Hello from Docker!`\n- `Docker Server` check the `local image cache`. If it is not exist in the `local image cache` it goes to `Docker Hub` and download the `image`.\n- Finally the `Docker Server` run the `image` as `container` or `image instance`.\n- If we run the same command again and the `image` is already in the cache, It does not download it from the `Docker Hub`.\n"},{"type":"file","name":"02 Docker Container","children":null,"path":"../..//01 Docker Essential/02 Docker Container.md","content":"## Docker Container\n\n### How OS runs on machine?\n\n---\n\n**Kernel :** Most OS has a kernel. It runs the software processes that govern the access between all the programs running on the computer and all the physical hardware connected to the computer.\n\nFor example, If we write a file in physical hard-disk using node.js, it's not node.js that speaking directly to the physical device. Actually node.js make a system call with necessary information to the kernel and the kernel persist the file in the physical storage.\n\nSo the kernel is an intermediary layer that govern the access between the program and physical devices.\n\n**System Call :** The way program pass instruction to the kernel is called system call. These system calls are very much like function invocation. The kernel exposes different endpoint as system call and program uses these system call to perform an operation.\n\nFor example, to write a file, the kernel expose an endpoint as system call. Program like node.js invoke that system call with necessary information as parameters. This parameters can contain the file name, file content etc.\n\n**A Chaos Scenario :** Lets consider a situation, where there are two program that require two types of node.js runtime.\n\n- Program 01 (Require Node.js 8.0)\n- Program 02 (Require Node.js 12.0)\n\nSince, we can not install these 2 versions in runtime, only one program can work at a time.\n\n**A Solution With Namespace :** OS has a feature to use namespace. Using namespace, we can segment hard-disk in multiple parts. For example, to use two versions of Node.js during runtime, one of the segment will contain node.js runtime version of 8.0 and another segment will contain node.js runtime version 12.0\n\nIn this way, we can ensure the `program 01` will use the segment of node.js with 8.0 runtime and the `program 02` will use the segment of node.js 12.0 as runtime.\n\nHere the kernel will determine the segment during system call by the programs. So for `program 01` the kernel drag the system call from segment with 8.0 node js and for `program 02` the kernel will drag the system call from the segment with 12.0 node.js runtime.\n\n> Selecting segment,, using system call, based on the program is called namespacing. This allow isolating resources per processes or a group of processes.\n\nNamespace can be expanded for both cases\n\n- Hardware Elements\n  - Physical Storage Device\n  - Network Devices\n- Software Elements\n  - Inter process communications\n  - Observer and use of processes\n\n**Control Group :** Also known as `cgroup`. This limits the amount of resources that a particular process can use. For example it determine,\n\n- Amount of memory can be used by a process\n- Number of I/O can be used by a process\n- How much CPU a process can use\n- How much Network bandwidth can a process use\n\n**Namespacing Vs cgroup :** `Namespacing` allow to restrict using a resource. While the `cgroup` restrict the amount of resource a process can use.\n\n### The Docker Container\n\n---\n\nDocker container is a group of\n\n- Process or Group of processes\n- Kernel\n- Isolated resource\n\nIn a `container` kernel observes the system call and for a process, guide to the specified `isolated resources`.\n\n> The windows and Mac OS does not have the feature of `namespacing` and `cgroup`. When we install docker in these machines, a linux virtual machine is installed to handle the isolation of resources for the processes.\n\n**Relations Between Image and Container :** An image have the following components\n\n- File System snapshot\n- Startup commands\n\nWhen we take an image and turn it into a container, the kernel isolated the specified resource just for the container. Then the process or file system snapshot takes places in the physical storage.\n\nWhile we run the startup commands, it installed these process from the physical storage and start making the `system call` using the `kernel`.\n"},{"type":"file","name":"03 Docker Lifecycle","children":null,"path":"../..//01 Docker Essential/03 Docker Lifecycle.md","content":"## Docker Lifecycle\n\nWhen we run a docker container using `run` command,\n\n```bash\ndocker run image_name\n```\n\nThen, the `docker run` is equivalent to the following 2 commands:\n\n1. `docker create`\n2. `docker start`\n\nWith `docker create`, the `file system snapshot` of the image is being copied to `isolated physical storage`.\n\nThen with `docker start` we start the `container`.\n\n**Example :**\n\nLet's do the hands on what we are claiming with a image `hello-world`.\n\n```bash\ndocker create hello-world\n```\n\nThis will return the `id` of the created container.\n\nUsing the `id` we can now start the docker.\n\n```bash\ndocker start -a id\n```\n\nThis will give us the output `Hello from Docker!`.\n\nHere the `-a` flag watch out the `container` output and print it in `console`.\n"},{"type":"file","name":"04 Docker Basic Commands","children":null,"path":"../..//01 Docker Essential/04 Docker Basic Commands.md","content":"## Docker Basic Commands\n\n### Creating and run a container from an image\n\n---\n\n```bash\ndocker run hello-world\n```\n\nHere, `docker` is the `docker-client` name, `run` is create and run the `container` and `hello-world` is the `docker image` name.\n\nWith this command we also run the `start-up` script. That `start-up` script is responsible to print the text `Hello from Docker!`.\n\n### Override startup or default command\n\n---\n\nAnytime we execute `docker run` with an image,\n\n- We took the file snapshot to the physical storage\n- Execute the startup script\n\nTo override the default startup command, we can pass our command as 4th parameter.\n\n```bash\ndocker run busybox echo hi there\n```\n\nWe will get the output\n\n```bash\nhi there\n```\n\nHere, `busybox` is the image and `echo hi there` is our over ride start up command.\n\nWe use `busybox` image instead of `hello-world` because `busy box` has the program like `echo`.\n\n### Listing container in local machine\n\n---\n\nCheck the currently running container.\n\n```bash\ndocker ps\n```\n\nIf there is no docker container is running the list will be empty.\n\nLet's run a container for long time and check it `docker ps` works.\n\n```bash\ndocker run busybox ping google.com\n```\n\nThis will run a significant time to measure the latency. In the meantime let's run the listing command\n\n```bash\ndocker ps\n```\n\nNow we should see the `busybox` is running along with it's other properties.\n\nTo check all the containers ever been created in the local machine, we can use a flag `all`.\n\n```bash\ndocker pas --all\n```\n\nThis will output all the containers we been created in local machine.\n\n### Restart stopped docker container\n\n---\n\nWe can restart the docker container in future at any point.\n\nTo do so, we need the `container_id`. We can get the `container_id` by following command\n\n```bash\ndocker ps --all\n```\n\nIn the first column of the table, contains `container_id`.\n\nNow we can start the stopped docker using the following commands\n\n```bash\ndocker start -a cee8f8c62478\n```\n\nHere `-a` watch out the output of the `container` and print it in the `console`.\n\n### Removing stopped containers\n\n---\n\nWe can see all the containers we run before by\n\n```bash\ndocker ps --all\n```\n\nTo remove all these containers along with the build file we can use the following\n\n```bash\ndocker system prune\n```\n\n> With `docker system prune`, we have to re-download all the images we downloaded earlier\n\n### Retrieve the logs of a container\n\n---\n\nIn some scenario, we might need to see the output of the docker container.\n\nFor example,\n\n```bash\n# get ready the busybox container that will print `Hi there!`\ndocker create busybox echo Hi there!\n# start the container\ndocker start container_id\n```\n\nNow the docker provide the output `Hi there`, but since we did not use `-a` flag, the output will not be printed in the console.\n\nTo see logs, we can use the following commands\n\n```bash\ndocker logs container_id\n```\n\nThis will give us the output `Hi there!`.\n\n> `docker logs` does not re-run or restart the docker `container`. It just get the logs emitted from the `container`.\n\n### Stopping a container\n\n---\n\nTo stop a docker container we can use the following method\n\n- `Stop` a container by `docker stop container_id`\n- `Kill` a container by `docker kill container_id`\n\nWhen we use the `stop` command a `SIGTERM` signal is passed to the primary process of the container. It gives the process `10 seconds` to close the process and do the post process, like save files, do logging etc. If the primary process does not close within `10 seconds`, it then pass `SIGKILL` signal that kill the process immediately.\n\nThe `kill` command pass `SIGKILL` signal that kill the process immediately.\n\n> `SIGTERM` stands for `Terminate Signal`. `SIGKILL` stands for `Kill Signal`.\n\n### Multiple commands in container\n\n---\n\nLet's say we need to tweak the `redis-cli`. In this case, if redis server is installed in my local machine, in one terminal, we can start the `redis server`. In another terminal, we can start the `redis-cli`.\n\nWith `docker`, if we install the `redis`, we can start the redis server by the startup command. But in this case, if we go to another terminal window and try to start the `redis-cli`, we can not access it. Because, from a regular machine terminal window, we can not access the redis server that is running in a isolated namespace.\n\n**Example :**\n\nLets run the `redis` container by\n\n```bash\ndocker run redis\n```\n\nThis will install the redis server and start the server also.\n\nNow If we go to another terminal and try to access `redis-cli`, we will get an error.\n\n**Solution :**\n\nTo execute the `redis-cli` inside the `container` we need to follow the skeleton,\n\n```bash\ndocker exec -it container_id redis-cli\n```\n\nThis will execute the `redis-cli` command inside the container and with `-it` flag, we can pass the command through the terminal and get the emitted output by the container inside our terminal.\n\nIn this way we can interact with redis server like\n\n```bash\n127.0.0.1:6379> set myVal 5\n127.0.0.1:6379> get MyVal\n```\n\n> Here the `exec` stands for `Execute`. Without `-it` the `redis-cli` will run inside the `container` but we can not interact using host machine terminal.\n\n> Each process running in the docker or `virtual linux environment` contains `STDIN`, `STDOUT`, `STDERR`. The `STDIN` stands for `Standard In`, `STDOUT` stands for `Standard Output` and `STDERR` stands for `Standard Error`. `-it` is combined of two standalone flag. `-i` stands connect with processes `STDIN` and `-t` ensure formatted text.\n\n### Start a terminal/shell of container context along with docker start\n\n---\n\nMay be we do not want to always use the `exec` command to open a terminal for a container every time we want to execute some command inside the container.\n\nTo open a terminal inside the container we can use the following command\n\n```bash\ndocker exec -it container_id sh\n```\n\nNow a terminal inside the container will be appeared and we can execute all the `unix` command there. For example we can run the previous `redis-cli` command here and interact with `redis-server`.\n\n> `sh` is program execute inside the container, known as `command processor`. Similar programs are `zsh`, `powershell`, `bash` etc. Traditionally most of the container has `sh` program included.\n\nIf we want to run the terminal inside the container on startup, we can use the following\n\n```bash\ndocker run -it image_name sh\n```\n\nThis will start the `container` and also start a terminal in the `container` context.\n\n> This `sh` program will prevent other startup command supposed to be run during the `container` start.\n"},{"type":"file","name":"05 Create Custom Image","children":null,"path":"../..//01 Docker Essential/05 Create Custom Image.md","content":"## Create Custom Image\n\nThroughout the previous notes, we been using `images` created by other engineers. To create our own image, we can do the followings,\n\n- Create a `dockerfile`.\n- Once a `dockerfile` is created, we will pass it to the `docker client`\n- This `docker client` will provide the `image` to the `docker server`\n- The `docker-server` will make the `image`, that we can use\n\n> A `dockerfile` is a plain text file. It contains all the configs along with the startup commands to define how a docker `container` should behave. It determine what programs should be inside the container and how will these program behave during `container` star up.\n\n> `Docker server` does most of heavy lifting while creating a `image`. It takes the docker file, go through the configuration and build the useable `image`\n\nFor each `docker file`, there's always a patter\n\n- A `base image`\n- Some config to install additional programs, dependencies that is required to create and execute the container\n- A start up command, that will be executed on the moment a container start\n\n### Hands On\n\n---\n\nLet's create our own docker `image` that will run a `redis-server`.\nWe will follow the following procedures\n\n1. Define base `image`\n2. Download and install the `dependencies`\n3. Instruct the images initial behaviour\n\nSo,\n\n- First create a file named `Dockerfile`\n\n  ```bash\n  touch Dockerfile\n  ```\n\n- In the `Dockerfile`, add a base image\n\n  ```bash\n  FROM alpine\n  ```\n\n- Download the dependency\n\n  ```bash\n  RUN apk add --update redis\n  ```\n\n- Define the initial command\n\n  ```bash\n  CMD [\"redis-server\"]\n  ```\n\nFinally our `Dockerfile` should be the following\n\n```bash\nFROM alpine\nRUN apk add --update redis\nCMD [\"redis-server\"]\n```\n\nNow let's build the container\n\n```bash\ndocker build .\n```\n\nThis will create the image and return the `image_id`.\n\nNow we can run the `container` from the `image_id` using the followings,\n\n```bash\ndocker run image_id\n```\n\nThis will run the redis server.\n\n> `Dockerfile` is a plain file with no extension\n\n### Dockerfile Teardown\n\n---\n\nWe just going through the process of creating a docker image. But we don't explain what really happen there. Now lets explain what actually we are done inside the `Dockerfile` configuration.\n\nWe ran 3 commands to build the image, and all three has a very similar pattern. Each command like `FROM`, `RUN`, `CMD` are the docker `instruction` and they took some `arguments`.\n\n> The `FROM` instruction specify a `docker image` we want to use as base. While we are preparing our custom image, the `RUN` execute some commands. The `CMD` specify, what should run on startup when our image will be used to create a new `container`.\n\nEvery line of configuration we are going to add inside the `Dockerfile` will always start with a `instruction`\n\n> A base image is an initial set of programs that can be used to to further customize the the image. `Alpine` is a base image that comes with a package manager named `apk` (Apache Package Manager). `apk` can be used to download the `redis-server` and install in the system.\n\nWith `FROM alpine` we are using an initial operating system named `alpine`. This `alpine` operating system has couple of preinstalled program, that are very much useful for what we are trying to accomplish. Since we are here to create `redis server` and we need to install some dependencies, `alpine` has these tools and programs preinstalled like `apk`.\n\nWith `RUN apk add --update redis` we download and install the `redis` server. Here `apk` is nothing related to the `docker`. It's a dependency manager preinstalled in the `base image`, download and install the `redis` server.\n\nThe `CMD [\"redis-server\"]` ensure, when we create a container from this docker `image`, the redis server will be started using `redis-server`command.\n\n### Image build process\n\n---\n\nTo build a image from the `Dockerfile`, we use\n\n```bash\ndocker build .\n```\n\nThis ship our `Dockerfile` to the `docker client`. `build` is responsible for take the `Dockerfile` and build an `image` out of it.\n\nThe `.` is the build context. The build context contains all the set of files and folders that belongs to our project needs to wrap or encapsulate in our docker `container`.\n\nIf we notice the logs of the `docker build .`, we can see except the first instruction `FROM alpine`, every other instruction has an `intermediary container` that being removed automatically.\n\nThis means, except first step, each step took the container from the previous step and step itself acts as the startup instruction. Then when step is done, it simply pass the updated file system snapshot and remove the temporary container from itself.\n\nFor the last step, it took the container from the previous step, and do not run itself as the start-up command. Instead it only set itself as the first instruction and ensure if someone in future create and run the `container` out of the image, it then execute this last step as the `startup instruction`.\n\n> For each step, except first one, we take the `image` from the previous step, create a temporary container, execute instructions, make changes, took the snapshot of the file system and return the file system output as output, so it can be used as the image for the next step. For last step, the final instruction is considered as the `start-up` instruction of the `docker container`.\n\n### Rebuild image from cache\n\n---\n\nLet's update the `Dockerfile` with an additional command\n\n```bash\nRUN apk add --update gcc\n```\n\nNow the `Dockerfile` should be like\n\n```bash\nFROM alpine\nRUN apk add --update redis\nRUN apk add --update gcc\nCMD [\"redis-server\"]\n```\n\nLet's build a image out of this `Dockerfile`,\n\n```bash\ndocker build .\n```\n\nIf we observer the `logs` closely, we see, in the second step, it does not create a `intermediary container` from the previous step. Instead it is using the cache. So we do not need to install the `redis-server` multiple times. This gives the docker robustness and faster build performance.\n\nFrom the instruction `RUN apk add --update gcc` it will create the `intermediary container` and since the file snapshot is being changed, it will do the same from the next steps.\n\nIf we build the image from the same `Dockerfile` for the 3rd time, it will take all the changes from the cache instead of the `intermediary container`.\n\n> An `intermediary container` is created from the previous step image and use to run current instruction, make changes, take the snapshot of the new changed file system and got removed. This snapshot is being used for the next step to create another `intermediary container` and goes on.\n\n> Altering the instruction sequence will not use the cache.\n\n### Tagging a docker image\n\n---\n\nTill now, we have created `image` from the `Dockerfile` and getting an `image_id`.\n\nWhen we want a name instead of an `image_id` we can use the `tagging`.\n\nTo get a name, after a `image` being created, we can use the following\n\n```bash\ndocker build -t user_name/image_name:version_number .\n```\n\nThis will return\n\n```bash\nSuccessfully tagged user_name/image_name:version_number\n```\n\n> Here the `user_name` is the `username`, that is used to login to the `docker-hub`. `image_name` is our desired image name. The `version_number` is the `image` version number. Instead of `version_number` we can use `lated` keyword to use the latest version number handled by the docker itself.\n\nWe can now run the docker with the tag\n\n```bash\ndocker run user_name/image_name:version_number\n```\n\nHere only the `version_number` is the tag itself. The `user_name` itself is always the `docker id` and `image_name` is the project name or the `repo name`.\n\n> While running our tagged custom `image` we can ignore the `version_number`. It will simply take the latest version.\n"},{"type":"file","name":"06 Wrap Web App in Docker","children":null,"path":"../..//01 Docker Essential/06 Wrap Web App in Docker.md","content":"## Wrap Web App in Docker\n\nLet's create a tiny node.js web application and wrap it inside the `docker container`.\n\nThen we should access the app from the browser of the local machine. Right now we will not be worry about deploying the app.\n\n### Create a Node.js app\n\n---\n\nFirst create a directory, enter into it and run\n\n```bash\nnpm init -y\n```\n\nInstall `express.js` package by\n\n```bash\nnpm i express\n```\n\nNow create a file named `index.js` and create a server. `index.js` should be like following\n\n```js\nconst express = require('express');\nconst app = express();\n\n// from browser, a get request in `/` route, should return `Hi There!!`\napp.get('/', (req, res) => res.send('Hi There!!'));\n\n// app will be run on port `8080`\napp.listen(8080);\n```\n\nTime to create a `Dockerfile`. Before we create the `Dockerfile`, we need to consider some concept and instructions.\n\n### Selecting A Base Image\n\n---\n\nA `base image` contains is a collection of preinstalled programs. In the [docker hub](https://hub.docker.com), if we look for the `node` image, we should find images with different version along with `alpine` tag. This `node` images has the `node` and `npm` installed.\n\n> `alpine` is being used in the docker world to signify an image as small and compact as possible.\n\n> `docker hub` is a repository of docker images.\n\nWe can select a `base image` of `node` with the command.\n\n```bash\nFROM node:alpine\n```\n\n### Load Application Files\n\n---\n\nAfter building a container, we can stat the image. As soon as the `container` boots up, `file snapshot` of the `image` is copied to the `namespace` area. And this `file snapshot` is the base image `file snapshot`, does not contains our working directory, like `package.json` or `index.js`. We need to use an `instruction` to take our files to the `container` file systems.\n\nTo copy our local files to the `container` file system, we need to use `COPY` instruction. It takes two arguments, first one is the path of local file system path. The second one is the `container` file path.\n\n```bash\nCOPY ./ ./\n```\n\n### Port Mapping\n\n---\n\nWhen we run a web application in the `container` tha port in the `container` can not be accessed from the `local` machine by default. We need to do a `port mapping` that will define which port from the `local server` direct the traffic to the `container` port.\n\nSo while we start the `container` we have to map the `port`.\n\n```bash\ndocker run -p local_machine_port:container_app_port image_id\n```\n\n### Create A Working Directory\n\nBy default while we copy the file from the local machine to the `container`, in container the files persist on the `root directory`. We might want to put the project files in the separate directory. We can define our working directory by `WORKDIR` instruction. This `WORKDIR` will create if not exist and use as the project directory when we copy the files from the local file system to the `container`.\n\n---\n\n```bash\nWORKDIR /usr/app\n```\n\n### Avoid Unnecessary Builds\n\n---\n\nWe need to be careful, for each file changes, we should not reinstall the packages. We can copy the `package.json` file before the `package installation`. Then we will copy the all other files.\n\nIn this case, for random files changes, the `package installation` will be taken from the cache. Example\n\n```bash\nCOPY ./package.json ./\nRUN npm install\nCOPY ./ ./\n```\n\n### **Final `Docker` File**\n\nWith all these consideration, create a `Dockerfile` in the project root directory. our `Dockerfile` should be,\n\n```bash\nFROM node:alpine\nWORKDIR /usr/app\nCOPY ./package.json ./\nRUN npm install\nCOPY ./ ./\nCMD [\"node\", \"index.js\"]\n```\n\nLet's build a `image` from this `Dockerfile`,\n\n```bash\ndocker build -t bmshamsnahid/myapp .\n```\n\nAfter build the image, let run it with the `tag`,\n\n```bash\n# Import the base image\nFROM node:alpine\n\n# Create a working directory for the app\nWORKDIR /usr/app\n\n# Only if we change the `package.json` file, the npm will install all the modules along with the new one\nCOPY ./package.json ./\nRUN npm install\n\n# Copy all the project files to the `container`\nCOPY ./ ./\n\n# Set the start up command to run the server\nCMD [\"node\", \"index.js\"]\n```\n\nIn the web browser `http://localhost:5000/` should print the following\n\n```\nHi There!!\n```\n\nCongratulations!!! You just ran a node application in docker container.\n"},{"type":"file","name":"07 Using Docker Compose","children":null,"path":"../..//01 Docker Essential/07 Using Docker Compose.md","content":"## Using Docker Compose: Managing Multiple Local Containers\n\n### Docker Compose\n\n---\n\n`docker-compose` helps us to avoid repetitive commands, that we might have to write with `docker-cli` during a `container` start up.\n\nFor example, if we have two `container` and need a networking between them, we have to configure these with `docker-cli` every time we start the `container`.\n\nUsing `docker-compose` we can resolve the issue.\n\n> This `docker-compose` allows us to start up multiple container at the same time in a very easy and straightforward way. Also it will set up some sort of networking between them and all behind the scene.\n\nTo make use of `docker-compose`, we essentially going to the `docker-cli` startup commands of long form and encode these command in `docker-compose.yml` file. We do not entirely copy and paste the start-up commands, instead we use special syntax more or less similar to the start up commands.\n\nAfter creating the `docker-compose.yml` file, we will feed the file to the `docker-compose-cli` to parse the file and create container with our desired configurations.\n\nIn `windows` and `mac` OS the `docker-compose` is shipped with the `docker` installation. But for linux machines, you might need to install the `docker-compose` library separately. To install the `docker-compose` in `Ubuntu 20.04`, that I am using, can follow the [instructions](https://stackoverflow.com/questions/36685980/docker-is-installed-but-docker-compose-is-not-why)\n\n```bash\nsudo curl -L \"https://github.com/docker/compose/releases/download/1.26.0/docker-compose-$(uname -s)-$(uname -m)\"  -o /usr/local/bin/docker-compose\n\nsudo mv /usr/local/bin/docker-compose /usr/bin/docker-compose\n\nsudo chmod +x /usr/bin/docker-compose\n```\n\nFirst instruction download the library, second one move it to `/usr/bin/docker-compose` and using the third instruction, we are giving the `docker-compose` appropriate permissions.\n\n### A Hands On\n\n---\n\nLet's develop a classic docker example.\n\nHere we will create a little docker container that will host a web application. This web application will count the number of visit to that web app.\n\nWe will need a node app that will response on the `HTTP` request and a `redis` server that will count the number of visit. Although `redis` is a in memory server, in this case we will consider itself as our tiny database.\n\nOff course we can use the node server to store the number of visits. To make the container a little bit complex we are using both a `node` server and a `redis` server.\n\nWe can consider a single container with both `node server` and `redis server` in it. But this will create problem on scalability.\n\nFor more traffic, if we increase the number of containers, for each container, there will be individual `node server` and `redis server`. Also each `redis server` will be isolated from each others. So one `redis server` will give us total visit of `10`, another `redis server` will give us total visit of `5`.\n\nSo our actual approach will be both `node server` and `redis server` will be in isolated container. An in case of scaling we will scale the `node-app-container` and all the `node-app-container` will be connected to the single `redis-server-container`.\n\n### Creating The Node Server\n\n---\n\nCreate a project directory named `visits`,\n\n```bash\nmkdir visits\n```\n\nNow go to the directory and create a node project\n\n```bash\ncd visits\nyarn init -y\n```\n\nCreate a file `index.js`\n\n```bash\ntouch index.js\n```\n\nThe `index.js` will be responsible for creating the `node server` and connect with the `redis server` to display the `number of site visits` in the browser on response of a `HTTP` request.\n\nThe code of the `index.js` will be like followings,\n\n```js\n// import required modules\nconst express = require('express');\nconst redis = require('redis');\n\nconst app = express(); // create app instance\nconst client = redis.createClient({\n  host: 'redis-server', // service name of the `redis-server` we are using, will be defined in the `services` section of `docker-compose.yml` file\n  port: 6379 // default port of the `redis-server`\n}); // connect the node server with redis server\nclient.set('visits', 0); // initially set number of visits to 0\n\napp.get('/', (req, res) => {\n  client.get('visits', (err, visits) => {\n    res.send(`Number of visits: ${visits}`); // in browser, showing the client, number of visits\n    client.set('visits', parseInt(visits) + 1); // increase the number visits\n  });\n});\n\nconst PORT = 8081; // determine node server port no\n\n// run the server\napp.listen(PORT, () => console.log(`App is listening on port: ${PORT}`));\n```\n\nExcept the redis connection on line\n\n```js\nconst client = redis.createClient();\n```\n\nHere we will have to put necessary networking config of the redis server.\n\n### Assembling `Dockerfile` to Node Server\n\n---\n\nOur `Dockerfile` will be very simple to just run the node server\n\n```bash\n# define base image\nFROM node:alpine\n\n# define working directory inside the container\nWORKDIR /app\n\n# Copy the package.json file to the project directory\nCOPY package.json .\n# install all the dependencies\nRUN npm install\n\n# Copy all the source code from host machine to the container project directory\nCOPY . .\n\n# define the start up command of the container to run the server\nCMD [\"node\", \"index.js\"]\n```\n\nNow let's build the image,\n\n```bash\ndocker build -t docker_user_id/repo_name:latest .\n```\n\nThis will create the `image` of our `node-app` named `docker_user_id/repo_name`.\n\nNow if we try to run the `node-app` (Although it will throw error, because `redis` server is not running yet),\n\n```bash\ndocker run docker_user_id/repo_name\n```\n\nHere, we will get an error message,\n\n```bash\node:events:356\n      throw er; // Unhandled 'error' event\n      ^\n\nError: connect ECONNREFUSED 127.0.0.1:6379\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1139:16)\nEmitted 'error' event on RedisClient instance at:\n    at RedisClient.on_error (/app/node_modules/redis/index.js:406:14)\n    at Socket.<anonymous> (/app/node_modules/redis/index.js:279:14)\n    at Socket.emit (node:events:379:20)\n    at emitErrorNT (node:internal/streams/destroy:188:8)\n    at emitErrorCloseNT (node:internal/streams/destroy:153:3)\n    at processTicksAndRejections (node:internal/process/task_queues:81:21) {\n  errno: -111,\n  code: 'ECONNREFUSED',\n  syscall: 'connect',\n  address: '127.0.0.1',\n  port: 6379\n}\n```\n\nIn summary, it says, the `node-app` can not connect to the `redis-server`, as expected. We will fix it now.\n\n### `Redis` Server\n\nWe can use vanilla `redis` image from `docker-hub`. We will simply run the `redis-server` by\n\n```bash\ndocker run redis\n```\n\nEven with running the `redis-server`, if we run the `node-app` again, we will get the same error as before.\n\nSince both `node-app` and `redis-server` is in isolated container and there is no networking communication between them, the `node-app` will not be able to communicate with the `redis-server`.\n\n### Bring The `Docker Compose`\n\n---\n\nWith `docker-compose` first we do the following configurations:\n\n- For `redis-server` make use of the `redis` image\n- For `node-app` make use of the `Dockerfile`\n- Also for the `node-app` map port `8081` from local machine port `4001`\n\nBy defining multiple services in the `docker-compose`, docker will put all the services essentially the same network. And as a result the containers can access each other freely.\n\n> There are different versions of `docker-compose`. Here we will use version `3` as our `docker-version`.\n\n> In the `docker-compose` the `redis-server` and the `node-app` are considered as `services`.\n\n> For a `service` in the `docker-compose`, we have to define how we get the image. It could be an image from the `docker-hub` or from the `Dockerfile` we wrote. For `redis-server` we will use `docker-hub` image and for the `node-app` we will use our made up `Dockerfile`\n\n> For a `service` we can do the port mapping between local machine and container.\n\n> For a `service` we can use a restart policy, discussed in details in the `maintenance` section.\n\nTo do so, in the project directory, first create the `docker-compose.yml` file\n\n```bash\ntouch docker-compose.yml\n```\n\nNow our `docker-compose.yml` file be\n\n```yml\nversion: '3'\nservices:\n  redis-server:\n    image: 'redis'\n  node-app:\n    restart: always\n    build: .\n    ports:\n      - '4001:8081'\n```\n\nWe used to run the container by `docker run my_image_name`, which is similar to `docker-compose up`.\n\nWe used 2 commands, one for `build` the image and another for `run` the container. The `docker-compose up --build` is similar to the followings existing commands,\n\n```bash\ndocker build .\ndocker run my_image_name\n```\n\nSo to build and run our two docker image we can use the followings,\n\n```\ndocker-compose up --build\n```\n\nThis will\n\n- Run container for `redis-server`\n- Build image for `node-app`\n- Run container for `node-app`\n- Put both container in same network\n- Start the `redis-server` container\n- Start the `node-app` container\n\nIn output we should see\n\n```\nApp is listening on port: 8081\n```\n\nSince we map port `4001` from local machine to `8081` of the host machine, from browser, we can access the `node-app` by `http://localhost:4001/`. If we go the browser `http://localhost:4001/`, we should see\n\n```\nNumber of Visits: visit_count\n```\n\n### Stop Containers with Docker Compose\n\nWith `docker-cli` we used to run a container background using\n\n```bash\ndocker run -d image_id\n```\n\nTo stop the instance, we used\n\n```bash\ndocker stop image_id\n```\n\nWith `docker-compose`, to run the containers in background, we can use the following command,\n\n```bash\ndocker-compose up -d\n```\n\nAlso to stop all the containers using the `docker-compose` we can use the followings,\n\n```bash\ndocker-compose down\n```\n\nWe can verify if the container being stopped or not by `docker ps`.\n\n### Container Maintenance with Docker Compose\n\n---\n\nIt is possible that, out `node-server` may crash or hang over time. In this case, we might want to restart the server.\n\nIn `docker-compose` there are 4 different restart policy,\n\n1. **no**: Never restart the container, no matter what happen. This is default restart policy.\n2. **always**: If the container stops for any reason, the `docker-compose` will restart the server.\n3. **on-failure**: Only restart the container, if it crashes with an `error-code`.\n4. **unless-stopped**: Always restart the container on crash except developer forcibly stop it.\n\nWe need to put this `restart-policy` under the `service` declaration.\n\nIn `node.js` we can exit from the app with `process.exit(exit_code)`. As exit code, we have\n\n- `0`, means everything is okay and we want to exit from the node application\n- `non-zero`, any value other than `0` means, there's something wrong and the `error-code` specifies that issue.\n\nThe `restart-policy` of `always` work on when it encounters the `0` as `error-code`.\n\nIf we use `restart-policy` as `on-failure`, we have to use `error-code` other than zero.\n\nFinally, If we use `unless-stopped` as `restart-policy`, then the `container` will always restart, unless we (`develop`) stop in from the `terminal`.\n\n> As `restart-policy`, if we use `no` then we have to put `no` inside a quote. Because in `yml` file, `no` is interpreted as `false`. For other `restart-policy` like, `always`, `on-failure` or `unless-stopped` we can use plain text without the `quote`.\n\n> Between, `always` and `on-failure` use cases, we might 100% time want a public web server restarted on crash. In this case, we use `always`. If we do some worker process, that is meant to do some specific job and then exit, then thats a good case to use `on-failure` exit policy.\n\n### Checking Container Status With `Docker Compose`\n\n---\n\nTraditionally, we used to check the container status using the `docker-cli` by\n\n```bash\ndocker ps\n```\n\n`Docker Compose` has a similar command,\n\n```bash\ndocker-compose ps\n```\n\nThis command should be executed inside the project directory, where the `docker-compose.yml` file exist.\n\nSo, only the `containers` defined inside the `docker-compose` status will be displayed.\n"}],"path":"../..//01 Docker Essential","content":""},{"type":"folder","name":"02 CI-CD With Docker","children":[{"type":"folder","name":"01  multi-container","children":[{"type":"file","name":"01 Multi-container development environment","children":null,"path":"../..//02 CI-CD With Docker/01  multi-container/01 Multi-container development environment.md","content":"## Multi-container development environment\n\nToday we are going to make an app that is responsible to generate fibonacci number. But instead of writing a simple method, we will take it to next level and put couple of complexity layer to serve our sole purpose, multi container CI/CD.\n\nWe will have a react application, that will take the input for a user to get the fibonacci number of a certain index.\n\nThis index will pass to the backend server. We will use an express server in the backend. The express server will save the index in `Postgres` and also store the index in the redis server. A worker process will be responsible for generating the fibonacci number. It will generate, put the result in the redis and then finally return the response to the react application.\n\nToo much complexity!! We are taking this complexity just to make multiple container and implement CI/CD.\n\n### Application Architecture With Image\n\n---\n\nArchitecture image goes here.\n\n### Boilerplate Code\n\n---\n\nA boilerplate code is being found in the `resource directory`.\n\n**description of boilerplate goes here**\n\nTo make the development process smoother, we will make development version of each docker container. This will help us not to rebuild the image every time we make changes in development phase.\n\nFor each of the projects, we will set up pretty similar docker file workflow. For each of the project we will go through,\n\n- Copy the `package.json` to the container\n- Run `npm install` to install all the dependencies\n- Copy everything else\n- Volume mapping for hot-reload feature\n\n### Docker Dev For React App\n\n---\n\nFirst go to the `client` directory and create a `Dockerfile.dev`,\n\n```bash\ncd client\ntouch Dockerfile.dev\n```\n\nAnd our `Dockerfile.dev` should be,\n\n```docker\nFROM node:alpine\nWORKDIR '/app'\nCOPY ./package.json ./\nRUN npm install\nCOPY . .\nCMD [\"npm\", \"run\", \"start\"]\n```\n\nLet's build an image out of this `Dockerfile`,\n\n```bash\ndocker build -f Dockerfile.dev .\n```\n\nThis should build an image and give us a `image_id`.\n\nNow we can run the react app using the image id,\n\n```bash\ndocker run -it image_id\n```\n\nThis should start the development server of our react app. Since we have not port mapping yet, we can not access the site.\n\n### Docker Dev For Express Server\n\n---\n\nGo to the `server` directory and create a file named `Dockerfile.dev`,\n\n```bash\ncd server\ntouch Dockerfile.dev\n```\n\nOur `Dockerfile.dev` file should be like following,\n\n```docker\nFROM node:14.14.0-alpine\nWORKDIR '/app'\nCOPY ./package.json ./\nRUN npm install\nCOPY . .\nCMD [\"npm\", \"run\", \"dev\"]\n```\n\nLet's build an image out of this `Dockerfile`,\n\n```bash\ndocker build -f Dockerfile.dev .\n```\n\nThis should build an image and give us a `image_id`.\n\nNow we can run the react app using the image id,\n\n```bash\ndocker run -it image_id\n```\n\nThis should start the express server on port `5000`.\n\n### Docker Dev For Worker\n\nGo to the `worker` directory and create a `docker-file` named `Dockerfile.dev`,\n\n```bash\ncd worker\ntouch Dockerfile.dev\n```\n\nOur `Dockerfile.dev` should be like the following, same as the express server `Dockerfile.dev`,\n\n```docker\nFROM node:14.14.0-alpine\nWORKDIR '/app'\nCOPY ./package.json ./\nRUN npm install\nCOPY . .\nCMD [\"npm\", \"run\", \"dev\"]\n```\n\nLet's build an image out of this `Dockerfile`,\n\n```bash\ndocker build -f Dockerfile.dev .\n```\n\nThis should build an image and give us a `image_id`.\n\nNow we can run the react app using the image id,\n\n```bash\ndocker run -it image_id\n```\n\nThis should make the worker process standby, so it can listen whenever we insert a message in the redis server.\n\n### Adding `Postgres`, `Server`, `Worker` and `Client` Service\n\n---\n\nNow we have docker-file for the client, server and worker process. Now, we are going to put a docker-compose file to make all the application start up more easy.\n\nEach of the application container require different arguments, like the express server require a port mapping for port `5000`, react app need a port mapping `3000`. We also need to make sure the worker process has the access to redis server. Also the express server needs access of `redis` server and `postgres` server. Along with these integrations, we have to provide all the environment variables to the container.\n\nTo do so, we first integrate the express server with the `redis-server` and `postgres-database`. After that, we will connect all other pieces, the `Nginx server`, react app and worker process.\n\nLet's create the `docker-compose.yml` file in the project root directory,\n\n```bash\ntouch docker-compose.yml\n```\n\nOur `docker-compose.yml` file should be,\n\n```yml\nversion: \"3\"\nservices:\n  postgres:\n    image: \"postgres:latest\"\n    environment:\n      - POSTGRES_PASSWORD=postgres_password\n  redis:\n    image: \"redis:latest\"\n  api:\n    build:\n      dockerfile: Dockerfile.dev\n      context: ./server\n    volumes:\n      - /app/node_modules\n      - ./server:/app\n    environment:\n      - REDIS_HOST=redis\n      - REDIS_PORT=6379\n      - PGUSER=postgres\n      - PGHOST=postgres\n      - PGDATABASE=postgres\n      - PGPASSWORD=postgres_password\n      - PGPORT=5432\n  client:\n    stdin_open: true\n    build:\n      dockerfile: Dockerfile.dev\n      context: ./client\n    volumes:\n      - /app/node_modules\n      - ./client:/app\n  worker:\n    build:\n      dockerfile: Dockerfile.dev\n      context: ./worker\n    volumes:\n      - /app/node_modules\n      - ./worker:/app\n    environment:\n      - REDIS_HOST=redis\n      - REDIS_PORT=6379\n```\n\nWe cn build and run the container from our root directory by,\n\n```bash\ndocker-compose up --build\n```\n\n### `Nginx` Configuration\n\n---\n\nFrom browser, we will make request for static resources and seek `API`. For react application, we will make the call similar like, `/main.js`, `/index.html`. But for server api, we will make call on endpoints like `/api/values/all`, `/api/values/current` etc. You might notice our express server does not have `/api` as prefix. It has endpoints like `/values/all`, `/values/current`.\n\nOur `Nginx` server will handle and do the separation. For api endpoints, start with `/api` it will remove the `/api` part and redirect to the express server. Other request will be send to the `react` application.\n\nWhenever we create a `Nginx` server, it will use a configuration file named `default.conf`. Here in this `default.conf` file, we have to put couple of following information,\n\n- Notify `Nginx` that, we have a upstream server at `client:3000`\n- Notify `Nginx` that, we have a upstream server at `server:5000`\n- Both `client:3000` and `server:3000` should listen to port `80`\n- Add a condition to pass all the `/` request to `client:3000`\n- Add another condition to pass all the `/api` request to `server:5000`\n\nHere `client:3000` and `server:5000`, comes from the service name we are using in the `docker-compose` file.\n\nLets create a directory named `nginx` inside the root project and create a file `default.conf` inside the directory.\n\n```bash\nmkdir nginx\ncd nginx\ntouch default.conf\n```\n\nOur `default.conf` file should be,\n\n```nginx\nupstream client {\n  server client:3000;\n}\n\nupstream api {\n  server api:5000;\n}\n\nserver {\n  listen 80;\n\n  location / {\n    proxy_pass http://client;\n  }\n\n  location /api {\n    rewrite /api/(.*) /$1 break;\n    proxy_pass http://api;\n  }\n}\n```\n\n> In Nginx config `rewrite /api/(.*) /$1 break;` means, replace `/api` with `$1` and `$1` stands for the matching part `(.*)` of the url. `break` keyword stands for stopping any other rewriting rules after applying the current one.\n\n### `Nginx` Container\n\n---\n\nWe set up the `nginx` configuration. Time to set up a docker file for the `nginx server`.\n\nGo to the `nginx` directory and create a file named `Dockerfile.dev`,\n\n```bash\ncd nginx\ntouch Dockerfile.dev\n```\n\nOur `Dockerfile.dev` should look like the following,\n\n```yml\nFROM nginx\nCOPY ./default.conf /etc/nginx/conf.d/default.conf\n```\n\nThats pretty much it. Last thing we need to do, is add the `nginx` service in our `docker-compose.yml` file.\n\nWe need to add the following `nginx` service to our `docker-compose` file,\n\n```yml\nnginx:\n  restart: always\n  build:\n    dockerfile: Dockerfile.dev\n    context: ./nginx\n  ports:\n    - \"3050:80\"\n```\n\nSince our `nginx` server is do all the routing, no matter what, we want our `nginx` server up and running. So, we put `restart` property `always`. In this case, we also do the port mapping from local machine to the container.\n\nWith adding the `nginx` service to our existing `docker-compose`, our `docker-compose.yml` file should be,\n\n```yml\nversion: \"3\"\nservices:\n  postgres:\n    image: \"postgres:latest\"\n    environment:\n      - POSTGRES_PASSWORD=postgres_password\n  redis:\n    image: \"redis:latest\"\n  nginx:\n    restart: always\n    build:\n      dockerfile: Dockerfile.dev\n      context: ./nginx\n    ports:\n      - \"3050:80\"\n  api:\n    build:\n      dockerfile: Dockerfile.dev\n      context: ./server\n    volumes:\n      - /app/node_modules\n      - ./server:/app\n    environment:\n      - REDIS_HOST=redis\n      - REDIS_PORT=6379\n      - PGUSER=postgres\n      - PGHOST=postgres\n      - PGDATABASE=postgres\n      - PGPASSWORD=postgres_password\n      - PGPORT=5432\n  client:\n    stdin_open: true\n    build:\n      dockerfile: Dockerfile.dev\n      context: ./client\n    volumes:\n      - /app/node_modules\n      - ./client:/app\n  worker:\n    build:\n      dockerfile: Dockerfile.dev\n      context: ./worker\n    volumes:\n      - /app/node_modules\n      - ./worker:/app\n    environment:\n      - REDIS_HOST=redis\n      - REDIS_PORT=6379\n```\n\nNow time to start all the containers by,\n\n```bash\ndocker-compose up --build\n```\n\nMost probably, first time, the server and worker both try to get the redis instance, even it might not being copied. So In case of any error, we just have do run the container one more time by,\n\n```bash\ndocker-compose up\n```\n\nNow, from the local machine browser, if we go to `http://localhost:3050/`, we should see the react app and calculation should work with manual refresh.\n\n### Enable Websocket Connection\n\nThe react application keep an connection with it's development server to maintain hot reload. Every time there is a source code changes, react app listen these changes via websocket connection and reload the web app.\n\nWe need to configure the `nginx` server to enable the websocket to handle the issue.\n\nTo add websocket connection we need a route in the `default.config` file,\n\n```nginx\nlocation /sockjs-node {\n    proxy_pass http://client;\n    proxy_http_version 1.1;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection \"Upgrade\";\n  }\n```\n\nSo our final configuration for the `nginx` server will be,\n\n```nginx\nupstream client {\n  server client:3000;\n}\n\nupstream api {\n  server api:5000;\n}\n\nserver {\n  listen 80;\n\n  location / {\n    proxy_pass http://client;\n  }\n\n  location /sockjs-node {\n    proxy_pass http://client;\n    proxy_http_version 1.1;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection \"Upgrade\";\n  }\n\n  location /api {\n    rewrite /api/(.*) /$1 break;\n    proxy_pass http://api;\n  }\n}\n```\n\nNow, we can test all the container by running,\n\n```bash\ndocker-compose up --build\n```\n\n### Update the UI\n\nGo to `client` directory and from the `/src` directory, update the `App.js` by the followings,\n\n```jsx\nimport React from 'react';\nimport { BrowserRouter as Router, Route } from 'react-router-dom';\nimport OtherPage from './OtherPage';\nimport Fib from './Fib';\n\nfunction App() {\n  return (\n    <Router>\n      <div>\n        <Route exact path=\"/\" component={Fib} />\n        <Route path=\"/otherpage\" component={OtherPage} />\n      </div>\n    </Router>\n  );\n}\n\nexport default App;\n```\n\nOur app should be running on `http://localhost:3050/`. \n\nGo to browser and go the address `http://localhost:3050/`. In the input box, put the value `2` and click submit. Now if we reload the web page, the value 4 should be appeared.\n\nSeems like our app is running smoothly on the development machine as expected.\n\n"},{"type":"file","name":"02 Multi-container Continuous Integration","children":null,"path":"../..//02 CI-CD With Docker/01  multi-container/02 Multi-container Continuous Integration.md","content":"## Managing multiple containers in local environment\n\n### Approach\n\n---\n\nWe already got our development environment for multiple environment. To implement the continuous integration, we can do the following steps,\n\n- Make sure, codebase is already in the `Github` repository\n- `Travis CI` will pull the code\n- `Travis CI` will build the test image, run the tests and remove the test image\n- `Travis CI` will build the production image\n- `Travis CI` will push the image to `Docker Hub`\n- `Travis CI` will notify `Elastic Beanstalk` that the image is being uploaded to `Docker Hub`\n- `Elastic Beanstalk` will pull the image from the `Docker Hub`, run the container and serve web request\n\n### Worker Process Production Dockerfile\n\n---\n\nFor production, in the worker process, we will use the very similar configuration of development config. The only change will be the start-up command of the container.\n\nFirst go to the `worker` directory and create a `Dockerfile`,\n\n```bash\ncd worker\ntouch Dockerfile\n```\n\nOur production version of `Dockerfile` should look like the followings,\n\n```docker\nFROM node:14.14.0-alpine\nWORKDIR '/app'\nCOPY ./package.json ./\nRUN npm install\nCOPY . .\nCMD [\"npm\", \"run\", \"start\"]\n```\n\n### Production API Dockerfile\n\n---\n\nFor production, like previous `worker` process, our `Dockerfile` will be similar to the `Dockerfile.dev` except the container startup command.\n\nSo, go to the `server` directory and create a `Dockerfile`,\n\n```bash\ncd server\ntouch Dockerfile\n```\n\nAnd our `Dockerfile` for `API` should be the following,\n\n```docker\nFROM node:14.14.0-alpine\nWORKDIR '/app'\nCOPY ./package.json ./\nRUN npm install\nCOPY . .\nCMD [\"npm\", \"run\", \"start\"]\n```\n\n### Nginx Server Production Dockerfile\n\n---\n\nSometimes, our docker configuration for both development and production will be same. It's always good practice to use separate files for development and production. For our `Nginx` server, since both development and production docker config file is same, we will just copy the `Dockerfile.dev` to `Dockerfile`.\n\n```bash\ncd nginx\ncp Dockerfile.dev Dockerfile\n```\n\n> Since we have a route for handling socket connection in development phase, which is not required in the production, We might consider a separate config file for nginx server.\n\n### Dockerfile For React App\n\n---\n\nFor the react we will make use of a separate `nginx` file. We can definitely use our existing `nginx`, but to make the application more robust and independent, we will use the another one. In the client directory, create a folder named `nginx` and a file `default.conf`,\n\n```bash\ncd client\nmkdir nginx\ncd nginx\ntouch default.conf\n```\n\nOur `default.conf` file should be,\n\n```nginx\nserver {\n  listen 3000;\n\n  location / {\n    root /usr/share/nginx/html;\n    index index.html index.htm;\n    try_files $uri $uri/ /index.html;\n  }\n}\n```\n\nNow we have to create the production version of our react application `Dockerfile`. Go to the client directory and create `Dockerfile`,\n\n```bash\ncd client\ntouch Dockerfile\n```\n\nOur production version of `Dockerfile` will be,\n\n```docker\nFROM node:alpine as builder\nWORKDIR '/app'\nCOPY ./package.json ./\nRUN npm install\nCOPY . .\nRUN npm run build\n\nFROM nginx\nEXPOSE 3000\nCOPY ./nginx/default.conf /etc/nginx/conf.d/default.conf\nCOPY --from=builder /app/build /usr/share/nginx/html\n```\n\nIn the react application, if we try to run the test, it will throw an error. The reason is in the test file `App.test.js`, we are testing if the `App` component can render without any crash.\n\nThis test flow do the followings,\n\n- First try to render the `App` component\n- `App` component try to render the `Fib` component\n- `Fib` component try to invoke the express server\n\nSince our express server is not running, this will throw an error. In real application, we might simulate an face api response. in this case, we can keep a dummy test execution by removing the following 3 lines from `App.test.js`,\n\n```js\nconst { getByText } = render(<App />);\nconst linkElement = getByText(/learn react/i);\nexpect(linkElement).toBeInTheDocument();\n```\n\nOur `App.test.js` file will look like the following,\n\n```js\ntest(\"renders learn react link\", () => {});\n```\n\n### Travis CI Setup\n\n---\n\nWe now completely set up the production version of docker for each of the application container. Now we have to create a github account, push all our code to github and then hook it to the `Travis CI`. Then, in our code, we need a `Travis CI` configuration file, responsible for\n\n- Build a test image and test code\n- Building production image\n- Push the production image to `Docker Hub`\n- Notify `Elastic Beanstalk` on code changes\n- `Elastic Beanstalk` will pull the image and run the containers\n\nFirst push all the codebase, make sure your github repository is integrated and synced with `Travis CI`. Now for [settings](https://travis-ci.org/account/repositories), find and enable the switch button to mark as build project.\n\nSince we have not create the `Elastic Beanstalk` instance in the `AWS`, we will not configure that part in the `Travis CI` for now. Also, we are only do the test coverage for the react app, not the api or worker service.\n\nLet's create a `Travis CI` config file named `.travis.yml` in the root project directory,\n\n```bash\ntouch .travis.yml\n```\n\nOur `.travis.yml` file should be like the following,\n\n```yml\nsudo: required\nservices:\n  - docker\n\nbefore_install:\n  - docker build -t DOCKER_HUB_USER_NAME/react-test -f ./client/Dockerfile.dev ./client\n\nscript:\n  - docker run -e CI=true USERNAME/react-test npm test -- --coverage\n\nafter_success:\n  - docker build -t DOCKER_HUB_USER_NAME/multi-client ./client\n  - docker build -t DOCKER_HUB_USER_NAME/multi-nginx ./nginx\n  - docker build -t DOCKER_HUB_USER_NAME/multi-server ./server\n  - docker build -t DOCKER_HUB_USER_NAME/multi-worker ./worker\n  - echo \"$DOCKER_PASSWORD\" | docker login -u \"$DOCKER_ID\" --password-stdin\n  - docker push DOCKER_HUB_USER_NAME/multi-client\n  - docker push DOCKER_HUB_USER_NAME/multi-nginx\n  - docker push DOCKER_HUB_USER_NAME/multi-server\n  - docker push DOCKER_HUB_USER_NAME/multi-worker\n```\n\nTo access the docker hub to upload the image, we need to put the credentials in the `Travis CI` environment section.\n\nTo set environment variable, go to `Dashboard -> Select Repository -> Options -> Settings -> Environment Variables`. In the environment variables section, add the following environment variables,\n\n1. `DOCKER_ID`\n2. `DOCKER_PASSWORD`\n\nNow, if e push the codebase to the github, the `Travis CI` should test the react app and pushed the build image to the docker hub.\n"},{"type":"file","name":"03 Multi-container Continuous Deployment","children":null,"path":"../..//02 CI-CD With Docker/01  multi-container/03 Multi-container Continuous Deployment.md","content":"## Multi-container Continuous Deployment\n\nOur `Travis CI` si already configured to build the image and push the image to the `Docker Hub`. Now we have to think about how use these images and deploy them to production. To deploy these image we are going to make use of `Elastic Beanstalk`.\n\nWhen we have only one container, the `Elastic Beanstalk` will automatically build and run the the container, we do not have to set up any custom configuration. In the root directory, we just have to take the project files and a `Dockerfile` and `Elastic Beanstalk` do the rest.\n\nNow, the scenario is different. we have multiple `Dockerfile` in different folder. Anytime we have multiple `Dockerfile`, we have to tell the `Elastic Beanstalk` with little configuration, how these `Dockerfile` will be treated.\n\nTo put the configuration for `Elastic Beanstalk`, how our multiple `Dockerfile` will be treated, we have to create special config file in the project directory named `Dockerrun.aws.json`. This config file will define,\n\n- From where the `image` files will be pulled\n- Resources allocated for the `image`\n- Port mapping\n- Some associated configurations like handling environment variables\n\nThe configurations of `Dockerrun.aws.json` will be very much similar to the `docker-compose.yml` configuration.\n\n> `docker-compose.yml` is all about, how we build images, whereas `Dockerrun.aws.json` is all about definitions of container.\n\n> When it comes to handle multiple container, the `Elastic Beanstalk` does not know, how to handle multiple containers. For multiple container, it delegates the tasks to another `AWS` server named `Elastic Container Service` aka `ECS`. In the `ECS` for each container, we will define `Task`, also known as `Task Definition`. A doc for defining the container configuration is given [here](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/create-task-definition.html#task-definition-template).\n\n## Container Definition\n\n---\n\nGo to the project root directory and create a file named `Dockerrun.aws.json`,\n\n```bash\ntouch Dockerrun.aws.json\n```\n\nFor the `Dockerrun.aws.json` file, we have to consider the following conventions,\n\n- **version :** We have to specify the template version. Different version compile differently\n- List of containers Definitions\n- For each container\n  - **name :** Name should be the directory name\n  - **image :** Image name that is deployed to the docker hub with the docker hub user name\n  - **hostname :** Hostname should be the service name. This is also being used in the nginx configuration\n  - **essential :** If this is true, crashing this container will make crash other containers. Among all the container, one should marked as `essential`\n  - **memory :** Need to define the allocated memory in mega bytes, required for a container\n  - **links :** To do the routing we have to use the directory name\n\n> For the worker and nginx, since no one is routing to these, we do not need any `hostname` in these configuration.\n\n> Since, the `nginx` server is responsible for communicating with the outside world, we need to do the port mapping.\n\n> Also, in the `nginx` server configuration we have to specify the routes to other containers\n\n> It's challenging to allocate exactly essential memory for each of the services. Traditionally there are couple of stackoverflow posts, can be used to find out the desired memory allocation.\n\nOur `Dockerrun.aws.json` should be like the following,\n\n```json\n{\n  \"AWSEBDockerrunVersion\": 2,\n  \"containerDefinitions\": [\n    {\n      \"name\": \"client\",\n      \"image\": \"bmshamsnahid/multi-client\",\n      \"hostname\": \"client\",\n      \"essential\": false,\n      \"memory\": 128\n    },\n    {\n      \"name\": \"server\",\n      \"image\": \"bmshamsnahid/multi-server\",\n      \"hostname\": \"api\",\n      \"essential\": false,\n      \"memory\": 128\n    },\n    {\n      \"name\": \"worker\",\n      \"image\": \"bmshamsnahid/multi-worker\",\n      \"hostname\": \"worker\",\n      \"essential\": false,\n      \"memory\": 128\n    },\n    {\n      \"name\": \"nginx\",\n      \"image\": \"bmshamsnahid/multi-nginx\",\n      \"hostname\": \"nginx\",\n      \"essential\": true,\n      \"portMappings\": [\n        {\n          \"hostPort\": 80,\n          \"containerPort\": 80\n        }\n      ],\n      \"links\": [\"client\", \"server\"],\n      \"memory\": 128\n    }\n  ]\n}\n```\n\n### Using Managed DB Service in Production\n\n---\n\nIn development, we are using the `redis` and `postgres` in our own development machine. But for production, we might need to consider a managed version of `redis` aka `AWS Elastic Cache` and for `postgres` we will make use of `AWS Relational Database Service`.\n\nAdvantages of `AWS Elastic Cache` and `AWS Relational Database Service`,\n\n- Managed creation and maintenance\n- Easy scaling policy\n- Built in logging\n- Better security\n- Easy migration\n- Multi AZ configuration\n\nAdditionally `AWS Relational Database Service` has some advantages,\n\n- Database backup and rollback facility\n- Easy tuning on Multi AZ and Read Replicas\n\n### Set Up Managed Services\n\n---\n\nLet's build our cloud infrastructure to the using manged `AWS Services`. Along with the `AWS Elastic Beanstalk` we will make use of\n\n- AWS RDS Instance\n- AWS Elastic Cache\n\n**Set Up Associated Security Group :**\n\nGo to `VPC` section and from security group create one named `multi-docker`. The inbound rules should allow port range of `5432-6379` and source should be the newly created security group `multi-docker`.\n\n**Set Up Elastic Beanstalk :**\n\nGo to `Elastic Beanstalk` service and create application with the following config\n\n- Name as `multi-docker`\n- Platform as `Docker`\n- Platform Branch as `Multi Container Docker running on 64bit Amazon Linux`\n- After creating the environment, go to configuration and edit instances security by adding the security group `multi-docker`\n\n**Set Up RDS Postgres Instance :**\n\nGo to `AWS RDS` service and create database of `Postgres` instance with following configuration,\n\n- Identifier as `multi-docker-postgres`\n- Username as `postgres`\n- Master Password as `postgrespassword`\n- Initial database name from the `Additional Settings` should be `fibvalues`\n- After creation of DB instance, modify the network security by adding security group `multi-docker`\n\n**Set Up Elastic Cache Redis Instance :**\n\nGo to `Elastic Cache` service and create a `redis` instance with following configuration,\n\n- Name should be `multi-docker-redis`\n- Node type as `cache.t2.micro` and replicas 0 per shard for less pricing\n- After creating the instance, from action add the security group `multi-docker`\n\n**Generate a IAM User With Appropriate Roles :**\n\nFor simplicity of the `IAM` user existing policies, search `elasticbeanstalk` and mark all the services. This will provide an `AWS Access Key` and `AWS Secret Key`. This key and secret has to be provided to the `Travis cI` for invoking the `Elastic Beanstalk`.\n\n### Update Travis CI Config File For Production Deployment\n\n---\n\nWe have left two config for the production deployment.\n\n- Notify the `Elastic Beanstalk`, a new changes being happen in the codebase\n- Push the entire project to `Elastic Beanstalk`\n\nAlthough we are pushing the whole codebase to `Elastic Beanstalk`, the only file `ELB` care about is, `Dockerrun.aws.json`.\n\nFrom `Dockerrun.aws.json`, `ELB` download all the images from the docker hub and will run the container.\n\nOue final `.travis.yml` with deploy configuration should be look like the following,\n\n```yml\nsudo: required\nservices:\n  - docker\n\nbefore_install:\n  - docker build -t bmshamsnahid/react-test -f ./client/Dockerfile.dev ./client\n\nscript:\n  - docker run -e CI=true bmshamsnahid/react-test npm test -- --coverage\n\nafter_success:\n  - docker build -t bmshamsnahid/multi-client ./client\n  - docker build -t bmshamsnahid/multi-nginx ./nginx\n  - docker build -t bmshamsnahid/multi-server ./server\n  - docker build -t bmshamsnahid/multi-worker ./worker\n  - echo \"$DOCKER_PASSWORD\" | docker login -u \"$DOCKER_ID\" --password-stdin\n  - docker push bmshamsnahid/multi-client\n  - docker push bmshamsnahid/multi-nginx\n  - docker push bmshamsnahid/multi-server\n  - docker push bmshamsnahid/multi-worker\n\ndeploy:\n  provider: elasticbeanstalk\n  region: \"ap-south-1\"\n  app: \"multi-docker\"\n  env: \"MultiDocker-env\"\n  bucket_name: \"elasticbeanstalk-ap-south-1-366735605679\"\n  bucket_path: \"docker-multi\"\n  on:\n    branch: master\n  access_key_id: $AWS_ACCESS_KEY\n  secret_access_key: $AWS_SECRET_KEY\n```\n\nIn the `Travis CI` dashboard, select the environment and from options of the `multi-container-ci-cd` repository, add the following environment variables,\n\n1. AWS_ACCESS_KEY\n2. AWS_SECRET_KEY\n\nNow we make a commit and push the changed code to the master branch, `Travis CI` should automatically ensure the continuous integration and `Elastic Beanstalk` should ensure continuous deployment. After successful deployment, the `Elastic Beanstalk` environment should be show `Green` success check.\n\nOur application should be automatically deployed to the `Elastic Beanstalk`.\n\n### Cleaning Up AWS Resources\n\n---\n\nAlong the way, we have been using the following services,\n\n- Elastic Beanstalk\n- RDS Service\n- Elastic Cache (Managed Redis)\n- Security Group\n- IAM User with necessary permissions\n\n**Deleting Elastic Beanstalk :**\n\n- Go to the Elastic `Beanstalk Service` and select the `multi-docker` environment\n- From action select the `Terminate Application`\n\n**Deleting RDS Service :**\n\n- Go to `RDS` service\n- Select the `multi-docker-postgres` and from action select `Delete`\n\n**Deleting Elastic Cache :**\n\n- Go to `Elastic Cache`\n- From `redis` select our instance `multi-docker-redis`\n- Select and from action, click the `Delete` option\n\n**Deleting Security Group (Optional) :**\n\n- Go to `VPC` service and from left panel select `Security Groups`\n- Delete the security group named `multi-docker` and all its associates if there any\n\n**Deleting IAM Users (Optional) :**\n\n- Go to `IAM` service and delete the user we have created\n\n> For security groups, we might not need to delete them, as they are not billing service. Same goes for `IAM` user, it is not included in a billing service, but good to delete if not necessary.\n"}],"path":"../..//02 CI-CD With Docker/01  multi-container","content":""},{"type":"file","name":"02 Single Container CI-CD Workflow","children":null,"path":"../..//02 CI-CD With Docker/02 Single Container CI-CD Workflow.md","content":"## Single Container Workflow\n\nLet's use docker in a production type environment. The important thing to keep in mind, we first explain the workflow without docker. Instead we discuss outside services that we are going to use to set up this development workflow. Once we get the bird view of the workflow and get the core design behind the workflow, then we introduce the docker and find how docker can facilitate everything.\n\nWe will create an application that will use docker and eventually push the application to `AWS`. Our workflow is going to be\n\n- Develop\n- Testing\n- Deployment\n\nAlso for any changes, we will repeat the workflow again.\n\n**Development Phase :** Our dev workflow is starting by creating a git repository. This git repository is going to be the center of coordination of all the code we will write. Out git repository will have two types of branch, master and feature branch. We make changes on the feature branch. By filing a PR we will merge the feature branch to master branch. When we do the PR there will be a series of actions, defining how we govern the codebase. The master branch will contain the very clean copy of our code base.\n\n**Test Phase :** As soon as we make the PR, the `Travis CI` will pull the new and updated code and run the test. If all the tests executed successfully, then we will merge the code to the master branch.\n\n**Production Phase :** After merging the feature branch, We then again push the code to `Travis CI` and run tests of the code. Any changes on the master branch will eventually and automatically be hosted in the `AWS Beanstalk`.\n\nNow we need to find out how `docker` fits in this place. To execute this workflow, we do not need to make use of `Docker`. But using docker will make the workflow lot lot easier. Thats the soul purpose of docker. It's not necessarily a requirement, it just make developer life easier.\n\nDocker is not the focus here, it's more of using these 3rd party services (like, Github, Travis CI, AWS Beanstalk) with docker.\n\n### Generating a Project\n\n---\n\nWe will use a `react` for simplicity. To create a react project, make sure `node.js` is installed in your system. Then create the project named `frontend` by the followings,\n\n```bash\nnpm create-react-app frontend\n```\n\nThis will create the react project and install all the necessary dependencies.\n\nNow go to the project directory,\n\n```bash\ncd frontend\n```\n\nTo run the test in local machine, we can use\n\n```bash\nnpm run test\n```\n\nTo build the application for future deployment, we can build using\n\n```bash\nnpm run build\n```\n\nAn finally to run the application in the local environment, we can use\n\n```bash\nnpm run start\n```\n\n### Generating Dev Dockerfile\n\n---\n\nFirst go to the project directory. Here we will create a `Dockerfile` named `Dockerfile.dev`. The purpose of using `.dev` with the `Dockerfile` is to make clear that, this docker file is only using in the development environment.\n\nIn future, we will use another `Dockerfile` with simply name `Dockerfile`, without any extension for our production environment.\n\nSo, lets create the `Dockerfile.dev` in the project root directory,\n\n```bash\ntouch Dockerfile.dev\n```\n\nInside the `Dockerfile.dev` we will write configuration to make our image.\n\nOur `Dockerfile.dev` will be similar like the following.\n\n```docker\n# Define base image\nFROM node:alpine\n\n# Define working directory inside the container\nWORKDIR /app\n\n# Copy the package.json file to the project directory\nCOPY package.json .\n# Install the dependencies\nRUN npm install\n\n# Copy all the source code from host machine to the container project directory\nCOPY . ./\n\n# Start up command of the container to run the server\nCMD [\"npm\", \"run\", \"start\"]\n```\n\n### Building Image From Dockerfile\n\n---\n\nSince we are not using default name of `Dockerfile`, instead we are using `Dockerfile.dev` we have to specify the name during building image. To do so, we can use `-f` flag, that helps to specify the `docker file` name. This time, during development, our build instruction will be,\n\n```bash\ndocker build -f Dockerfile.dev .\n```\n\nWhen we initially create our react application using `create-react-app`, the `create-react-app` automatically install all the `node modules` inside the `node_modules` directory. Now, when we are building the image out of the app, we will again installing the dependencies using `RUN npm install` instruction in the `Dockerfile.dev` file. We do not need the `node_modules` in the host machine, it usually increase the duration of image building process, since we are running the app inside the `container`. So we can delete the `node_modules` directory from the `host machine`.\n\n> `node_modules` directory is not necessary in the `host machine` project directory, it increase the time to build the image. We can delete `node_modules` from project directory.\n\nIf we again run the previous instruction to build the image, we will get much faster output,\n\n```bash\ndocker build -f Dockerfile.dev .\n```\n\n### Run Container in Interactive Mode\n\nNow we can run a container from the image by\n\n```bash\ndocker run -it -p 3000:3000 IMAGE_ID\n```\n\nIf we observe closely, we can notice, while we made changes in the codebase, the hot reloading is not working. The image was build on the snapshot of files. One way is after each changes in the code base we will rebuild the image. But this is a costly and inefficient approach. So we need to find a way to enable hot reload inside inside the image.\n\n### Enabling Hot Reload Using Docker Volume\n\nIn the last section, we made changes in a file and the changes was not reflected in the container. Now we will figure out a way to solve the issue without stopping, rebuild and restarting the container.\n\nA docker volume essentially a mapping of directory between host machine and container.\n\nWith docker volume, we can use the reference of local machine directory from the host machine. In this case, we do not copy the directory of the local machine in the container, instead, use the host machine directory by reference from container.\n\nTo use the volume mapping we need to use the followings,\n\n```bash\ndocker run -p 3000:3000 -v /app/node_modules -v $(pwd):/app image_id\n```\n\nHere we have used two switches. Each switch has a `-v` flag which is used to set up a volume. The `${pwd}` is stands for `print working directory`, used to get the current application path.\n\nHere first switch is `-v /app/node_modules`, is not using the reference. Instead it says, not to map the `/app/node_modules` of the host machine.\n\nFor the second switch, `-v ${pwd}:/app`, we are using the host machine volume. The `:` stands when we use host machine directory as reference in the docker.\n\nIf the hot reload not work, need to create a `.env` in the `root directory` and need to add `CHOKIDAR_USEPOLLING=true`.\nThe `.env` file should be\n\n```\nCHOKIDAR_USEPOLLING=true\n```\n\n### Shorthand of Docker CLI\n\n---\n\nThe downside of previous run command is it is ridiculously long. We have to specify the `port-mapping`, `volume-mapping` and also the image id. With docker compose we can dramatically shorten the command to run the container in development phase. Let's create a `docker-compose` inside the project directory and encode the port mapping and volume mapping.\n\nFirst go to the project root directory and create a file named `docker-compose.yml`\n\n```bash\ntouch docker-compose.yml\n```\n\nOur `docker-compose.yml` file will be like,\n\n```yml\nversion: \"3\"\nservices:\n  web:\n    stdin_open: true\n    build:\n      context: .\n      dockerfile: Dockerfile.dev\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - /app/node_modules\n      - .:/app\n```\n\nWith docker compose, from now on, we can simple run the application by,\n\n```bash\ndocker-compose up\n```\n\n### Executing Test Cases\n\n---\n\nNow we have a solid development infrastructure for the react application with docker container.\n\nIt's time to focus on running the tests inside the container. First we run the tests to our development environment and then shift to `Travis CI`.\n\nThe good things is, running test cases inside the container is very much straight forward. To run the test cases, we have to follow two steps,\n\n1. Create the image out of the `Dockerfile`\n2. Override the startup command with the test command\n\nFirst let's build the image,\n\n```bash\ndocker build -f Dockerfile.dev .\n```\n\nNow, to override our container startup command and replace it with the `npm run test`, we can do,\n\n```bash\ndocker run image_id npm run test\n```\n\nThis should run all the test cases for the app. To open the test runner in an interactive mode, we can utilize the `-it` flag.\n\n```bash\ndocker run -it image_id npm run test\n```\n\nNow the test suite will run in the interactive mode.\n\n### Live Update Test Cases\n\n---\n\nIf we run the tests in the container and update the test suite, we will notice the test cases changes does not impact inside the container.\n\nMay be you got the reason. Here we created a special container by `docker build -f Dockerfile.dev .` that take the snapshot of the working files and put then inside the container. Now this special temporary container does not have volume mapping set up. So changes inside the files, does not impact the test cases changes.\n\nTo resolve the issue we can take multiple approach. One is, start the container with `docker compose` and then run the `docker exec` to use the web service. Let's try this one,\n\nFrom one terminal, run the container with docker compose\n\n```bash\ndocker-compose up\n```\n\nNow from another terminal, first take the running container id by\n\n```bash\ndocker ps\n```\n\nFrom the out put we can get the `container_id` and run `npm run test` directly inside the container,\n\n```bash\ndocker exec -it container_id npm run test\n```\n\nThis should run the test suite in interactive mode with live update.\n\nNow this solution is not as good as it should be.\n\nHere in the `docker-compose.yml` file we will add another service, which will be solely responsible for run the test suite whenever we change the files.\n\nWe already have a service named `web` that is responsible for run the web application. Our new service, we can named `tests` will be responsible for run the test suites. Except the `web` service, the `tests` service do not require the `port-mapping`, instead it needs to override the start up command. In this `tests` service the startup command should be `npm run test`.\n\nOur new `docker-compose.yml` file will be,\n\n```yml\nversion: \"3\"\nservices:\n  web:\n    stdin_open: true\n    build:\n      context: .\n      dockerfile: Dockerfile.dev\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - /app/node_modules\n      - .:/app\n  tests:\n    build:\n      context: .\n      dockerfile: Dockerfile.dev\n    volumes:\n      - /app/node_modules\n      - .:/app\n    command: [\"npm\", \"run\", \"test\"]\n```\n\nNow run the container with a new build\n\n```bash\ndocker-compose up --build\n```\n\nThis will run both, the web application and the test suite.\n\nSo bottom line is both approaches has some downside. For the first approach, we have to find out the running container id and remember the `docker exec` command.\n\nFor the second approach, we get the test suite result in the docker log area. Also in this approach we can get the test suite in an interactive mode.\n\n### Nginx in Production Server\n\n---\n\nIn development phase, we have a development server provided by the `create-react-app` that handle the request of port `3000`. In production, we need a web server whose sole purpose will be to respond to browser request. In this case, we can use an extremely popular web server `Nginx`. It's a minimal server basically do the routing.\n\nSo we create a separate docker file, that will create a production version of web container. This production version of docker container will start an instance of Nginx. This `Nginx` server will be used to serve our `index.html` and `main.js` file.\n\n### Multi Step Docker Builds\n\n---\n\nTo use `Nginx` in the production environment, we will need\n\n- Build Phase (To build the react app)\n  - Use `Node` base image\n  - Copy the `package.json` file\n  - Install dependencies\n  - Build the app by `npm run build`\n- Run Phase (To run the Nginx server)\n  - Use `Nginx` image\n  - Copy the build file generated in the `build phase`\n  - Start the `Nginx` server\n\nLet's create a docker file for our production environment,\n\n```bash\ntouch Dockerfile\n```\n\nIn the `Dockerfile` we will have two distinctly different section. One is for build the app and second is run the Nginx server with build files.\n\n> `Nginx` base image has already integrated a startup command. We do not explicitly start the `Nginx Server`.\n\nIn the `Run Phage`, when we will use the `Nginx` using docker, it will automatically remove all files and folder from the `Build Phase` except the `build` directory. So our production image will be very small.\n\nOur `Dockerfile` will be,\n\n```docker\nFROM node:alpine\nWORKDIR '/app'\nCOPY package.json .\nRUN npm install\nCOPY . .\nRUN npm run build\n\nFROM nginx\nCOPY --from=0 /app/build /usr/share/nginx/html\n```\n\nNow, let's build the image\n\n```bash\ndocker build .\n```\n\nThe end result of the `build` command will be an `image_id`.\n\nTo to run the container, since `Nginx` is a web server, we have to do the port mapping. `Nginx` use `80` as default port.\n\nWe can run the container using the following,\n\n```bash\ndocker run -p 8080:80 image_id\n```\n\nNow, if we go to `http://localhost:8080`, we should see the react app running.\n\nSo, now we got a docker application, that can be build our application and serve the application from a `Nginx` server. Now we need to ship all our work to the outside world, the deployment.\n\n### Setting Up Git Repository\n\n---\n\nLet's create a git repository named `ix-docker-react`.\n\nPlease make sure the git repository is public. Off-course, the private repo will work but for private we have to set up couple of additional config.\n\nPush all your code base to the github master branch. Make sure in the github, your react-application source with `Dockerfile` exist. You can follow the commands,\n\n```bash\ngit init\ngit add .\ngit commit -m 'initial commit'\ngit remote add origin remote_git_repository_address\ngit push origin master\n```\n\n### Setting Up Travis CI\n\n---\n\nWe have not discussed about `Travis CI` a lot. `Travis CI` essentially set a sky limit for our code base, we can do whatever we want.\n\nWhen we push some changes to the github, github taps the `Travis CI` that some code changes. In this case, `Travis CI` pull the code base and do whatever we are asked to. Some developer use `Travis CI` for test the code base after any changes, someone use the `Travis CI` to deploy the codebase. In our case, we will use `Travis CI` for both, `Test the codebase` and `Deploy to the AWS`.\n\nTo set up `Travis CI`,\n\n- Sign in `Travis CI` with your github account (For simplicity)\n- Enable Github App (This will load all your github repo in `Travis CI` dashboard)\n- Find `ix-docker-react` and enable tracking by tapping the switch button\n\n> `Travis CI` ui is being changed time to time. If you have trouble finding the green switch button, please find legacy site [here](https://travis-ci.org/account/repositories) and tap the button\n\nNow we have to explicitly tell `Travis CI` what to do when a code base is being changed. For now we ignore the `AWS Deployment` and focus on testing.\n\nWe essentially tell `Travis CI` how to start the `Docker`, run the test suite and interpret the test result. In order to do so, we will require a file `.travis.yml` in the project root directory.\n\nLet's create the file `.travis.yml`,\n\n```bash\ntouch .travis.yml\n```\n\nIn the `.travis.yml` we make sure the `super user` privilege is being configured. We require the `super user` privilege for running the docker.\n\nWe also specify that, we are using the `docker`, so `Travis CI` will bring a copy of the `Docker`.\n\nFinally, we build an image out of the `Dockerfile.dev` to run our test suite.\n\nIn our local machine, after we build the `image` we used to get the `image_id` and used the `image_id` to run the container. In `Travis CI`, we can not manually copy and paste the `image_id`, so we can utilize the use of `tagging`.\n\nOur `.travis.yml` file should be like the following,\n\n```yml\nlanguage: generic\nsudo: required\nservices:\n  - docker\n\nbefore_install:\n  - docker build -t docker_username/github_repo_name -f Dockerfile.dev\n\nscript:\n  - docker run -e CI=true docker_username/github_repo_name npm run test -- --coverage\n```\n\n> As tag, we can use any name. But its a good practice to use the convention `docker_username`/`github_repo_name`\n\n> Default behaviour of `Jest` is run the test for the first time and bring an interactive terminal to run test according to developer input. With `-- -- coverage` we can change that default behaviour and the will run once and return status code and terminate.\n\n> When test results return with status code `0`, means test coverage is as expected\n\nNow push the changed codebase to the github repository.\n\n```bash\ngit add .travis.yml\ngit commit -m 'Added travis file'\ngit push origin master\n```\n\nThis should trigger the test in the `Travis CI`. In the dashboard, inside `react-docker` application, we should see the test suite running and should get passed all the tests.\n\nNow, we have a pipeline in place to automatically watch out our github repository for changes and pull down the source code to run the tests and report back if everything is alright.\n\n### Set Up AWS Elastic Beanstalk\n\n---\n\nElastic Beanstalk is the easiest way to run `Single Container` application in `AWS Infrastructure`.\n\nWhen we deploy a code base to `AWS Beanstalk` using `Travis CI`, in background, the `Travis CI` upload a zipped version of the code to the `S3 Bucket` and tap the `Beanstalk` to notify there's a new version of code is being pushed. Then the `AWS Beanstalk` pull the codebase from the `S3 Bucket` and redeploy. And good things is, when we create a `AWS Elastic Beanstalk` environment, this `S3 Bucket` is being automatically created.\n\n`AWS` recently update the `AWS Elastic Beanstalk` that can work with `docker compose`. By default, when platform branch is `Docker Running on 64bit Linux`, then the new feature with docker-compose works. In our case we will make use vanilla `Dockerfile`.\n\nWhen we set a `docker` environment in the `AWS Beanstalk`, `AWS Beanstalk` create a virtual machine that's sole purpose is to run the docker container we provide. In the `AWS Beanstalk` there is already an built in `load-balancer`. So whenever the traffic flows increase, the `AWS Beanstalk` automatically increase the number of `Virtual Machine`, as well as, our container and react-app inside the container.\n\nLogin to your `AWS` account and select the `Elastic Beanstalk` service.\n\nNow create an application with the following config,\n\n- Application name can be anything, I am using `ix-docker-react`\n- Platform should be `Docker`\n- Platform branch should be `Docker Running on 64bit Amazon Linux`\n- Platform version as `AWS Recommended`, I am using `2.16.4`\n\nCreating the application environment might take couple of minutes. After the environment being created, the environment will be listed in the `environment` section.\n\n### Travis Config AWS Deployment\n\n---\n\nFor deployment, we will require the following config,\n\n- `provider`, should be `elasticbeanstalk`, already configured and heavy lifting by the `Travis CI`\n- `region`, Where the `AWS Elastic Beanstalk` is being created\n- `app`, our `Elastic Beanstalk` app name\n- `env`, our `Elastic Beanstalk` environment name\n- `bucket_name`, automatically generated bucket name by the `Elastic Beanstalk`\n- `bucket_path`, same as the `app` name\n- `on -> branch`, on which branch code changes, we should re-deploy the code base\n- `credentials`, to get the credentials to access `Elastic Beanstalk` by `Travis CI`, we have to create a new IAM user with full programmatic access to `Elastic Beanstalk`. For security purpose, we will use the `Travis CI` environment variables to store our aws access key and secret.\n\nOur new `.travis.yml` file should be,\n\n```yml\nlanguage: generic\nsudo: required\nservices:\n  - docker\n\nbefore_install:\n  - docker build -t docker_user_name/ix-docker-react -f Dockerfile.dev .\n\nscript:\n  - docker run -e CI=true docker_user_name/ix-docker-react npm run test -- --coverage\n\ndeploy:\n  provider: elasticbeanstalk\n  region: \"ap-south-1\"\n  app: \"ix-docker-react\"\n  env: \"Dockerreact-env\"\n  bucket_name: \"elasticbeanstalk-ap-south-1-366735605679\"\n  bucket_path: \"ix-docker-react\"\n  on:\n    branch: master\n  access_key_id: $AWS_ACCESS_KEY\n  secret_access_key: $AWS_SECRET_KEY\n```\n\n> In the deploy config, the path name of the `S3` bucket should be same as the app name.\n\nIn the `Elastic Beanstalk`, it should open port `80` for `Nginx`. We have to specify the port in the `Dockerfile`. We can expose a port in the `Elastic Beanstalk` with `Dockerfile` by `EXPOSE 80`. Our new `Dockerfile` with updated configuration should be\n\n```docker\nFROM node:alpine\nWORKDIR '/app'\nCOPY package.json .\nRUN npm install\nCOPY . .\nRUN npm run build\n\nFROM nginx\nEXPOSE 80\nCOPY --from=0 /app/build /usr/share/nginx/html\n```\n\nNow, if we push the changes to the `github` the application should be deployed to the `AWS Elastic Beanstalk`.\n\nFrom `Elastic Beanstalk` we can get the web url and access our react application.\n\nNow we have a complete `CI/CD` with `docker`, `github`, `Travis CI` and `AWS Beanstalk`. In a team, engineers make commit in the feature branch. Other engineers will review and merge the code base to master branch. The moment, codebase is being merged to the master branch, the `CI/CD` will be triggered and make the deployment.\n\n### Cleanup\n\n---\n\nFor the `AWS Beanstalk`, when we create a environment, the `AWS Beanstalk` create a `EC2 Instance` internally. This `EC2 Instance` is costing money. So after we are done our experiments, we need to delete the `AWS Beanstalk` application. We can delete the `Beanstalk Application` from the dashboard.\n"}],"path":"../..//02 CI-CD With Docker","content":""},{"type":"folder","name":"03 Running container Like A Boss","children":[{"type":"file","name":"01 Intro","children":null,"path":"../..//03 Running container Like A Boss/01 Intro.md","content":"# Using Container Like A Boss\n\nContainers are the fundamental building blocks of `Docker` tool-kit. Before we get stared, we have to make sure the latest version of `Docker` is installed in the system. It is important to keep in mind, `Docker` is different from the `VM`.\n\nIn this article, we are going to discuss, how `Docker` is different from `VM`, play with `Nginx` server container and look into basic networking of `Docker`.\n\nBefore we dive, let's check the `Docker Version` in our system.\n\n```bash\ndocker version\n```\n\nWith this command, we should get an output, something like this,\n\n```\nClient: Docker Engine - Community\n Version:           20.10.5\n API version:       1.41\n Go version:        go1.13.15\n Git commit:        55c4c88\n Built:             Tue Mar  2 20:18:20 2021\n OS/Arch:           linux/amd64\n Context:           default\n Experimental:      true\n\nServer: Docker Engine - Community\n Engine:\n  Version:          20.10.5\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.13.15\n  Git commit:       363e9a8\n  Built:            Tue Mar  2 20:16:15 2021\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.4.4\n  GitCommit:        05f951a3781f4f2c1911b05e61c160e9c30eaa8e\n runc:\n  Version:          1.0.0-rc93\n  GitCommit:        12644e614e25b05da6fd08a38ffa0cfe1903fdec\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\nIf you get the `Docker` server information, that means we can talk to the server. Otherwise, there is something wrong with the `Docker` installation, like not installed properly, having permission error etc. \n\nSince, we are going to a lot of new features, ideally you should get the latest version as possible.\n\nAdditionally, to get more information about the installed `Docker` server, we can run,\n\n```bash\ndocker info\n```\n\n## Commands and Management Commands\n\nIf we want to look all the available commands for `Docker`, we can run,\n\n```bash\ndocker\n```\n\nFrom this output, we can notice two sections\n\n1. Management Commands\n2. Commands\n\nPreviously, all the `Docker` commands was available like `docker command`. Since the number of commands increase, the `Docker` team decided to separate them in `sum-commands` aka `Management Commands`. With `Management Commands`, we run commands like `docker <command> <sub-command>`.\n\n**Old Way:** `docker <command> (options)`\n\n**New Way:** `docker <command> <sub-command> (options)`\n\nAll the old commands are working fine with latest `Docker`. But `Docker` is pushing forward to use the new model `Management Command`.\n\n### Image Vs Container\n\nAn image is an application we want to run. On the other hand, a `Container` is an instance of the `image`. Essentially, we can have multiple container running of the same image."},{"type":"file","name":"02 Play With Nginx Server","children":null,"path":"../..//03 Running container Like A Boss/02 Play With Nginx Server.md","content":"# Play With Nginx Server\n\n### Run a Container\n\nTo play around the `Docker Container`, we will use `Nginx Server Image`. We will pull it from the `Docker Registry` aka `Docker Hub` and run in our local machine.\n\nGo to terminal and run \n\n```bash\ndocker container run --publish 80:80 nginx\n```\n\nIn browser, if we go to `http://localhost/`, we should see the `Nginx` server is up and running.\n\nWith this command `docker container run --publish 80:80 nginx`, \n\n-  The `Docker` engine looks for the `image` for the `Nginx` web server and if not found, pull it from the docker hub\n- Run the `Nginx` web server\n- Expose our local machine port `80` and server all the traffic to the `80` port of the `Nginx` server\n\nTo stop the container, we can simply try `Ctrl + c`.\n\nWe can run the container in background, by using `detach` flag.\n\n```bash\ndocker container run --publish 80:80 --detach nginx\n```\n\nThis will run the `Docker` container in background.\n\nWe can look all the containers list by\n\n```bash\ndocker container ls\n```\n\nWe can stop the container by \n\n```bash\ndocker container stop <container-id>\n```\n\nNow, if we look the container list by `docker container ls`, the list should be empty.\n\nTo observe all the stopped and running containers we can do,\n\n```bash\ndocker container ls -a\n```\n\nIn the last column of the list, we see random name of the `Container`. We can define the name while creating and running a container by,\n\n```bash\ndocker container run --publish 80:80 --detach --name <container-name> nginx\n```\n\nNow, if we print the list of running container by `docker container ls`, we will notice the new container with our defined name.\n\nWe might want to look for the logs of the `Docker` container, by\n\n```bash\ndocker container logs <container-name>\n```\n\nThere are couple of options, while looking for logs,\n\nWe can remove docker containers by,\n\n```bash\ndocker container rm <container-id-1> <container-id-2>\n```\n\nThis will only remove the stopped container. To stop running containers, we can first stop the running container and then then remove. Also, we can use `force` flag to remove the running container by,\n\n```bash\ndocker container rm -f <container-id>\n```\n\nThis will remove the running container.\n\nWith `Docker`, in a matter a of time, we are able to run a `Nginx` server with default configuration. We are also able to look the logs and eventually stop the container."},{"type":"file","name":"03 With Container Run","children":null,"path":"../..//03 Running container Like A Boss/03 With Container Run.md","content":"# With Container Run\n\nWhen we run the container using `docker container run --publish 80:80 --name <container-name> nginx`, it actually interpret by `docker container run --publish 80:80 --name <container-name> -d nginx:latest nginx -T`.\n\nThis command do the following steps:\n\n- Look for `Nginx` image in local cache, if not found get it from `Docker` registry.\n  - If we specify the image version, it will be downloaded\n  - If we do not specify the image, it will download the latest version\n- Create a container out of the image\n- Give a virtual IP on a private network inside the `Docker Engine`\n- Open port `80` in host machine and forward all traffic of `80` to docker container port `80`\n- Start executing container start up command\n\nSo while run the container using the specified command, we can change the port mapping and also the image version."},{"type":"file","name":"04 Container vs VM","children":null,"path":"../..//03 Running container Like A Boss/04 Container vs VM.md","content":"# Container vs VM\n\nContainer is just a process. We can observe this specific process from the host machine and even stop anytime.\n"},{"type":"file","name":"05 Windows Containers","children":null,"path":"../..//03 Running container Like A Boss/05 Windows Containers.md","content":"# Windows Containers\n\n`Docker` container is no longer mean to be `Linux Container`. Since 2017, windows server 2016, introduce the `Windows Container`. And they are `.exe` binaries run on the windows machine without linux being installed. "},{"type":"file","name":"06 Assignment of Multiple Containers","children":null,"path":"../..//03 Running container Like A Boss/06 Assignment of Multiple Containers.md","content":"## Assignment of Multiple Containers\n\nWe will do the following task:\n\n- No networking between containers are required\n- Run `Nginx` server \n  - With detach mode \n  - With a defined name `proxy`\n  - At port 80\n- Run `mysql` server\n  - With detach mode \n  - With a defined name `db`\n  - At port 3306\n  - Invoke to generate random password and find it from logs\n- Run `apache` server\n  - With detach mode \n  - With a defined name `web_server`\n  - At port 8080\n\n### Nginx\n\nWe can run `Nginx` by\n\n```\ndocker container run -d --name proxy -p 80:80 nginx\n```\n\nThe server should run in `http://localhost/`\n\nRef:\n1. [image](https://hub.docker.com/_/nginx)\n2. [Docker run](https://docs.docker.com/engine/reference/commandline/container_run/)\n3. [Docker Detach](https://www.freecodecamp.org/news/docker-detached-mode-explained/)\n4. [Assign Name](https://docs.docker.com/engine/reference/commandline/run/#assign-name-and-allocate-pseudo-tty---name--it)\n5. [Expose Port](https://docs.docker.com/engine/reference/commandline/run/#publish-or-expose-port--p---expose)\n\n### MySQL\n\nWe can run `mySQL` by\n\n```bash\ndocker run -d --name db -p 3306:3306 -e MYSQL_RANDOM_ROOT_PASSWORD=yes mysql\n```\n\nWe can look for logs\n\n```bash\ndocker logs\n```\n\nIn logs, we should find the root password\n\n```txt\nGENERATED ROOT PASSWORD: auto_generated_password\n```\n\nRef:\n\n1. Image from [Docker Hub](https://hub.docker.com/_/mysql)\n2. `MYSQL_RANDOM_ROOT_PASSWORD` environment name [Docker Hub](https://hub.docker.com/_/mysql)\n3. [Passing environment](https://docs.docker.com/engine/reference/run/#env-environment-variables)\n\n### Apache (httpd)\n\nWe can run the apache server by,\n\n```bash\ndocker container run -d --name web_server -p 8080:80 httpd\n```\n\nThe server should run in `http://localhost:8080/`.\n\nRef:\n\n1. [image](https://hub.docker.com/_/httpd)\n\n### Stopping all the containers\n\nWe can list down the containers by,\n\n```bash\ndocker container ls\n```\n\nWe can stop all these containers,\n\n```bash\ndocker container stop <nginx-container-id> <mysql-container-id> <nginx-container-id>\n```\n\nNow `docker container ls` should return list of containers as stopped status.\n\n### Delete all the images\n\nWe can list down the containers by,\n\n```bash\ndocker container ls\n```\n\nWe can remove all these containers,\n\n```bash\ndocker rm -f <nginx-container-id> <mysql-container-id> <nginx-container-id>\n```\n\nNow `docker container ls` should return empty list."},{"type":"file","name":"07 CLI Process Monitoring","children":null,"path":"../..//03 Running container Like A Boss/07 CLI Process Monitoring.md","content":"## CLI Process Monitoring\n\nUsing `docker-cli`, for containers, we will test the followings, \n\n- Running processes of a container\n- Container metadata\n- Performance stats of running container\n\n\nBefore we start, let's start two container, `nginx` and `mysql`,\n\n```bash\ndocker container run -d --name nginx nginx\ndocker container run -d --name mysql -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql \n```\n\n### Running Processes\n---\n\nTo check the running process of a container we can use the `top` command. [top](https://docs.docker.com/engine/reference/commandline/top/) display running processes of a container.\n\n```bash\ndocker container top mysql\n```\n\nThis will show the running process of `mysql` container.\n\n### Inspect Container Metadata\n---\n\n[inspect](https://docs.docker.com/engine/reference/commandline/inspect/) returns low level information and metadata of docker container, like\n\n- startup config\n- volume mapping\n- networking\n\n```bash\ndocker container inspect mysql\n```\n\n### Monitoring running container stats\n---\n\nWe can use `stats` to stream the running container stats, like `CPU` usage, `memory` usage etc.\n\n```bash\ndocker container stats\n```"},{"type":"file","name":"08 Getting a Shell Inside a Container","children":null,"path":"../..//03 Running container Like A Boss/08 Getting a Shell Inside a Container.md","content":"## Getting a Shell Inside a Container\n\nTo interact in the container, we need to get inside the container. We can consider 2 scenarios here,\n\n- Get interactive terminal in container\n- Run a program inside a container\n\n### Interactive Terminal on Container Startup \n---\n\n`-it` is a combination of two flag `i` and `t`. They provide an standard input output along with a terminal for a container. To get an interactive terminal for `nginx` server, we can run,\n\n```docker\ndocker container run -it --name proxy nginx bash\n```\n\nWe can exit using\n\n```bash\nexit\n```\n\nWe can do detail experiments for `ubuntu` os,\n\n```docker\ndocker container run -it --name ubuntu ubuntu\n```\n\nNow in the interactive terminal, first update the packages and install `curl`\n\n```bash\napt-get update\napt-get install -y curl\n```\n\nWe can use `curl` from the terminal.\n\n```bash\ncurl google.com\n```\n\nWe can exit from the terminal by\n\n```bash\nexit\n```\n\nIf we again want to get the terminal in the running `ubuntu` machine,\n\n```docker\ndocker container start ubuntu -ai\n```\n\n### Run Program Inside Container\n\n---\n\nWe can run program inside the container using the `exec` command.\n\nLet's run `mysql` inside a container,\n\n```docker\ndocker container run -d --name mysql -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql \n```\n\nIn the `mysql` container, there is a preinstalled program, called `bash`. We can execute the `bash` and get access to the interactive terminal.\n\n```docker\ndocker container exec -it mysql bash\n```\n\nLet's get image of `alpine` from the `docker-hub`,\n\n```docker\ndocker pull alpine\n```\n\nSince, the `bash` is not installed in the `alpine` and we try to run it inside the container, we will get an error, the program is not avaiable.\n\n```docker\ndocker container run -it alpine bash\n```\n\nIn `alpine` there is another program called `sh` with similar functionality. We can run `sh` in the `alpine` by,\n\n```docker\ndocker container run -it alpine sh\n```\n\n> `Alpine` is a minimal featured, security focused linux distribution."},{"type":"file","name":"09 IP Fact","children":null,"path":"../..//03 Running container Like A Boss/09 IP Fact.md","content":"## IP Fact\n\n### An Interesting IP Fact\n\nLet's run `nginx` server on port `80`,\n\n```docker\ndocker container run -p 80:80 --name webhost -d nginx\n```\n\nIn the container, we can check the `PORT` of the container by,\n\n```docker\ndocker container port webhost\n```\n\nI got output of,\n\n```txt\n80/tcp -> 0.0.0.0:80\n```\n\nSince, we are mapping the port in container at `80`, this seems fine.\n\nLet's check the `IP` of the container,\n\n```docker\ndocker container inspect --format '{{ .NetworkSettings.IPAddress }}' webhost\n```\n\nIn my machine, I got container IP,\n\n```txt\n172.17.0.3\n```\n\nNow, to check my host machine IP, first install a tool named `net-tools`.\n\n```bash\nsudo apt-get install -y net-tools\n```\n\nAssuming the `net-tools` is installed in the machine, I checked the IP of my host machine,\n\n```bash\nifconfig usb0\n```\n\nI got my host machine `IP` as output, \n\n```bash\n192.168.42.203\n```\n\nIt seems, the container IP `172.17.0.3` and host machine IP `192.168.42.203` is not same."},{"type":"file","name":"10 Container Networking","children":null,"path":"../..//03 Running container Like A Boss/10 Container Networking.md","content":"## Container Networking\n\nWhile we start a container with `-p` flag, it exposes a port from the host machine to the docker container. In background, a lot more is going on, we will discuss here.\n\nThis `networking` staff is plugable. Under the hood, We can add a container to a container, remover form a network.\n\nWhen we start a container, we are particularly behind a docker network, called `bridge-network`. This `bridge-network` routes through `NAT Firewall` of the host IP. It is configured by the `docker-daemon` on our behalf. So the container can go back and forth of the outside internet and other network.\n\nBut whenever we need to build a communication between specific containers we do not need any port mapping using `-p`.\n\nIf we have a network with a `node` application container and a `mongoDB` container, and they need to connect each other, we do not have to do the port mapping or open the port to the rest of the physical network.\n\nAnd if we have another network with `php` server container and `mysql` database container, they can communicate each other but can not communicate with `node` and `mongoDB` servers network.\n\nWith this setup, if the `php server` have to connect with the `node server`, it has to go through the host machine network.\n\nAll these configurations are configurable, can be add, removed or changed.\n\n> We can not have two container listening at the same port at the host level\n\n**Figure**\n\n### Networking CLI Hands On\n\nHere we will create two docker network with containers in it. Then play around of adding, removing containers of these networks.\n\n**Objective**\n\nWe will create two container, `ix_nginx_1` and `ix_nginx_2`. The `ix_nginx_1` should connect to a custom network `ix_network`. On the other hand, the `ix_nginx_2` should connect to the both networks, `ix_network` and the default docker network.\n\n**Networking Setup**\n\nWe can get the list of networks by,\n\n```docker\ndocker network ls\n```\n\nThis will output the list of network.\n\n> `bridge` is the default docker network. `host` is the network, skip `docker-virtual-network` and attach to host network directly. Disadvantages is, this skip default container security but could be useful for high throughput networking. `null` is not attached to anything.\n\nTo see the details of the default network,\n\n```docker\ndocker network inspect bridge\n```\n\nIn the output, under `containers`, a list of containers under the default `bridge` network will be displayed.\n\nTo create our own docker network named `ix_network`,\n\n```docker\ndocker network create ix_network\n```\n\nThis will create the network and return the network id.\n\nWe can verify this network existence by looking at the list of network by,\n\n```docker\ndocker network ls\n```\n\nIn output list, there should be a network named `ix_network`. It is using default driver bridge.\n\nNow, we will create a nginx container named `ix_nginx_1` that should go under our new custom network, `ix_network`,\n\n```docker\ndocker container run -d --name ix_nginx_1 --network ix_network nginx\n```\n\nTo check, if the new container is running under our new created network,\n\n```docker\ndocker network inspect ix_network\n```\n\nIn the `containers` property, the container named `ix_nginx_1` should appear.\n\nNow, create another container named `ix_nginx_2` under the default network,\n\n```docker\ndocker container run -d --name ix_nginx_2 nginx\n```\n\nIf we inspect the default network, in the containers property, the container, `ix_nginx_2` should appear.\n\n```docker\ndocker network inspect bridge\n```\n\n**Verify Setup**\n\nMake sure,\n\n- `ix_nginx_1` container is inside the `ix_network` network, `docker network inspect ix_network`\n  - We can inspect it through the `ix_network` network inspection\n  - Or inspecting the container itself, `docker container inspect ix_nginx_1`\n- `ix_nginx_2` container is inside the default network\n  - We can inspect it through the default network inspection, `docker network inspect bridge`\n  - Or inspecting the container itself, `docker container inspect ix_nginx_2`\n\n**Experiments**\n\nWe can add the `ix_nginx_2` to the `ix_network` network using the `connect` command.\n\nNow connect `ix_nginx_2` to the `ix_network` network\n\n```docker\ndocker network connect ix_network ix_nginx_2\n```\n\nWe can verify, `ix_nginx_2` is connected to two networks by,\n\n```docker\ndocker container inspect ix_nginx_2\n```\n\nUnder the `NetworkSettings.Networks` property, there should be the default `bridge` network and `ix_network`.\n\nWe can disconnect `ix_nginx_2` from the `ix_network` by disconnect command,\n\n```docker\ndocker network disconnect ix_network ix_nginx_2\n```\n\nThe beauty of containerization is using networking, even though we run all the app in a same server, we can protect them separately.\n\n### DNS\n\nIn the world of containers, there is constant change of containers like, launching, stopping, expanding, shrinking etc. Containers can go away, fail on runtime, can crash. In these cases, docker will bring them up with a new IP. Since things are so dynamic and complicated, We can not relay on IP addresses, or deal with IP addresses inside the container.\n\nFor this, `docker` provide a built-in solution, `DNS-Naming`.\n\n**Objective**\n\n**Figure**\n\nWe can get list of containers,\n\n```docker\ndocker container ls\n```\n\nMake sure, the container, `ix_nginx_1` is running under the `ix_network`. If not run them by,\n\n```docker\ndocker container start ix_nginx_1\n```\n\nAnd verify the `ix_network` contains the container `ix_nginx_1`\n\n```docker\ndocker network inspect ix_network\n```\n\nLet's create another container `ix_dns_nginx` inside the `ix_network`,\n\n```docker\ndocker container run -d --name ix_dns_nginx --network ix_network nginx\n```\n\nDocker default `bridge` network does not support built in `DNS Resolution`.\n\nSince `ix_nginx_1` and `ix_dns_nginx` are not the under default `bridge` network, it has the built in special feature, `DNS Resolution`.\n\nNow, let's again check, the `ix_nginx_1` and `ix_dns_nginx` are in the same network `ix_network`,\n\n```docker\ndocker network inspect ix_network\n```\n\nUnder the `Containers` property, both containers should appear.\n\nLets, try to ping `ix_nginx_1` from the `ix_dns_nginx` container, by\n\n```docker\ndocker container exec -it ix_dns_nginx ping ix_nginx_1\n```\n\nIt is possible the latest `nginx` container does not have the `ping` program pre-installed. In this case, it will throw an error like this,\n\n> OCI runtime exec failed: exec failed: container_linux.go:367: starting container process caused: exec: \"ping\": executable file not found in $PATH: unknown\n\nIf you notice this error, first install the `ping` program in the `ix_dns_nginx` container.\n\nRun `bash` in the `nginx` container,\n\n```docker\ndocker container exec -it ix_dns_nginx bash\n```\n\nFrom the `bash`, update the package manager and install the `ping` command,\n\n```bash\napt-get update\napt-get install iputils-ping\n```\n\nNow we can ping the `ix_nginx_1` from the bash by,\n\n```bash\nping ix_nginx_1\n```\n\nIf you ping from the terminal first exit from the bash,\n\n```bash\nexit\n```\n\nNow from your own terminal run,\n\n```docker\ndocker container exec -it ix_dns_nginx ping ix_nginx_1\n```\n\nWe can ping another container in the same network without IP.\n\nThe resolution works in both ways, we can ping `ix_dns_nginx` from the `ix_nginx_1` server also.\n\nThis makes super easy when we need to talk from one container to another container. These containers IP address may not be same but their container name or the host names will always be the same.\n\n> The default docker `bridge` network has a disadvantages. It does not have built in `DNS` server. In this case we have to use the `--link` flag. It's comparatively easier to use the custom network for this purpose, instead of using the default `bridge` network.\n\n> Using `docker-compose`, we can automatically spin up a `virtual` network for us.\n"},{"type":"file","name":"11 Assignments","children":null,"path":"../..//03 Running container Like A Boss/11 Assignments.md","content":"## Assignments\n\n- Check the `curl` version from two container, created from\n  - Cent OS 7\n  - Ubuntu 14.04\n- Remove all these containers\n\n### Cent OS\n\nCreate container from Cent OS 7. `Cent OS` with version 7 can be found [docker hub](https://hub.docker.com/_/centos?tab=tags&page=1&ordering=last_updated).\n\n```docker\ndocker container run -it centos:7 bash\n```\n\nWe can check curl version from bash by\n\n```bash\ncurl --version\n```\n\nThis return the `curl` version of `7.29.0`\n\n### Ubuntu\n\nCreate container from ubuntu 14.04. The ubuntu image of version 14.04 can be found in [docker-hub](https://hub.docker.com/layers/ubuntu/library/ubuntu/14.04/images/sha256-a664cf8519ac301fb0ef545c3b3e53dfdffcf9a0235ddaa51eca299948cc568f?context=explore)\n\n```docker\ndocker container run -it ubuntu:14.04 bash\n```\n\nBy default, the `curl` is not installed. To install `curl`, first update the `packages` by,\n\n```bash\napt-get update\n```\n\nNow, install the `curl` by,\n\n```bash\napt-get install -y curl\n```\n\nSince the `curl` is installed, check the version,\n\n```bash\ncurl --version\n```\n\nWe should get version of `curl` as `7.35.0`.\n\n### Remove Containers\n\nTo remove containers, we can use \n\n```docker\ndocker rm -f <cent_os_7_container_id> <ubuntu_14_04_container_id>\n```\n\nApparently, If we used `--rm` flag while starting the container, we do not have to remove these container manually.\n"},{"type":"file","name":"12 Assignment","children":null,"path":"../..//03 Running container Like A Boss/12 Assignment.md","content":"## Assignments\n\nHere we will use same host name for multiple docker container.\n\nWe will run two container from `elastic-search` named `elasticsearch1` and `elasticsearch2`. Both container should have same dns name, `search`.\n\nFrom alpine, we will run `nslookup` and ensure, the `search` is the dns name of both container.\n\nFrom `centos` we will verify. `search:9200` work similar to load balancer between these two containers.\n\n- Create 2 containers from `elasticsearch:2`\n  - Use `--network-alias search` while creating to provide additional `DNS` name\n- Create container from `alpine:3.10`\n  - From `alpine` use `--net` to see, both containers with same `DNS` name\n- Create container from `centos`\n  - `curl` to `search:9200` and see both name field shows\n\nCheck network list.\n\n```docker\ndocker network ls\n```\n\nCreate a custom network named `dns_rest`, we will do all the work.\n\n```docker\ndocker network create dns_res\n```\n\nMake sure network is created and in containers, no containers is connected.\n\n```docker\ndocker network inspect dns_res\n```\n\nCreate two container in the network `dns_res`, with\n\n- named `elastic_search1`, and `elastic_search2`\n- should go under network `dns_res`\n- should have network alias `search`\n\n```docker\ndocker container run --name elastic_search_1 --network dns_res --network-alias search -d elasticsearch:2\n```\n\n```docker\ndocker container run --name elastic_search_2 --network dns_res --network-alias search -d elasticsearch:2\n```\n\nCheck the network and these two container should be under the `dns_res` network.\n\n```docker\ndocker network inspect dns_res\n```\n\nRun a container of `alpine` and open shell inside,\n\n```docker\ndocker container run --network dns_res -it alpine:3.10 sh\n```\n\nCheck the two container is exist in the `search` hostname using `nslookup tool`\n\n```sh\nnslookup search\n```\n\nThis should give output like,\n\n```txt\nName:      search\nAddress 1: 172.25.0.2 elastic_search_1.dns_res\nAddress 2: 172.25.0.3 elastic_search_2.dns_res\n```\n\nNow create a container of `centos` and open terminal in it,\n\n```docker\ndocker container run -it --network dns_res centos:7\n```\n\nRun the following couple of times, only these two containers should appear randomly.\n\n```bash\ncurl -s search:9200\n```\n"}],"path":"../..//03 Running container Like A Boss","content":""},{"type":"folder","name":"04 Images","children":[{"type":"file","name":"01 Caching","children":null,"path":"../..//04 Images/01 Caching.md","content":"## Caching\n\n### Image Layers\n\n---\n\n**Image History**\n\nWe can look for the image history by,\n\n```docker\ndocker image history nginx\n```\n\nThis should output the `nginx` image history.\n\nWe get a list of image layers. All the images start from a scratch. Each layer contains the increment of the previous one. It could be `changes in file system` or `adding some config/metadata to the image`.\n\nWe can have on layer, we can even have thousands of layers.\n\n`Figure Image Layers`\n\nWe never store the same image layer twice in the system.\n\n`Figure Copy On Write`\n\n### Image Inspection\n\n---\n\nWe can closely inspect the image,\n\n```docker\ndocker image inspect nginx\n```\n\nImage inspections give us metadata, some interesting config we can look for,\n\nexposed ports\n\nenv variables\n\nstart up command\n"},{"type":"file","name":"02 Tagging","children":null,"path":"../..//04 Images/02 Tagging.md","content":"### Tagging\n\n---\n\nwe can get the list of images by,\n\n```docker\ndocker image ls\n```\n\nIn the image list, we can notice docker images does not have a name.\n\n> Only official images can have root namespace. Other images repository name be like `user_name/repository_name`\n\nThe image tag is not exactly a version or branch, it's the git tag and it can represent both.\n\n> In docker world, `latest` tag is more of a meaning to `default` or `stable` image. `latest` tag in docker image does not imply the latest image. It is possible to take older image and tagged as `latest`\n\nLet's assume, we have the nginx image with latest tag. If not get it,\n\n```docker\ndocker pull nginx:latest\n```\n\nThe latest nginx also tagged with `mainline`. So If we try to pull the `nginx:mainline`,\n\n```docker\ndocker pull nginx:mainline\n```\n\nInstead of download, it will use the downloaded `nginx:latest` image layer.\n\nNow, if we print the list of images,\n\n```docker\ndocker image ls\n```\n\nWe will see the nginx image has same `IMAGE ID` but different tag.\n\n### Tag Existing Image\n\n---\n\nWe can create a tagged target image from the existing image by [tag](https://docs.docker.com/engine/reference/commandline/image_tag/) command,\n\n```docker\ndocker image tag nginx bmshamsnahid/nginx\n```\n\nIf we now see the image list,\n\n```docker\ndocker image ls\n```\n\nWe will see a new nginx image repository `bmshamsnahid/nginx` with `latest` image. This new image should also have the same image id.\n\n> If we do not specify the tag, it will take the `latest` tag by default\n\n### Pushing Image to Docker Hub\n\n---\n\nMake sure you have a account in [docker hub](https://hub.docker.com/) and you are logged in to your local machine,\n\n```docker\ndocker login\n```\n\nNow, push our latest image to docker hub,\n\n```docker\ndocker push bmshamsnahid/nginx\n```\n\nIn docker hub, we should see the image `bmshamsnahid/nginx` with `latest` tag.\n\nNow to add another tag, `testing`, we can run,\n\n```docker\ndocker tag bmshamsnahid/nginx bmshamsnahid/nginx:testing\n```\n\nWe can verify the `testing` tag in local image list by,\n\n```docker\ndocker image ls\n```\n\nIn the output list, we should see a image `bmshamsnahid/nginx` with `testing` tag.\n\nWe can push this newly `testing` tagged image to docker hub by,\n\n```docker\ndocker push bmshamsnahid/nginx:testing\n```\n\nSince the image layer is same, only the tag is different, we should see `Layer already exists` in the console. Also it should add a image tag `testing` in the docker hub.\n\nSo if an image layer exist in the docker hub, it does not upload twice. And same for the local machine, if an image layer exists in the local machine cache, it does not download twice.\n\n> To upload a image in private repository, we first have to create a private repository in the docker hub\n"},{"type":"file","name":"03 Building Image","children":null,"path":"../..//04 Images/03 Building Image.md","content":"### Building Image\n\n`Dockerfile` is a recipe for building a docker image. Any docker images we used, is created from the dockerfile.\n\n### Best Practices\n\n- More changes should go below\n- Less changes should stay top\n"},{"type":"file","name":"04 Assignment","children":null,"path":"../..//04 Images/04 Assignment.md","content":"## Assignment\n\nIn this assignment, objective is create a `Dockerfile` that can run a node app.\n\n- Make sure `Dockerfile` run a node app in local environment\n- Push the image to `docker hub`\n- Remove local images from local machine cache\n- Get the image from `docker-hub` and run the node app again\n\nCreate a node app in a directory named, `dockerfile-assignment-1`,\n\n```bash\nmkdir dockerfile-assignment-1 # creating a directory for the node app\ncd dockerfile-assignment-1 # go to the directory\nnpm init -y # create a node app\n```\n\nNow create a `Hello World` app using `express.js` from [example](https://expressjs.com/en/starter/hello-world.html),\n\nOur `package.json` file should similar,\n\n```json\n{\n  \"name\": \"dockerfile-assignment-1\",\n  \"version\": \"1.0.0\",\n  \"description\": \"\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"dependencies\": {\n    \"express\": \"*\"\n  },\n  \"keywords\": [],\n  \"author\": \"\",\n  \"license\": \"ISC\"\n}\n```\n\nAnd the server file will be `index.js`,\n\n```bash\ntouch index.js\n```\n\nOur `index.js` file should be similar to the [example](https://expressjs.com/en/starter/hello-world.html)\n\n```js\nconst express = require('express');\nconst app = express();\nconst port = 3000;\n\napp.get('/', (req, res) => {\n  res.send('Hello World!');\n});\n\napp.listen(port, () =>\n  console.log(`Example app listening at http://localhost:${port}`)\n);\n```\n\nNow create a `Dockerfile`,\n\n```bash\ntouch Dockerfile\n```\n\nOur dockerfile should be like,\n\n```docker\nFROM node:15.14.0\nWORKDIR app\nCOPY package.json package.json\nRUN npm install\nCOPY . .\nEXPOSE 3000\nCMD [\"node\", \"index.js\"]\n```\n\nNow build a image and run the container from it,\n\n```docker\ndocker build . -t node-app && docker run -p 80:3000 node-app\n```\n\nNow we should see the server response from browser by browsing `http://127.0.0.1/`.\n\nTo push the docker image to the `docker hub` it should have a repository name of `user_name/repository_name`.\n\nWe can tagged the image to the docker hub format by,\n\n```docker\ndocker tag node-app bmshamsnahid/node-app\n```\n\nWe got the exact format to push the image to `docker-hub`. We can verify this by,\n\n```docker\ndocker image ls\n```\n\nWe should see the image with repository name `bmshamsnahid/node-app`.\n\nPush the image to docker hub by,\n\n```docker\ndocker push bmshamsnahid/node-app:latest\n```\n\nWe again run the node app from the image that is now in the docker hub. To do so, first remove the local image and running container.\n\nRemove images from local machine cache,\n\n```docker\ndocker image rm -f node-app bmshamsnahid/node-app\n```\n\nStop and remove the container,\n\n```docker\ndocker container stop <container_id>\n```\n\nNow, get the image from `docker-hub` and run a container from it,\n\n```docker\ndocker container run --rm -p 80:3000 bmshamsnahid/node-app\n```\n\nThis should run our node app and should be access in `http://127.0.0.1/`\n"}],"path":"../..//04 Images","content":""}]